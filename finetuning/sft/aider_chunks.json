[
  {
    "id": "aider/__init__.py::1",
    "metadata": {
      "file_path": "aider/__init__.py",
      "file_name": "__init__.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 122,
      "span_ids": [
        "imports"
      ],
      "start_line": 1,
      "end_line": 21,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from packaging import version\n\n__version__ = \"0.71.2.dev\"\nsafe_version = __version__\n\ntry:\n    from aider._version import __version__\nexcept Exception:\n    __version__ = safe_version + \"+import\"\n\nif type(__version__) is not str:\n    __version__ = safe_version + \"+type\"\nelse:\n    try:\n        if version.parse(__version__) < version.parse(safe_version):\n            __version__ = safe_version + \"+less\"\n    except Exception:\n        __version__ = safe_version + \"+parse\"\n\n__all__ = [__version__]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/__main__.py::1",
    "metadata": {
      "file_path": "aider/__main__.py",
      "file_name": "__main__.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 17,
      "span_ids": [
        "imports"
      ],
      "start_line": 1,
      "end_line": 5,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from .main import main\n\nif __name__ == \"__main__\":\n    main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/analytics.py::1",
    "metadata": {
      "file_path": "aider/analytics.py",
      "file_name": "analytics.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 128,
      "span_ids": [
        "imports",
        "compute_hex_threshold"
      ],
      "start_line": 1,
      "end_line": 27,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import json\nimport platform\nimport sys\nimport time\nimport uuid\nfrom pathlib import Path\n\nfrom mixpanel import MixpanelException\nfrom posthog import Posthog\n\nfrom aider import __version__\nfrom aider.dump import dump  # noqa: F401\nfrom aider.models import model_info_manager\n\nPERCENT = 10\n\n\ndef compute_hex_threshold(percent):\n    \"\"\"Convert percentage to 6-digit hex threshold.\n\n    Args:\n        percent: Percentage threshold (0-100)\n\n    Returns:\n        str: 6-digit hex threshold\n    \"\"\"\n    return format(int(0xFFFFFF * percent / 100), \"06x\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/analytics.py::2",
    "metadata": {
      "file_path": "aider/analytics.py",
      "file_name": "analytics.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 247,
      "span_ids": [
        "impl:3",
        "is_uuid_in_percentage"
      ],
      "start_line": 30,
      "end_line": 57,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def is_uuid_in_percentage(uuid_str, percent):\n    \"\"\"Check if a UUID string falls within the first X percent of the UUID space.\n\n    Args:\n        uuid_str: UUID string to test\n        percent: Percentage threshold (0-100)\n\n    Returns:\n        bool: True if UUID falls within the first X percent\n    \"\"\"\n    if not (0 <= percent <= 100):\n        raise ValueError(\"Percentage must be between 0 and 100\")\n\n    if not uuid_str:\n        return False\n\n    # Convert percentage to hex threshold (1% = \"04...\", 10% = \"1a...\", etc)\n    # Using first 6 hex digits\n    if percent == 0:\n        return False\n\n    threshold = compute_hex_threshold(percent)\n    return uuid_str[:6] <= threshold\n\n\nmixpanel_project_token = \"6da9a43058a5d1b9f3353153921fb04d\"\nposthog_project_api_key = \"phc_99T7muzafUMMZX15H8XePbMSreEUzahHbtWjy3l5Qbv\"\nposthog_host = \"https://us.i.posthog.com\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/analytics.py::3",
    "metadata": {
      "file_path": "aider/analytics.py",
      "file_name": "analytics.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 228,
      "span_ids": [
        "Analytics.enable",
        "Analytics",
        "Analytics.__init__"
      ],
      "start_line": 60,
      "end_line": 100,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Analytics:\n    # providers\n    mp = None\n    ph = None\n\n    # saved\n    user_id = None\n    permanently_disable = None\n    asked_opt_in = None\n\n    # ephemeral\n    logfile = None\n\n    def __init__(self, logfile=None, permanently_disable=False):\n        self.logfile = logfile\n        self.get_or_create_uuid()\n\n        if self.permanently_disable or permanently_disable or not self.asked_opt_in:\n            self.disable(permanently_disable)\n\n    def enable(self):\n        if not self.user_id:\n            self.disable(False)\n            return\n\n        if self.permanently_disable:\n            self.disable(True)\n            return\n\n        if not self.asked_opt_in:\n            self.disable(False)\n            return\n\n        # self.mp = Mixpanel(mixpanel_project_token)\n        self.ph = Posthog(\n            project_api_key=posthog_project_api_key,\n            host=posthog_host,\n            on_error=self.posthog_error,\n            enable_exception_autocapture=True,\n            super_properties=self.get_system_info(),  # Add system info to all events\n        )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/analytics.py::4",
    "metadata": {
      "file_path": "aider/analytics.py",
      "file_name": "analytics.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 689,
      "span_ids": [
        "Analytics.load_data",
        "Analytics.posthog_error",
        "Analytics.get_or_create_uuid",
        "Analytics.get_system_info",
        "Analytics.need_to_ask",
        "Analytics.get_data_file_path",
        "Analytics.save_data",
        "Analytics._redact_model_name",
        "Analytics.disable"
      ],
      "start_line": 102,
      "end_line": 203,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Analytics:\n\n    def disable(self, permanently):\n        self.mp = None\n        self.ph = None\n\n        if permanently:\n            self.asked_opt_in = True\n            self.permanently_disable = True\n            self.save_data()\n\n    def need_to_ask(self, args_analytics):\n        if args_analytics is False:\n            return False\n\n        could_ask = not self.asked_opt_in and not self.permanently_disable\n        if not could_ask:\n            return False\n\n        if args_analytics is True:\n            return True\n\n        assert args_analytics is None, args_analytics\n\n        if not self.user_id:\n            return False\n\n        return is_uuid_in_percentage(self.user_id, PERCENT)\n\n    def get_data_file_path(self):\n        try:\n            data_file = Path.home() / \".aider\" / \"analytics.json\"\n            data_file.parent.mkdir(parents=True, exist_ok=True)\n            return data_file\n        except OSError:\n            # If we can't create/access the directory, just disable analytics\n            self.disable(permanently=False)\n            return None\n\n    def get_or_create_uuid(self):\n        self.load_data()\n        if self.user_id:\n            return\n\n        self.user_id = str(uuid.uuid4())\n        self.save_data()\n\n    def load_data(self):\n        data_file = self.get_data_file_path()\n        if not data_file:\n            return\n\n        if data_file.exists():\n            try:\n                data = json.loads(data_file.read_text())\n                self.permanently_disable = data.get(\"permanently_disable\")\n                self.user_id = data.get(\"uuid\")\n                self.asked_opt_in = data.get(\"asked_opt_in\", False)\n            except (json.decoder.JSONDecodeError, OSError):\n                self.disable(permanently=False)\n\n    def save_data(self):\n        data_file = self.get_data_file_path()\n        if not data_file:\n            return\n\n        data = dict(\n            uuid=self.user_id,\n            permanently_disable=self.permanently_disable,\n            asked_opt_in=self.asked_opt_in,\n        )\n\n        try:\n            data_file.write_text(json.dumps(data, indent=4))\n        except OSError:\n            # If we can't write the file, just disable analytics\n            self.disable(permanently=False)\n\n    def get_system_info(self):\n        return {\n            \"python_version\": sys.version.split()[0],\n            \"os_platform\": platform.system(),\n            \"os_release\": platform.release(),\n            \"machine\": platform.machine(),\n            \"aider_version\": __version__,\n        }\n\n    def _redact_model_name(self, model):\n        if not model:\n            return None\n\n        info = model_info_manager.get_model_from_cached_json_db(model.name)\n        if info:\n            return model.name\n        elif \"/\" in model.name:\n            return model.name.split(\"/\")[0] + \"/REDACTED\"\n        return None\n\n    def posthog_error(self):\n        \"\"\"disable posthog if we get an error\"\"\"\n        print(\"X\" * 100)\n        # https://github.com/PostHog/posthog-python/blob/9e1bb8c58afaa229da24c4fb576c08bb88a75752/posthog/consumer.py#L86\n        # https://github.com/Aider-AI/aider/issues/2532\n        self.ph = None",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/analytics.py::5",
    "metadata": {
      "file_path": "aider/analytics.py",
      "file_name": "analytics.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 316,
      "span_ids": [
        "Analytics.event",
        "impl:9"
      ],
      "start_line": 205,
      "end_line": 251,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Analytics:\n\n    def event(self, event_name, main_model=None, **kwargs):\n        if not self.mp and not self.ph and not self.logfile:\n            return\n\n        properties = {}\n\n        if main_model:\n            properties[\"main_model\"] = self._redact_model_name(main_model)\n            properties[\"weak_model\"] = self._redact_model_name(main_model.weak_model)\n            properties[\"editor_model\"] = self._redact_model_name(main_model.editor_model)\n\n        properties.update(kwargs)\n\n        # Handle numeric values\n        for key, value in properties.items():\n            if isinstance(value, (int, float)):\n                properties[key] = value\n            else:\n                properties[key] = str(value)\n\n        if self.mp:\n            try:\n                self.mp.track(self.user_id, event_name, dict(properties))\n            except MixpanelException:\n                self.mp = None  # Disable mixpanel on connection errors\n\n        if self.ph:\n            self.ph.capture(self.user_id, event_name, dict(properties))\n\n        if self.logfile:\n            log_entry = {\n                \"event\": event_name,\n                \"properties\": properties,\n                \"user_id\": self.user_id,\n                \"time\": int(time.time()),\n            }\n            try:\n                with open(self.logfile, \"a\") as f:\n                    json.dump(log_entry, f)\n                    f.write(\"\\n\")\n            except OSError:\n                pass  # Ignore OS errors when writing to logfile\n\n\nif __name__ == \"__main__\":\n    dump(compute_hex_threshold(PERCENT))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/args.py::1",
    "metadata": {
      "file_path": "aider/args.py",
      "file_name": "args.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 909,
      "span_ids": [
        "default_env_file",
        "get_parser",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 117,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "#!/usr/bin/env python\n\nimport argparse\nimport os\nimport sys\n\nimport configargparse\n\nfrom aider import __version__\nfrom aider.args_formatter import (\n    DotEnvFormatter,\n    MarkdownHelpFormatter,\n    YamlHelpFormatter,\n)\n\nfrom .dump import dump  # noqa: F401\n\n\ndef default_env_file(git_root):\n    return os.path.join(git_root, \".env\") if git_root else \".env\"\n\n\ndef get_parser(default_config_files, git_root):\n    parser = configargparse.ArgumentParser(\n        description=\"aider is AI pair programming in your terminal\",\n        add_config_file_help=True,\n        default_config_files=default_config_files,\n        config_file_parser_class=configargparse.YAMLConfigFileParser,\n        auto_env_var_prefix=\"AIDER_\",\n    )\n    group = parser.add_argument_group(\"Main model\")\n    group.add_argument(\n        \"files\", metavar=\"FILE\", nargs=\"*\", help=\"files to edit with an LLM (optional)\"\n    )\n    group.add_argument(\n        \"--model\",\n        metavar=\"MODEL\",\n        default=None,\n        help=\"Specify the model to use for the main chat\",\n    )\n    opus_model = \"claude-3-opus-20240229\"\n    group.add_argument(\n        \"--opus\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=opus_model,\n        help=f\"Use {opus_model} model for the main chat\",\n    )\n    sonnet_model = \"claude-3-5-sonnet-20241022\"\n    group.add_argument(\n        \"--sonnet\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=sonnet_model,\n        help=f\"Use {sonnet_model} model for the main chat\",\n    )\n    haiku_model = \"claude-3-5-haiku-20241022\"\n    group.add_argument(\n        \"--haiku\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=haiku_model,\n        help=f\"Use {haiku_model} model for the main chat\",\n    )\n    gpt_4_model = \"gpt-4-0613\"\n    group.add_argument(\n        \"--4\",\n        \"-4\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=gpt_4_model,\n        help=f\"Use {gpt_4_model} model for the main chat\",\n    )\n    gpt_4o_model = \"gpt-4o\"\n    group.add_argument(\n        \"--4o\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=gpt_4o_model,\n        help=f\"Use {gpt_4o_model} model for the main chat\",\n    )\n    gpt_4o_mini_model = \"gpt-4o-mini\"\n    group.add_argument(\n        \"--mini\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=gpt_4o_mini_model,\n        help=f\"Use {gpt_4o_mini_model} model for the main chat\",\n    )\n    gpt_4_turbo_model = \"gpt-4-1106-preview\"\n    group.add_argument(\n        \"--4-turbo\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=gpt_4_turbo_model,\n        help=f\"Use {gpt_4_turbo_model} model for the main chat\",\n    )\n    gpt_3_model_name = \"gpt-3.5-turbo\"\n    group.add_argument(\n        \"--35turbo\",\n        \"--35-turbo\",\n        \"--3\",\n        \"-3\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=gpt_3_model_name,\n        help=f\"Use {gpt_3_model_name} model for the main chat\",\n    )\n    deepseek_model = \"deepseek/deepseek-chat\"\n    group.add_argument(\n        \"--deepseek\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=deepseek_model,\n        help=f\"Use {deepseek_model} model for the main chat\",\n    )\n    o1_mini_model = \"o1-mini\"\n\n    ##########\n\n    ##########\n\n    ##########\n\n    ##########\n\n    ##########\n\n    ##########\n\n    #########\n\n    ##########\n\n    ##########\n\n    ######\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/args.py::2",
    "metadata": {
      "file_path": "aider/args.py",
      "file_name": "args.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 755,
      "span_ids": [
        "get_parser"
      ],
      "start_line": 118,
      "end_line": 224,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_parser(default_config_files, git_root):\n    # ... other code\n    group.add_argument(\n        \"--o1-mini\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=o1_mini_model,\n        help=f\"Use {o1_mini_model} model for the main chat\",\n    )\n    o1_preview_model = \"o1-preview\"\n    group.add_argument(\n        \"--o1-preview\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=o1_preview_model,\n        help=f\"Use {o1_preview_model} model for the main chat\",\n    )\n\n    ##########\n    group = parser.add_argument_group(\"API Keys and settings\")\n    group.add_argument(\n        \"--openai-api-key\",\n        help=\"Specify the OpenAI API key\",\n    )\n    group.add_argument(\n        \"--anthropic-api-key\",\n        help=\"Specify the Anthropic API key\",\n    )\n    group.add_argument(\n        \"--openai-api-base\",\n        help=\"Specify the api base url\",\n    )\n    group.add_argument(\n        \"--openai-api-type\",\n        help=\"(deprecated, use --set-env OPENAI_API_TYPE=<value>)\",\n    )\n    group.add_argument(\n        \"--openai-api-version\",\n        help=\"(deprecated, use --set-env OPENAI_API_VERSION=<value>)\",\n    )\n    group.add_argument(\n        \"--openai-api-deployment-id\",\n        help=\"(deprecated, use --set-env OPENAI_API_DEPLOYMENT_ID=<value>)\",\n    )\n    group.add_argument(\n        \"--openai-organization-id\",\n        help=\"(deprecated, use --set-env OPENAI_ORGANIZATION=<value>)\",\n    )\n    group.add_argument(\n        \"--set-env\",\n        action=\"append\",\n        metavar=\"ENV_VAR_NAME=value\",\n        help=\"Set an environment variable (to control API settings, can be used multiple times)\",\n        default=[],\n    )\n    group.add_argument(\n        \"--api-key\",\n        action=\"append\",\n        metavar=\"PROVIDER=KEY\",\n        help=(\n            \"Set an API key for a provider (eg: --api-key provider=<key> sets\"\n            \" PROVIDER_API_KEY=<key>)\"\n        ),\n        default=[],\n    )\n    group = parser.add_argument_group(\"Model settings\")\n    group.add_argument(\n        \"--list-models\",\n        \"--models\",\n        metavar=\"MODEL\",\n        help=\"List known models which match the (partial) MODEL name\",\n    )\n    group.add_argument(\n        \"--model-settings-file\",\n        metavar=\"MODEL_SETTINGS_FILE\",\n        default=\".aider.model.settings.yml\",\n        help=\"Specify a file with aider model settings for unknown models\",\n    )\n    group.add_argument(\n        \"--model-metadata-file\",\n        metavar=\"MODEL_METADATA_FILE\",\n        default=\".aider.model.metadata.json\",\n        help=\"Specify a file with context window and costs for unknown models\",\n    )\n    group.add_argument(\n        \"--alias\",\n        action=\"append\",\n        metavar=\"ALIAS:MODEL\",\n        help=\"Add a model alias (can be used multiple times)\",\n    )\n    group.add_argument(\n        \"--verify-ssl\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Verify the SSL cert when connecting to models (default: True)\",\n    )\n    group.add_argument(\n        \"--timeout\",\n        type=int,\n        default=None,\n        help=\"Timeout in seconds for API calls (default: None)\",\n    )\n    group.add_argument(\n        \"--edit-format\",\n        \"--chat-mode\",\n        metavar=\"EDIT_FORMAT\",\n        default=None,\n        help=\"Specify what edit format the LLM should use (default depends on model)\",\n    )\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/args.py::3",
    "metadata": {
      "file_path": "aider/args.py",
      "file_name": "args.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 888,
      "span_ids": [
        "get_parser"
      ],
      "start_line": 225,
      "end_line": 327,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_parser(default_config_files, git_root):\n    # ... other code\n    group.add_argument(\n        \"--model\",\n        metavar=\"MODEL\",\n        default=None,\n        help=\"Specify the model to use for the main chat\",\n    )\n    # ... other code\n    group.add_argument(\n        \"--opus\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=opus_model,\n        help=f\"Use {opus_model} model for the main chat\",\n    )\n    # ... other code\n    group.add_argument(\n        \"--sonnet\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=sonnet_model,\n        help=f\"Use {sonnet_model} model for the main chat\",\n    )\n    # ... other code\n    group.add_argument(\n        \"--architect\",\n        action=\"store_const\",\n        dest=\"edit_format\",\n        const=\"architect\",\n        help=\"Use architect edit format for the main chat\",\n    )\n    group.add_argument(\n        \"--weak-model\",\n        metavar=\"WEAK_MODEL\",\n        default=None,\n        help=(\n            \"Specify the model to use for commit messages and chat history summarization (default\"\n            \" depends on --model)\"\n        ),\n    )\n    group.add_argument(\n        \"--editor-model\",\n        metavar=\"EDITOR_MODEL\",\n        default=None,\n        help=\"Specify the model to use for editor tasks (default depends on --model)\",\n    )\n    group.add_argument(\n        \"--editor-edit-format\",\n        metavar=\"EDITOR_EDIT_FORMAT\",\n        default=None,\n        help=\"Specify the edit format for the editor model (default: depends on editor model)\",\n    )\n    group.add_argument(\n        \"--show-model-warnings\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Only work with models that have meta-data available (default: True)\",\n    )\n    group.add_argument(\n        \"--max-chat-history-tokens\",\n        type=int,\n        default=None,\n        help=(\n            \"Soft limit on tokens for chat history, after which summarization begins.\"\n            \" If unspecified, defaults to the model's max_chat_history_tokens.\"\n        ),\n    )\n\n    ##########\n    group = parser.add_argument_group(\"Cache settings\")\n    group.add_argument(\n        \"--cache-prompts\",\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help=\"Enable caching of prompts (default: False)\",\n    )\n    group.add_argument(\n        \"--cache-keepalive-pings\",\n        type=int,\n        default=0,\n        help=\"Number of times to ping at 5min intervals to keep prompt cache warm (default: 0)\",\n    )\n\n    ##########\n    group = parser.add_argument_group(\"Repomap settings\")\n    group.add_argument(\n        \"--map-tokens\",\n        type=int,\n        default=None,\n        help=\"Suggested number of tokens to use for repo map, use 0 to disable\",\n    )\n    group.add_argument(\n        \"--map-refresh\",\n        choices=[\"auto\", \"always\", \"files\", \"manual\"],\n        default=\"auto\",\n        help=(\n            \"Control how often the repo map is refreshed. Options: auto, always, files, manual\"\n            \" (default: auto)\"\n        ),\n    )\n    group.add_argument(\n        \"--map-multiplier-no-files\",\n        type=float,\n        default=2,\n        help=\"Multiplier for map tokens when no files are specified (default: 2)\",\n    )\n\n    ##########\n    group = parser.add_argument_group(\"History Files\")\n    default_input_history_file = (\n        os.path.join(git_root, \".aider.input.history\") if git_root else \".aider.input.history\"\n    )\n    default_chat_history_file = (\n        os.path.join(git_root, \".aider.chat.history.md\") if git_root else \".aider.chat.history.md\"\n    )\n    group.add_argument(\n        \"--input-history-file\",\n        metavar=\"INPUT_HISTORY_FILE\",\n        default=default_input_history_file,\n        help=f\"Specify the chat input history file (default: {default_input_history_file})\",\n    )\n    group.add_argument(\n        \"--chat-history-file\",\n        metavar=\"CHAT_HISTORY_FILE\",\n        default=default_chat_history_file,\n        help=f\"Specify the chat history file (default: {default_chat_history_file})\",\n    )\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/args.py::4",
    "metadata": {
      "file_path": "aider/args.py",
      "file_name": "args.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 888,
      "span_ids": [
        "get_parser"
      ],
      "start_line": 328,
      "end_line": 441,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_parser(default_config_files, git_root):\n    # ... other code\n    group.add_argument(\n        \"--haiku\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=haiku_model,\n        help=f\"Use {haiku_model} model for the main chat\",\n    )\n    # ... other code\n    group.add_argument(\n        \"--4\",\n        \"-4\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=gpt_4_model,\n        help=f\"Use {gpt_4_model} model for the main chat\",\n    )\n    # ... other code\n    group.add_argument(\n        \"--restore-chat-history\",\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help=\"Restore the previous chat history messages (default: False)\",\n    )\n    group.add_argument(\n        \"--llm-history-file\",\n        metavar=\"LLM_HISTORY_FILE\",\n        default=None,\n        help=\"Log the conversation with the LLM to this file (for example, .aider.llm.history)\",\n    )\n\n    ##########\n    group = parser.add_argument_group(\"Output settings\")\n    group.add_argument(\n        \"--dark-mode\",\n        action=\"store_true\",\n        help=\"Use colors suitable for a dark terminal background (default: False)\",\n        default=False,\n    )\n    group.add_argument(\n        \"--light-mode\",\n        action=\"store_true\",\n        help=\"Use colors suitable for a light terminal background (default: False)\",\n        default=False,\n    )\n    group.add_argument(\n        \"--pretty\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable pretty, colorized output (default: True)\",\n    )\n    group.add_argument(\n        \"--stream\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable streaming responses (default: True)\",\n    )\n    group.add_argument(\n        \"--user-input-color\",\n        default=\"#00cc00\",\n        help=\"Set the color for user input (default: #00cc00)\",\n    )\n    group.add_argument(\n        \"--tool-output-color\",\n        default=None,\n        help=\"Set the color for tool output (default: None)\",\n    )\n    group.add_argument(\n        \"--tool-error-color\",\n        default=\"#FF2222\",\n        help=\"Set the color for tool error messages (default: #FF2222)\",\n    )\n    group.add_argument(\n        \"--tool-warning-color\",\n        default=\"#FFA500\",\n        help=\"Set the color for tool warning messages (default: #FFA500)\",\n    )\n    group.add_argument(\n        \"--assistant-output-color\",\n        default=\"#0088ff\",\n        help=\"Set the color for assistant output (default: #0088ff)\",\n    )\n    group.add_argument(\n        \"--completion-menu-color\",\n        metavar=\"COLOR\",\n        default=None,\n        help=\"Set the color for the completion menu (default: terminal's default text color)\",\n    )\n    group.add_argument(\n        \"--completion-menu-bg-color\",\n        metavar=\"COLOR\",\n        default=None,\n        help=(\n            \"Set the background color for the completion menu (default: terminal's default\"\n            \" background color)\"\n        ),\n    )\n    group.add_argument(\n        \"--completion-menu-current-color\",\n        metavar=\"COLOR\",\n        default=None,\n        help=(\n            \"Set the color for the current item in the completion menu (default: terminal's default\"\n            \" background color)\"\n        ),\n    )\n    group.add_argument(\n        \"--completion-menu-current-bg-color\",\n        metavar=\"COLOR\",\n        default=None,\n        help=(\n            \"Set the background color for the current item in the completion menu (default:\"\n            \" terminal's default text color)\"\n        ),\n    )\n    group.add_argument(\n        \"--code-theme\",\n        default=\"default\",\n        help=(\n            \"Set the markdown code theme (default: default, other options include monokai,\"\n            \" solarized-dark, solarized-light, or a Pygments builtin style,\"\n            \" see https://pygments.org/styles for available themes)\"\n        ),\n    )\n    group.add_argument(\n        \"--show-diffs\",\n        action=\"store_true\",\n        help=\"Show diffs when committing changes (default: False)\",\n        default=False,\n    )\n\n    ##########\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/args.py::5",
    "metadata": {
      "file_path": "aider/args.py",
      "file_name": "args.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 738,
      "span_ids": [
        "get_parser"
      ],
      "start_line": 442,
      "end_line": 541,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_parser(default_config_files, git_root):\n    # ... other code\n    group = parser.add_argument_group(\"Git settings\")\n    group.add_argument(\n        \"--git\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable looking for a git repo (default: True)\",\n    )\n    group.add_argument(\n        \"--gitignore\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable adding .aider* to .gitignore (default: True)\",\n    )\n    default_aiderignore_file = (\n        os.path.join(git_root, \".aiderignore\") if git_root else \".aiderignore\"\n    )\n    group.add_argument(\n        \"--aiderignore\",\n        metavar=\"AIDERIGNORE\",\n        default=default_aiderignore_file,\n        help=\"Specify the aider ignore file (default: .aiderignore in git root)\",\n    )\n    group.add_argument(\n        \"--subtree-only\",\n        action=\"store_true\",\n        help=\"Only consider files in the current subtree of the git repository\",\n        default=False,\n    )\n    group.add_argument(\n        \"--auto-commits\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable auto commit of LLM changes (default: True)\",\n    )\n    group.add_argument(\n        \"--dirty-commits\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable commits when repo is found dirty (default: True)\",\n    )\n    group.add_argument(\n        \"--attribute-author\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Attribute aider code changes in the git author name (default: True)\",\n    )\n    group.add_argument(\n        \"--attribute-committer\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Attribute aider commits in the git committer name (default: True)\",\n    )\n    group.add_argument(\n        \"--attribute-commit-message-author\",\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help=\"Prefix commit messages with 'aider: ' if aider authored the changes (default: False)\",\n    )\n    group.add_argument(\n        \"--attribute-commit-message-committer\",\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help=\"Prefix all commit messages with 'aider: ' (default: False)\",\n    )\n    group.add_argument(\n        \"--commit\",\n        action=\"store_true\",\n        help=\"Commit all pending changes with a suitable commit message, then exit\",\n        default=False,\n    )\n    group.add_argument(\n        \"--commit-prompt\",\n        metavar=\"PROMPT\",\n        help=\"Specify a custom prompt for generating commit messages\",\n    )\n    group.add_argument(\n        \"--dry-run\",\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help=\"Perform a dry run without modifying files (default: False)\",\n    )\n    group.add_argument(\n        \"--skip-sanity-check-repo\",\n        action=\"store_true\",\n        help=\"Skip the sanity check for the git repository (default: False)\",\n        default=False,\n    )\n    group.add_argument(\n        \"--watch-files\",\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help=\"Enable/disable watching files for ai coding comments (default: False)\",\n    )\n    group = parser.add_argument_group(\"Fixing and committing\")\n    group.add_argument(\n        \"--lint\",\n        action=\"store_true\",\n        help=\"Lint and fix provided files, or dirty files if none provided\",\n        default=False,\n    )\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/args.py::6",
    "metadata": {
      "file_path": "aider/args.py",
      "file_name": "args.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 927,
      "span_ids": [
        "get_parser"
      ],
      "start_line": 542,
      "end_line": 661,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_parser(default_config_files, git_root):\n    # ... other code\n    group.add_argument(\n        \"--4o\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=gpt_4o_model,\n        help=f\"Use {gpt_4o_model} model for the main chat\",\n    )\n    # ... other code\n    group.add_argument(\n        \"--mini\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=gpt_4o_mini_model,\n        help=f\"Use {gpt_4o_mini_model} model for the main chat\",\n    )\n    # ... other code\n    group.add_argument(\n        \"--4-turbo\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=gpt_4_turbo_model,\n        help=f\"Use {gpt_4_turbo_model} model for the main chat\",\n    )\n    # ... other code\n    group.add_argument(\n        \"--lint-cmd\",\n        action=\"append\",\n        help=(\n            'Specify lint commands to run for different languages, eg: \"python: flake8'\n            ' --select=...\" (can be used multiple times)'\n        ),\n        default=[],\n    )\n    group.add_argument(\n        \"--auto-lint\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable automatic linting after changes (default: True)\",\n    )\n    group.add_argument(\n        \"--test-cmd\",\n        help=\"Specify command to run tests\",\n        default=[],\n    )\n    group.add_argument(\n        \"--auto-test\",\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help=\"Enable/disable automatic testing after changes (default: False)\",\n    )\n    group.add_argument(\n        \"--test\",\n        action=\"store_true\",\n        help=\"Run tests, fix problems found and then exit\",\n        default=False,\n    )\n\n    ##########\n    group = parser.add_argument_group(\"Analytics\")\n    group.add_argument(\n        \"--analytics\",\n        action=argparse.BooleanOptionalAction,\n        default=None,\n        help=\"Enable/disable analytics for current session (default: random)\",\n    )\n    group.add_argument(\n        \"--analytics-log\",\n        metavar=\"ANALYTICS_LOG_FILE\",\n        help=\"Specify a file to log analytics events\",\n    )\n    group.add_argument(\n        \"--analytics-disable\",\n        action=\"store_true\",\n        help=\"Permanently disable analytics\",\n        default=False,\n    )\n\n    #########\n    group = parser.add_argument_group(\"Upgrading\")\n    group.add_argument(\n        \"--just-check-update\",\n        action=\"store_true\",\n        help=\"Check for updates and return status in the exit code\",\n        default=False,\n    )\n    group.add_argument(\n        \"--check-update\",\n        action=argparse.BooleanOptionalAction,\n        help=\"Check for new aider versions on launch\",\n        default=True,\n    )\n    group.add_argument(\n        \"--show-release-notes\",\n        action=argparse.BooleanOptionalAction,\n        help=\"Show release notes on first run of new version (default: None, ask user)\",\n        default=None,\n    )\n    group.add_argument(\n        \"--install-main-branch\",\n        action=\"store_true\",\n        help=\"Install the latest version from the main branch\",\n        default=False,\n    )\n    group.add_argument(\n        \"--upgrade\",\n        \"--update\",\n        action=\"store_true\",\n        help=\"Upgrade aider to the latest version from PyPI\",\n        default=False,\n    )\n    group.add_argument(\n        \"--version\",\n        action=\"version\",\n        version=f\"%(prog)s {__version__}\",\n        help=\"Show the version number and exit\",\n    )\n\n    ##########\n    group = parser.add_argument_group(\"Modes\")\n    group.add_argument(\n        \"--message\",\n        \"--msg\",\n        \"-m\",\n        metavar=\"COMMAND\",\n        help=(\n            \"Specify a single message to send the LLM, process reply then exit (disables chat mode)\"\n        ),\n    )\n    group.add_argument(\n        \"--message-file\",\n        \"-f\",\n        metavar=\"MESSAGE_FILE\",\n        help=(\n            \"Specify a file containing the message to send the LLM, process reply, then exit\"\n            \" (disables chat mode)\"\n        ),\n    )\n    group.add_argument(\n        \"--gui\",\n        \"--browser\",\n        action=argparse.BooleanOptionalAction,\n        help=\"Run aider in your browser (default: False)\",\n        default=False,\n    )\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/args.py::7",
    "metadata": {
      "file_path": "aider/args.py",
      "file_name": "args.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 870,
      "span_ids": [
        "get_parser"
      ],
      "start_line": 662,
      "end_line": 774,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_parser(default_config_files, git_root):\n    # ... other code\n    group.add_argument(\n        \"--35turbo\",\n        \"--35-turbo\",\n        \"--3\",\n        \"-3\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=gpt_3_model_name,\n        help=f\"Use {gpt_3_model_name} model for the main chat\",\n    )\n    # ... other code\n    group.add_argument(\n        \"--deepseek\",\n        action=\"store_const\",\n        dest=\"model\",\n        const=deepseek_model,\n        help=f\"Use {deepseek_model} model for the main chat\",\n    )\n    # ... other code\n    group.add_argument(\n        \"--copy-paste\",\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help=\"Enable automatic copy/paste of chat between aider and web UI (default: False)\",\n    )\n    group.add_argument(\n        \"--apply\",\n        metavar=\"FILE\",\n        help=\"Apply the changes from the given file instead of running the chat (debug)\",\n    )\n    group.add_argument(\n        \"--apply-clipboard-edits\",\n        action=\"store_true\",\n        help=\"Apply clipboard contents as edits using the main model's editor format\",\n        default=False,\n    )\n    group.add_argument(\n        \"--exit\",\n        action=\"store_true\",\n        help=\"Do all startup activities then exit before accepting user input (debug)\",\n        default=False,\n    )\n    group.add_argument(\n        \"--show-repo-map\",\n        action=\"store_true\",\n        help=\"Print the repo map and exit (debug)\",\n        default=False,\n    )\n    group.add_argument(\n        \"--show-prompts\",\n        action=\"store_true\",\n        help=\"Print the system prompts and exit (debug)\",\n        default=False,\n    )\n\n    ##########\n    group = parser.add_argument_group(\"Voice settings\")\n    group.add_argument(\n        \"--voice-format\",\n        metavar=\"VOICE_FORMAT\",\n        default=\"wav\",\n        choices=[\"wav\", \"mp3\", \"webm\"],\n        help=\"Audio format for voice recording (default: wav). webm and mp3 require ffmpeg\",\n    )\n    group.add_argument(\n        \"--voice-language\",\n        metavar=\"VOICE_LANGUAGE\",\n        default=\"en\",\n        help=\"Specify the language for voice using ISO 639-1 code (default: auto)\",\n    )\n    group.add_argument(\n        \"--voice-input-device\",\n        metavar=\"VOICE_INPUT_DEVICE\",\n        default=None,\n        help=\"Specify the input device name for voice recording\",\n    )\n\n    ######\n    group = parser.add_argument_group(\"Other settings\")\n    group.add_argument(\n        \"--file\",\n        action=\"append\",\n        metavar=\"FILE\",\n        help=\"specify a file to edit (can be used multiple times)\",\n    )\n    group.add_argument(\n        \"--read\",\n        action=\"append\",\n        metavar=\"FILE\",\n        help=\"specify a read-only file (can be used multiple times)\",\n    )\n    group.add_argument(\n        \"--vim\",\n        action=\"store_true\",\n        help=\"Use VI editing mode in the terminal (default: False)\",\n        default=False,\n    )\n    group.add_argument(\n        \"--chat-language\",\n        metavar=\"CHAT_LANGUAGE\",\n        default=None,\n        help=\"Specify the language to use in the chat (default: None, uses system settings)\",\n    )\n    group.add_argument(\n        \"--yes-always\",\n        action=\"store_true\",\n        help=\"Always say yes to every confirmation\",\n        default=None,\n    )\n    group.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"store_true\",\n        help=\"Enable verbose output\",\n        default=False,\n    )\n    group.add_argument(\n        \"--load\",\n        metavar=\"LOAD_FILE\",\n        help=\"Load and execute /commands from a file on launch\",\n    )\n    group.add_argument(\n        \"--encoding\",\n        default=\"utf-8\",\n        help=\"Specify the encoding for input and output (default: utf-8)\",\n    )\n    group.add_argument(\n        \"--line-endings\",\n        choices=[\"platform\", \"lf\", \"crlf\"],\n        default=\"platform\",\n        help=\"Line endings to use when writing files (default: platform)\",\n    )\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/args.py::8",
    "metadata": {
      "file_path": "aider/args.py",
      "file_name": "args.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 363,
      "span_ids": [
        "get_parser"
      ],
      "start_line": 775,
      "end_line": 822,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_parser(default_config_files, git_root):\n    # ... other code\n    group.add_argument(\n        \"-c\",\n        \"--config\",\n        is_config_file=True,\n        metavar=\"CONFIG_FILE\",\n        help=(\n            \"Specify the config file (default: search for .aider.conf.yml in git root, cwd\"\n            \" or home directory)\"\n        ),\n    )\n    # This is a duplicate of the argument in the preparser and is a no-op by this time of\n    # argument parsing, but it's here so that the help is displayed as expected.\n    group.add_argument(\n        \"--env-file\",\n        metavar=\"ENV_FILE\",\n        default=default_env_file(git_root),\n        help=\"Specify the .env file to load (default: .env in git root)\",\n    )\n    group.add_argument(\n        \"--suggest-shell-commands\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable suggesting shell commands (default: True)\",\n    )\n    group.add_argument(\n        \"--fancy-input\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable fancy input with history and completion (default: True)\",\n    )\n    group.add_argument(\n        \"--multiline\",\n        action=argparse.BooleanOptionalAction,\n        default=False,\n        help=\"Enable/disable multi-line input mode with Meta-Enter to submit (default: False)\",\n    )\n    group.add_argument(\n        \"--detect-urls\",\n        action=argparse.BooleanOptionalAction,\n        default=True,\n        help=\"Enable/disable detection and offering to add URLs to chat (default: True)\",\n    )\n    group.add_argument(\n        \"--editor\",\n        help=\"Specify which editor to use for the /editor command\",\n    )\n\n    return parser",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/args.py::9",
    "metadata": {
      "file_path": "aider/args.py",
      "file_name": "args.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 281,
      "span_ids": [
        "get_sample_yaml",
        "get_sample_dotenv",
        "main",
        "get_md_help",
        "impl"
      ],
      "start_line": 825,
      "end_line": 878,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_md_help():\n    os.environ[\"COLUMNS\"] = \"70\"\n    sys.argv = [\"aider\"]\n    parser = get_parser([], None)\n\n    # This instantiates all the action.env_var values\n    parser.parse_known_args()\n\n    parser.formatter_class = MarkdownHelpFormatter\n\n    return argparse.ArgumentParser.format_help(parser)\n\n\ndef get_sample_yaml():\n    os.environ[\"COLUMNS\"] = \"100\"\n    sys.argv = [\"aider\"]\n    parser = get_parser([], None)\n\n    # This instantiates all the action.env_var values\n    parser.parse_known_args()\n\n    parser.formatter_class = YamlHelpFormatter\n\n    return argparse.ArgumentParser.format_help(parser)\n\n\ndef get_sample_dotenv():\n    os.environ[\"COLUMNS\"] = \"120\"\n    sys.argv = [\"aider\"]\n    parser = get_parser([], None)\n\n    # This instantiates all the action.env_var values\n    parser.parse_known_args()\n\n    parser.formatter_class = DotEnvFormatter\n\n    return argparse.ArgumentParser.format_help(parser)\n\n\ndef main():\n    arg = sys.argv[1] if len(sys.argv[1:]) else None\n\n    if arg == \"md\":\n        print(get_md_help())\n    elif arg == \"dotenv\":\n        print(get_sample_dotenv())\n    else:\n        print(get_sample_yaml())\n\n\nif __name__ == \"__main__\":\n    status = main()\n    sys.exit(status)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/args_formatter.py::1",
    "metadata": {
      "file_path": "aider/args_formatter.py",
      "file_name": "args_formatter.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 205,
      "span_ids": [
        "DotEnvFormatter.start_section",
        "DotEnvFormatter._format_usage",
        "DotEnvFormatter",
        "DotEnvFormatter._format_text",
        "imports"
      ],
      "start_line": 1,
      "end_line": 39,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import argparse\n\nfrom aider import urls\n\nfrom .dump import dump  # noqa: F401\n\n\nclass DotEnvFormatter(argparse.HelpFormatter):\n    def start_section(self, heading):\n        res = \"\\n\\n\"\n        res += \"#\" * (len(heading) + 3)\n        res += f\"\\n# {heading}\"\n        super().start_section(res)\n\n    def _format_usage(self, usage, actions, groups, prefix):\n        return \"\"\n\n    def _format_text(self, text):\n        return f\"\"\"\n##########################################################\n# Sample aider .env file.\n# Place at the root of your git repo.\n# Or use `aider --env <fname>` to specify.\n##########################################################\n\n#################\n# LLM parameters:\n#\n# Include xxx_API_KEY parameters and other params needed for your LLMs.\n# See {urls.llms} for details.\n\n## OpenAI\n#OPENAI_API_KEY=\n\n## Anthropic\n#ANTHROPIC_API_KEY=\n\n##...\n\"\"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/args_formatter.py::2",
    "metadata": {
      "file_path": "aider/args_formatter.py",
      "file_name": "args_formatter.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 218,
      "span_ids": [
        "DotEnvFormatter._format_action",
        "DotEnvFormatter._format_action_invocation",
        "DotEnvFormatter._format_args"
      ],
      "start_line": 41,
      "end_line": 78,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class DotEnvFormatter(argparse.HelpFormatter):\n\n    def _format_action(self, action):\n        if not action.option_strings:\n            return \"\"\n\n        if not action.env_var:\n            return\n\n        parts = [\"\"]\n\n        default = action.default\n        if default == argparse.SUPPRESS:\n            default = \"\"\n        elif isinstance(default, str):\n            pass\n        elif isinstance(default, list) and not default:\n            default = \"\"\n        elif action.default is not None:\n            default = \"true\" if default else \"false\"\n        else:\n            default = \"\"\n\n        if action.help:\n            parts.append(f\"## {action.help}\")\n\n        if action.env_var:\n            env_var = action.env_var\n            if default:\n                parts.append(f\"#{env_var}={default}\\n\")\n            else:\n                parts.append(f\"#{env_var}=\\n\")\n\n        return \"\\n\".join(parts) + \"\\n\"\n\n    def _format_action_invocation(self, action):\n        return \"\"\n\n    def _format_args(self, action, default_metavar):\n        return \"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/args_formatter.py::3",
    "metadata": {
      "file_path": "aider/args_formatter.py",
      "file_name": "args_formatter.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 177,
      "span_ids": [
        "YamlHelpFormatter.start_section",
        "YamlHelpFormatter._format_text",
        "YamlHelpFormatter._format_usage",
        "YamlHelpFormatter"
      ],
      "start_line": 81,
      "end_line": 103,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class YamlHelpFormatter(argparse.HelpFormatter):\n    def start_section(self, heading):\n        res = \"\\n\\n\"\n        res += \"#\" * (len(heading) + 3)\n        res += f\"\\n# {heading}\"\n        super().start_section(res)\n\n    def _format_usage(self, usage, actions, groups, prefix):\n        return \"\"\n\n    def _format_text(self, text):\n        return \"\"\"\n##########################################################\n# Sample .aider.conf.yml\n# This file lists *all* the valid configuration entries.\n# Place in your home dir, or at the root of your git repo.\n##########################################################\n\n# Note: You can only put OpenAI and Anthropic API keys in the yaml\n# config file. Keys for all APIs can be stored in a .env file\n# https://aider.chat/docs/config/dotenv.html\n\n\"\"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/args_formatter.py::4",
    "metadata": {
      "file_path": "aider/args_formatter.py",
      "file_name": "args_formatter.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 389,
      "span_ids": [
        "YamlHelpFormatter._format_action",
        "YamlHelpFormatter._format_args",
        "YamlHelpFormatter._format_action_invocation"
      ],
      "start_line": 105,
      "end_line": 166,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class YamlHelpFormatter(argparse.HelpFormatter):\n\n    def _format_action(self, action):\n        if not action.option_strings:\n            return \"\"\n\n        parts = [\"\"]\n\n        metavar = action.metavar\n        if not metavar and isinstance(action, argparse._StoreAction):\n            metavar = \"VALUE\"\n\n        default = action.default\n        if default == argparse.SUPPRESS:\n            default = \"\"\n        elif isinstance(default, str):\n            pass\n        elif isinstance(default, list) and not default:\n            default = \"\"\n        elif action.default is not None:\n            default = \"true\" if default else \"false\"\n        else:\n            default = \"\"\n\n        if action.help:\n            parts.append(f\"## {action.help}\")\n\n        for switch in action.option_strings:\n            if switch.startswith(\"--\"):\n                break\n        switch = switch.lstrip(\"-\")\n\n        if isinstance(action, argparse._StoreTrueAction):\n            default = False\n        elif isinstance(action, argparse._StoreConstAction):\n            default = False\n\n        if default is False:\n            default = \"false\"\n        if default is True:\n            default = \"true\"\n\n        if default:\n            parts.append(f\"#{switch}: {default}\\n\")\n        elif action.nargs in (\"*\", \"+\") or isinstance(action, argparse._AppendAction):\n            parts.append(f\"#{switch}: xxx\")\n            parts.append(\"## Specify multiple values like this:\")\n            parts.append(f\"#{switch}:\")\n            parts.append(f\"#  - xxx\")\n            parts.append(f\"#  - yyy\")\n            parts.append(f\"#  - zzz\")\n        else:\n            parts.append(f\"#{switch}: xxx\\n\")\n\n        ###\n        # parts.append(str(action))\n\n        return \"\\n\".join(parts) + \"\\n\"\n\n    def _format_action_invocation(self, action):\n        return \"\"\n\n    def _format_args(self, action, default_metavar):\n        return \"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/args_formatter.py::5",
    "metadata": {
      "file_path": "aider/args_formatter.py",
      "file_name": "args_formatter.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 348,
      "span_ids": [
        "MarkdownHelpFormatter._format_action",
        "MarkdownHelpFormatter._format_usage",
        "MarkdownHelpFormatter._format_text",
        "MarkdownHelpFormatter.start_section",
        "MarkdownHelpFormatter",
        "MarkdownHelpFormatter._format_action_invocation",
        "MarkdownHelpFormatter._format_args"
      ],
      "start_line": 169,
      "end_line": 223,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class MarkdownHelpFormatter(argparse.HelpFormatter):\n    def start_section(self, heading):\n        super().start_section(f\"## {heading}\")\n\n    def _format_usage(self, usage, actions, groups, prefix):\n        res = super()._format_usage(usage, actions, groups, prefix)\n        quote = \"```\\n\"\n        return quote + res + quote\n\n    def _format_text(self, text):\n        return \"\"\n\n    def _format_action(self, action):\n        if not action.option_strings:\n            return \"\"\n\n        parts = [\"\"]\n\n        metavar = action.metavar\n        if not metavar and isinstance(action, argparse._StoreAction):\n            metavar = \"VALUE\"\n\n        for switch in action.option_strings:\n            if switch.startswith(\"--\"):\n                break\n\n        if metavar:\n            parts.append(f\"### `{switch} {metavar}`\")\n        else:\n            parts.append(f\"### `{switch}`\")\n        if action.help:\n            parts.append(action.help + \"  \")\n\n        if action.default not in (argparse.SUPPRESS, None):\n            parts.append(f\"Default: {action.default}  \")\n\n        if action.env_var:\n            parts.append(f\"Environment variable: `{action.env_var}`  \")\n\n        if len(action.option_strings) > 1:\n            parts.append(\"Aliases:\")\n            for switch in action.option_strings:\n                if metavar:\n                    parts.append(f\"  - `{switch} {metavar}`\")\n                else:\n                    parts.append(f\"  - `{switch}`\")\n\n        return \"\\n\".join(parts) + \"\\n\"\n\n    def _format_action_invocation(self, action):\n        return \"\"\n\n    def _format_args(self, action, default_metavar):\n        return \"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/__init__.py::1",
    "metadata": {
      "file_path": "aider/coders/__init__.py",
      "file_name": "__init__.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 193,
      "span_ids": [
        "imports"
      ],
      "start_line": 1,
      "end_line": 27,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from .architect_coder import ArchitectCoder\nfrom .ask_coder import AskCoder\nfrom .base_coder import Coder\nfrom .editblock_coder import EditBlockCoder\nfrom .editblock_fenced_coder import EditBlockFencedCoder\nfrom .editor_editblock_coder import EditorEditBlockCoder\nfrom .editor_whole_coder import EditorWholeFileCoder\nfrom .help_coder import HelpCoder\nfrom .udiff_coder import UnifiedDiffCoder\nfrom .wholefile_coder import WholeFileCoder\n\n# from .single_wholefile_func_coder import SingleWholeFileFunctionCoder\n\n__all__ = [\n    HelpCoder,\n    AskCoder,\n    Coder,\n    EditBlockCoder,\n    EditBlockFencedCoder,\n    WholeFileCoder,\n    UnifiedDiffCoder,\n    #    SingleWholeFileFunctionCoder,\n    ArchitectCoder,\n    EditorEditBlockCoder,\n    EditorWholeFileCoder,\n]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/architect_coder.py::1",
    "metadata": {
      "file_path": "aider/coders/architect_coder.py",
      "file_name": "architect_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 343,
      "span_ids": [
        "imports",
        "ArchitectCoder",
        "ArchitectCoder.reply_completed"
      ],
      "start_line": 1,
      "end_line": 48,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from .architect_prompts import ArchitectPrompts\nfrom .ask_coder import AskCoder\nfrom .base_coder import Coder\n\n\nclass ArchitectCoder(AskCoder):\n    edit_format = \"architect\"\n    gpt_prompts = ArchitectPrompts()\n\n    def reply_completed(self):\n        content = self.partial_response_content\n\n        if not content or not content.strip():\n            return\n\n        if not self.io.confirm_ask(\"Edit the files?\"):\n            return\n\n        kwargs = dict()\n\n        # Use the editor_model from the main_model if it exists, otherwise use the main_model itself\n        editor_model = self.main_model.editor_model or self.main_model\n\n        kwargs[\"main_model\"] = editor_model\n        kwargs[\"edit_format\"] = self.main_model.editor_edit_format\n        kwargs[\"suggest_shell_commands\"] = False\n        kwargs[\"map_tokens\"] = 0\n        kwargs[\"total_cost\"] = self.total_cost\n        kwargs[\"cache_prompts\"] = False\n        kwargs[\"num_cache_warming_pings\"] = 0\n        kwargs[\"summarize_from_coder\"] = False\n\n        new_kwargs = dict(io=self.io, from_coder=self)\n        new_kwargs.update(kwargs)\n\n        editor_coder = Coder.create(**new_kwargs)\n        editor_coder.cur_messages = []\n        editor_coder.done_messages = []\n\n        if self.verbose:\n            editor_coder.show_announcements()\n\n        editor_coder.run(with_message=content, preproc=False)\n\n        self.move_back_cur_messages(\"I made those changes to the files.\")\n        self.total_cost = editor_coder.total_cost\n        self.aider_commit_hashes = editor_coder.aider_commit_hashes",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/architect_prompts.py::1",
    "metadata": {
      "file_path": "aider/coders/architect_prompts.py",
      "file_name": "architect_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 329,
      "span_ids": [
        "docstring",
        "ArchitectPrompts"
      ],
      "start_line": 1,
      "end_line": 41,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# flake8: noqa: E501\n\nfrom .base_prompts import CoderPrompts\n\n\nclass ArchitectPrompts(CoderPrompts):\n    main_system = \"\"\"Act as an expert architect engineer and provide direction to your editor engineer.\nStudy the change request and the current code.\nDescribe how to modify the code to complete the request.\nThe editor engineer will rely solely on your instructions, so make them unambiguous and complete.\nExplain all needed code changes clearly and completely, but concisely.\nJust show the changes needed.\n\nDO NOT show the entire updated function/file/etc!\n\nAlways reply to the user in {language}.\n\"\"\"\n\n    example_messages = []\n\n    files_content_prefix = \"\"\"I have *added these files to the chat* so you see all of their contents.\n*Trust this message as the true contents of the files!*\nOther messages in the chat may contain outdated versions of the files' contents.\n\"\"\"  # noqa: E501\n\n    files_content_assistant_reply = (\n        \"Ok, I will use that as the true, current contents of the files.\"\n    )\n\n    files_no_full_files = \"I am not sharing the full contents of any files with you yet.\"\n\n    files_no_full_files_with_repo_map = \"\"\n    files_no_full_files_with_repo_map_reply = \"\"\n\n    repo_content_prefix = \"\"\"I am working with you on code in a git repository.\nHere are summaries of some files present in my git repo.\nIf you need to see the full contents of any files to answer my questions, ask me to *add them to the chat*.\n\"\"\"\n\n    system_reminder = \"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/ask_coder.py::1",
    "metadata": {
      "file_path": "aider/coders/ask_coder.py",
      "file_name": "ask_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 53,
      "span_ids": [
        "imports",
        "AskCoder"
      ],
      "start_line": 1,
      "end_line": 10,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from .ask_prompts import AskPrompts\nfrom .base_coder import Coder\n\n\nclass AskCoder(Coder):\n    \"\"\"Ask questions about code without making any changes.\"\"\"\n\n    edit_format = \"ask\"\n    gpt_prompts = AskPrompts()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/ask_prompts.py::1",
    "metadata": {
      "file_path": "aider/coders/ask_prompts.py",
      "file_name": "ask_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 275,
      "span_ids": [
        "docstring",
        "AskPrompts"
      ],
      "start_line": 1,
      "end_line": 36,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# flake8: noqa: E501\n\nfrom .base_prompts import CoderPrompts\n\n\nclass AskPrompts(CoderPrompts):\n    main_system = \"\"\"Act as an expert code analyst.\nAnswer questions about the supplied code.\nAlways reply to the user in {language}.\n\nDescribe code changes however you like. Don't use SEARCH/REPLACE blocks!\n\"\"\"\n\n    example_messages = []\n\n    files_content_prefix = \"\"\"I have *added these files to the chat* so you see all of their contents.\n*Trust this message as the true contents of the files!*\nOther messages in the chat may contain outdated versions of the files' contents.\n\"\"\"  # noqa: E501\n\n    files_content_assistant_reply = (\n        \"Ok, I will use that as the true, current contents of the files.\"\n    )\n\n    files_no_full_files = \"I am not sharing the full contents of any files with you yet.\"\n\n    files_no_full_files_with_repo_map = \"\"\n    files_no_full_files_with_repo_map_reply = \"\"\n\n    repo_content_prefix = \"\"\"I am working with you on code in a git repository.\nHere are summaries of some files present in my git repo.\nIf you need to see the full contents of any files to answer my questions, ask me to *add them to the chat*.\n\"\"\"\n\n    system_reminder = \"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::1",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 391,
      "span_ids": [
        "UnknownEditFormat",
        "wrap_fence",
        "FinishReasonLength",
        "docstring",
        "impl",
        "UnknownEditFormat.__init__",
        "MissingAPIKeyError"
      ],
      "start_line": 1,
      "end_line": 69,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "#!/usr/bin/env python\n\nimport base64\nimport hashlib\nimport json\nimport locale\nimport math\nimport mimetypes\nimport os\nimport platform\nimport re\nimport sys\nimport threading\nimport time\nimport traceback\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom typing import List\n\nfrom aider import __version__, models, prompts, urls, utils\nfrom aider.analytics import Analytics\nfrom aider.commands import Commands\nfrom aider.exceptions import LiteLLMExceptions\nfrom aider.history import ChatSummary\nfrom aider.io import ConfirmGroup, InputOutput\nfrom aider.linter import Linter\nfrom aider.llm import litellm\nfrom aider.repo import ANY_GIT_ERROR, GitRepo\nfrom aider.repomap import RepoMap\nfrom aider.run_cmd import run_cmd\nfrom aider.sendchat import RETRY_TIMEOUT, send_completion\nfrom aider.utils import format_content, format_messages, format_tokens, is_image_file\n\nfrom ..dump import dump  # noqa: F401\nfrom .chat_chunks import ChatChunks\n\n\nclass UnknownEditFormat(ValueError):\n    def __init__(self, edit_format, valid_formats):\n        self.edit_format = edit_format\n        self.valid_formats = valid_formats\n        super().__init__(\n            f\"Unknown edit format {edit_format}. Valid formats are: {', '.join(valid_formats)}\"\n        )\n\n\nclass MissingAPIKeyError(ValueError):\n    pass\n\n\nclass FinishReasonLength(Exception):\n    pass\n\n\ndef wrap_fence(name):\n    return f\"<{name}>\", f\"</{name}>\"\n\n\nall_fences = [\n    (\"`\" * 3, \"`\" * 3),\n    (\"`\" * 4, \"`\" * 4),\n    wrap_fence(\"source\"),\n    wrap_fence(\"code\"),\n    wrap_fence(\"pre\"),\n    wrap_fence(\"codeblock\"),\n    wrap_fence(\"sourcecode\"),\n]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::2",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 259,
      "span_ids": [
        "Coder"
      ],
      "start_line": 72,
      "end_line": 107,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n    abs_fnames = None\n    abs_read_only_fnames = None\n    repo = None\n    last_aider_commit_hash = None\n    aider_edited_files = None\n    last_asked_for_commit_time = 0\n    repo_map = None\n    functions = None\n    num_exhausted_context_windows = 0\n    num_malformed_responses = 0\n    last_keyboard_interrupt = None\n    num_reflections = 0\n    max_reflections = 3\n    edit_format = None\n    yield_stream = False\n    temperature = 0\n    auto_lint = True\n    auto_test = False\n    test_cmd = None\n    lint_outcome = None\n    test_outcome = None\n    multi_response_content = \"\"\n    partial_response_content = \"\"\n    commit_before_message = []\n    message_cost = 0.0\n    message_tokens_sent = 0\n    message_tokens_received = 0\n    add_cache_headers = False\n    cache_warming_thread = None\n    num_cache_warming_pings = 0\n    suggest_shell_commands = True\n    detect_urls = True\n    ignore_mentions = None\n    chat_language = None\n    file_watcher = None",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::3",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 569,
      "span_ids": [
        "Coder.create",
        "Coder.clone"
      ],
      "start_line": 109,
      "end_line": 181,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    @classmethod\n    def create(\n        self,\n        main_model=None,\n        edit_format=None,\n        io=None,\n        from_coder=None,\n        summarize_from_coder=True,\n        **kwargs,\n    ):\n        import aider.coders as coders\n\n        if not main_model:\n            if from_coder:\n                main_model = from_coder.main_model\n            else:\n                main_model = models.Model(models.DEFAULT_MODEL_NAME)\n\n        if edit_format == \"code\":\n            edit_format = None\n        if edit_format is None:\n            if from_coder:\n                edit_format = from_coder.edit_format\n            else:\n                edit_format = main_model.edit_format\n\n        if not io and from_coder:\n            io = from_coder.io\n\n        if from_coder:\n            use_kwargs = dict(from_coder.original_kwargs)  # copy orig kwargs\n\n            # If the edit format changes, we can't leave old ASSISTANT\n            # messages in the chat history. The old edit format will\n            # confused the new LLM. It may try and imitate it, disobeying\n            # the system prompt.\n            done_messages = from_coder.done_messages\n            if edit_format != from_coder.edit_format and done_messages and summarize_from_coder:\n                done_messages = from_coder.summarizer.summarize_all(done_messages)\n\n            # Bring along context from the old Coder\n            update = dict(\n                fnames=list(from_coder.abs_fnames),\n                read_only_fnames=list(from_coder.abs_read_only_fnames),  # Copy read-only files\n                done_messages=done_messages,\n                cur_messages=from_coder.cur_messages,\n                aider_commit_hashes=from_coder.aider_commit_hashes,\n                commands=from_coder.commands.clone(),\n                total_cost=from_coder.total_cost,\n                ignore_mentions=from_coder.ignore_mentions,\n                file_watcher=from_coder.file_watcher,\n            )\n            use_kwargs.update(update)  # override to complete the switch\n            use_kwargs.update(kwargs)  # override passed kwargs\n\n            kwargs = use_kwargs\n\n        for coder in coders.__all__:\n            if hasattr(coder, \"edit_format\") and coder.edit_format == edit_format:\n                res = coder(main_model, io, **kwargs)\n                res.original_kwargs = dict(kwargs)\n                return res\n\n        valid_formats = [\n            str(c.edit_format)\n            for c in coders.__all__\n            if hasattr(c, \"edit_format\") and c.edit_format is not None\n        ]\n        raise UnknownEditFormat(edit_format, valid_formats)\n\n    def clone(self, **kwargs):\n        new_coder = Coder.create(from_coder=self, **kwargs)\n        return new_coder",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::4",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 596,
      "span_ids": [
        "Coder.get_announcements"
      ],
      "start_line": 183,
      "end_line": 259,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def get_announcements(self):\n        lines = []\n        lines.append(f\"Aider v{__version__}\")\n\n        # Model\n        main_model = self.main_model\n        weak_model = main_model.weak_model\n\n        if weak_model is not main_model:\n            prefix = \"Main model\"\n        else:\n            prefix = \"Model\"\n\n        output = f\"{prefix}: {main_model.name} with {self.edit_format} edit format\"\n        if self.add_cache_headers or main_model.caches_by_default:\n            output += \", prompt cache\"\n        if main_model.info.get(\"supports_assistant_prefill\"):\n            output += \", infinite output\"\n        lines.append(output)\n\n        if self.edit_format == \"architect\":\n            output = (\n                f\"Editor model: {main_model.editor_model.name} with\"\n                f\" {main_model.editor_edit_format} edit format\"\n            )\n            lines.append(output)\n\n        if weak_model is not main_model:\n            output = f\"Weak model: {weak_model.name}\"\n            lines.append(output)\n\n        # Repo\n        if self.repo:\n            rel_repo_dir = self.repo.get_rel_repo_dir()\n            num_files = len(self.repo.get_tracked_files())\n\n            lines.append(f\"Git repo: {rel_repo_dir} with {num_files:,} files\")\n            if num_files > 1000:\n                lines.append(\n                    \"Warning: For large repos, consider using --subtree-only and .aiderignore\"\n                )\n                lines.append(f\"See: {urls.large_repos}\")\n        else:\n            lines.append(\"Git repo: none\")\n\n        # Repo-map\n        if self.repo_map:\n            map_tokens = self.repo_map.max_map_tokens\n            if map_tokens > 0:\n                refresh = self.repo_map.refresh\n                lines.append(f\"Repo-map: using {map_tokens} tokens, {refresh} refresh\")\n                max_map_tokens = self.main_model.get_repo_map_tokens() * 2\n                if map_tokens > max_map_tokens:\n                    lines.append(\n                        f\"Warning: map-tokens > {max_map_tokens} is not recommended. Too much\"\n                        \" irrelevant code can confuse LLMs.\"\n                    )\n            else:\n                lines.append(\"Repo-map: disabled because map_tokens == 0\")\n        else:\n            lines.append(\"Repo-map: disabled\")\n\n        # Files\n        for fname in self.get_inchat_relative_files():\n            lines.append(f\"Added {fname} to the chat.\")\n\n        for fname in self.abs_read_only_fnames:\n            rel_fname = self.get_rel_fname(fname)\n            lines.append(f\"Added {rel_fname} to the chat (read-only).\")\n\n        if self.done_messages:\n            lines.append(\"Restored previous conversation history.\")\n\n        if self.io.multiline_mode:\n            lines.append(\"Multiline mode: Enabled. Enter inserts newline, Alt-Enter submits text\")\n\n        return lines",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::5",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 1402,
      "span_ids": [
        "Coder.__init__"
      ],
      "start_line": 261,
      "end_line": 486,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def __init__(\n        self,\n        main_model,\n        io,\n        repo=None,\n        fnames=None,\n        read_only_fnames=None,\n        show_diffs=False,\n        auto_commits=True,\n        dirty_commits=True,\n        dry_run=False,\n        map_tokens=1024,\n        verbose=False,\n        stream=True,\n        use_git=True,\n        cur_messages=None,\n        done_messages=None,\n        restore_chat_history=False,\n        auto_lint=True,\n        auto_test=False,\n        lint_cmds=None,\n        test_cmd=None,\n        aider_commit_hashes=None,\n        map_mul_no_files=8,\n        commands=None,\n        summarizer=None,\n        total_cost=0.0,\n        analytics=None,\n        map_refresh=\"auto\",\n        cache_prompts=False,\n        num_cache_warming_pings=0,\n        suggest_shell_commands=True,\n        chat_language=None,\n        detect_urls=True,\n        ignore_mentions=None,\n        file_watcher=None,\n        auto_copy_context=False,\n    ):\n        # Fill in a dummy Analytics if needed, but it is never .enable()'d\n        self.analytics = analytics if analytics is not None else Analytics()\n\n        self.event = self.analytics.event\n        self.chat_language = chat_language\n        self.commit_before_message = []\n        self.aider_commit_hashes = set()\n        self.rejected_urls = set()\n        self.abs_root_path_cache = {}\n\n        self.auto_copy_context = auto_copy_context\n\n        self.ignore_mentions = ignore_mentions\n        if not self.ignore_mentions:\n            self.ignore_mentions = set()\n\n        self.file_watcher = file_watcher\n        if self.file_watcher:\n            self.file_watcher.coder = self\n\n        self.suggest_shell_commands = suggest_shell_commands\n        self.detect_urls = detect_urls\n\n        self.num_cache_warming_pings = num_cache_warming_pings\n\n        if not fnames:\n            fnames = []\n\n        if io is None:\n            io = InputOutput()\n\n        if aider_commit_hashes:\n            self.aider_commit_hashes = aider_commit_hashes\n        else:\n            self.aider_commit_hashes = set()\n\n        self.chat_completion_call_hashes = []\n        self.chat_completion_response_hashes = []\n        self.need_commit_before_edits = set()\n\n        self.total_cost = total_cost\n\n        self.verbose = verbose\n        self.abs_fnames = set()\n        self.abs_read_only_fnames = set()\n\n        if cur_messages:\n            self.cur_messages = cur_messages\n        else:\n            self.cur_messages = []\n\n        if done_messages:\n            self.done_messages = done_messages\n        else:\n            self.done_messages = []\n\n        self.io = io\n\n        self.shell_commands = []\n\n        if not auto_commits:\n            dirty_commits = False\n\n        self.auto_commits = auto_commits\n        self.dirty_commits = dirty_commits\n\n        self.dry_run = dry_run\n        self.pretty = self.io.pretty\n\n        self.main_model = main_model\n\n        self.stream = stream and main_model.streaming\n\n        if cache_prompts and self.main_model.cache_control:\n            self.add_cache_headers = True\n\n        self.show_diffs = show_diffs\n\n        self.commands = commands or Commands(self.io, self)\n        self.commands.coder = self\n\n        self.repo = repo\n        if use_git and self.repo is None:\n            try:\n                self.repo = GitRepo(\n                    self.io,\n                    fnames,\n                    None,\n                    models=main_model.commit_message_models(),\n                )\n            except FileNotFoundError:\n                pass\n\n        if self.repo:\n            self.root = self.repo.root\n\n        for fname in fnames:\n            fname = Path(fname)\n            if self.repo and self.repo.git_ignored_file(fname):\n                self.io.tool_warning(f\"Skipping {fname} that matches gitignore spec.\")\n\n            if self.repo and self.repo.ignored_file(fname):\n                self.io.tool_warning(f\"Skipping {fname} that matches aiderignore spec.\")\n                continue\n\n            if not fname.exists():\n                if utils.touch_file(fname):\n                    self.io.tool_output(f\"Creating empty file {fname}\")\n                else:\n                    self.io.tool_warning(f\"Can not create {fname}, skipping.\")\n                    continue\n\n            if not fname.is_file():\n                self.io.tool_warning(f\"Skipping {fname} that is not a normal file.\")\n                continue\n\n            fname = str(fname.resolve())\n\n            self.abs_fnames.add(fname)\n            self.check_added_files()\n\n        if not self.repo:\n            self.root = utils.find_common_root(self.abs_fnames)\n\n        if read_only_fnames:\n            self.abs_read_only_fnames = set()\n            for fname in read_only_fnames:\n                abs_fname = self.abs_root_path(fname)\n                if os.path.exists(abs_fname):\n                    self.abs_read_only_fnames.add(abs_fname)\n                else:\n                    self.io.tool_warning(f\"Error: Read-only file {fname} does not exist. Skipping.\")\n\n        if map_tokens is None:\n            use_repo_map = main_model.use_repo_map\n            map_tokens = 1024\n        else:\n            use_repo_map = map_tokens > 0\n\n        max_inp_tokens = self.main_model.info.get(\"max_input_tokens\") or 0\n\n        has_map_prompt = hasattr(self, \"gpt_prompts\") and self.gpt_prompts.repo_content_prefix\n\n        if use_repo_map and self.repo and has_map_prompt:\n            self.repo_map = RepoMap(\n                map_tokens,\n                self.root,\n                self.main_model,\n                io,\n                self.gpt_prompts.repo_content_prefix,\n                self.verbose,\n                max_inp_tokens,\n                map_mul_no_files=map_mul_no_files,\n                refresh=map_refresh,\n            )\n\n        self.summarizer = summarizer or ChatSummary(\n            [self.main_model.weak_model, self.main_model],\n            self.main_model.max_chat_history_tokens,\n        )\n\n        self.summarizer_thread = None\n        self.summarized_done_messages = []\n\n        if not self.done_messages and restore_chat_history:\n            history_md = self.io.read_text(self.io.chat_history_file)\n            if history_md:\n                self.done_messages = utils.split_chat_history_markdown(history_md)\n                self.summarize_start()\n\n        # Linting and testing\n        self.linter = Linter(root=self.root, encoding=io.encoding)\n        self.auto_lint = auto_lint\n        self.setup_lint_cmds(lint_cmds)\n        self.lint_cmds = lint_cmds\n        self.auto_test = auto_test\n        self.test_cmd = test_cmd\n\n        # validate the functions jsonschema\n        if self.functions:\n            from jsonschema import Draft7Validator\n\n            for function in self.functions:\n                Draft7Validator.check_schema(function)\n\n            if self.verbose:\n                self.io.tool_output(\"JSON Schema:\")\n                self.io.tool_output(json.dumps(self.functions, indent=4))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::6",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 363,
      "span_ids": [
        "Coder.show_pretty",
        "Coder.setup_lint_cmds",
        "Coder.show_announcements",
        "Coder.drop_rel_fname",
        "Coder.add_rel_fname",
        "Coder.abs_root_path",
        "Coder:72",
        "Coder.get_abs_fnames_content"
      ],
      "start_line": 488,
      "end_line": 542,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def setup_lint_cmds(self, lint_cmds):\n        if not lint_cmds:\n            return\n        for lang, cmd in lint_cmds.items():\n            self.linter.set_linter(lang, cmd)\n\n    def show_announcements(self):\n        bold = True\n        for line in self.get_announcements():\n            self.io.tool_output(line, bold=bold)\n            bold = False\n\n    def add_rel_fname(self, rel_fname):\n        self.abs_fnames.add(self.abs_root_path(rel_fname))\n        self.check_added_files()\n\n    def drop_rel_fname(self, fname):\n        abs_fname = self.abs_root_path(fname)\n        if abs_fname in self.abs_fnames:\n            self.abs_fnames.remove(abs_fname)\n            return True\n\n    def abs_root_path(self, path):\n        key = path\n        if key in self.abs_root_path_cache:\n            return self.abs_root_path_cache[key]\n\n        res = Path(self.root) / path\n        res = utils.safe_abs_path(res)\n        self.abs_root_path_cache[key] = res\n        return res\n\n    fences = all_fences\n    fence = fences[0]\n\n    def show_pretty(self):\n        if not self.pretty:\n            return False\n\n        # only show pretty output if fences are the normal triple-backtick\n        if self.fence[0][0] != \"`\":\n            return False\n\n        return True\n\n    def get_abs_fnames_content(self):\n        for fname in list(self.abs_fnames):\n            content = self.io.read_text(fname)\n\n            if content is None:\n                relative_fname = self.get_rel_fname(fname)\n                self.io.tool_warning(f\"Dropping {relative_fname} from the chat.\")\n                self.abs_fnames.remove(fname)\n            else:\n                yield fname, content",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::7",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 207,
      "span_ids": [
        "Coder.choose_fence"
      ],
      "start_line": 544,
      "end_line": 570,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def choose_fence(self):\n        all_content = \"\"\n        for _fname, content in self.get_abs_fnames_content():\n            all_content += content + \"\\n\"\n        for _fname in self.abs_read_only_fnames:\n            content = self.io.read_text(_fname)\n            if content is not None:\n                all_content += content + \"\\n\"\n\n        lines = all_content.splitlines()\n        good = False\n        for fence_open, fence_close in self.fences:\n            if any(line.startswith(fence_open) or line.startswith(fence_close) for line in lines):\n                continue\n            good = True\n            break\n\n        if good:\n            self.fence = (fence_open, fence_close)\n        else:\n            self.fence = self.fences[0]\n            self.io.tool_warning(\n                \"Unable to find a fencing strategy! Falling back to:\"\n                f\" {self.fence[0]}...{self.fence[1]}\"\n            )\n\n        return",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::8",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 159,
      "span_ids": [
        "Coder.get_files_content"
      ],
      "start_line": 572,
      "end_line": 592,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def get_files_content(self, fnames=None):\n        if not fnames:\n            fnames = self.abs_fnames\n\n        prompt = \"\"\n        for fname, content in self.get_abs_fnames_content():\n            if not is_image_file(fname):\n                relative_fname = self.get_rel_fname(fname)\n                prompt += \"\\n\"\n                prompt += relative_fname\n                prompt += f\"\\n{self.fence[0]}\\n\"\n\n                prompt += content\n\n                # lines = content.splitlines(keepends=True)\n                # lines = [f\"{i+1:03}:{line}\" for i, line in enumerate(lines)]\n                # prompt += \"\".join(lines)\n\n                prompt += f\"{self.fence[1]}\\n\"\n\n        return prompt",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::9",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 210,
      "span_ids": [
        "Coder.get_read_only_files_content",
        "Coder.get_cur_message_text",
        "Coder.get_ident_mentions"
      ],
      "start_line": 594,
      "end_line": 617,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def get_read_only_files_content(self):\n        prompt = \"\"\n        for fname in self.abs_read_only_fnames:\n            content = self.io.read_text(fname)\n            if content is not None and not is_image_file(fname):\n                relative_fname = self.get_rel_fname(fname)\n                prompt += \"\\n\"\n                prompt += relative_fname\n                prompt += f\"\\n{self.fence[0]}\\n\"\n                prompt += content\n                prompt += f\"{self.fence[1]}\\n\"\n        return prompt\n\n    def get_cur_message_text(self):\n        text = \"\"\n        for msg in self.cur_messages:\n            text += msg[\"content\"] + \"\\n\"\n        return text\n\n    def get_ident_mentions(self, text):\n        # Split the string on any character that is not alphanumeric\n        # \\W+ matches one or more non-word characters (equivalent to [^a-zA-Z0-9_]+)\n        words = set(re.split(r\"\\W+\", text))\n        return words",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::10",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 162,
      "span_ids": [
        "Coder.get_ident_filename_matches"
      ],
      "start_line": 619,
      "end_line": 642,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def get_ident_filename_matches(self, idents):\n        all_fnames = defaultdict(set)\n        for fname in self.get_all_relative_files():\n            # Skip empty paths or just '.'\n            if not fname or fname == \".\":\n                continue\n\n            try:\n                # Handle dotfiles properly\n                path = Path(fname)\n                base = path.stem.lower()  # Use stem instead of with_suffix(\"\").name\n                if len(base) >= 5:\n                    all_fnames[base].add(fname)\n            except ValueError:\n                # Skip paths that can't be processed\n                continue\n\n        matches = set()\n        for ident in idents:\n            if len(ident) < 5:\n                continue\n            matches.update(all_fnames[ident.lower()])\n\n        return matches",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::11",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 291,
      "span_ids": [
        "Coder.get_repo_map"
      ],
      "start_line": 644,
      "end_line": 683,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def get_repo_map(self, force_refresh=False):\n        if not self.repo_map:\n            return\n\n        cur_msg_text = self.get_cur_message_text()\n        mentioned_fnames = self.get_file_mentions(cur_msg_text)\n        mentioned_idents = self.get_ident_mentions(cur_msg_text)\n\n        mentioned_fnames.update(self.get_ident_filename_matches(mentioned_idents))\n\n        all_abs_files = set(self.get_all_abs_files())\n        repo_abs_read_only_fnames = set(self.abs_read_only_fnames) & all_abs_files\n        chat_files = set(self.abs_fnames) | repo_abs_read_only_fnames\n        other_files = all_abs_files - chat_files\n\n        repo_content = self.repo_map.get_repo_map(\n            chat_files,\n            other_files,\n            mentioned_fnames=mentioned_fnames,\n            mentioned_idents=mentioned_idents,\n            force_refresh=force_refresh,\n        )\n\n        # fall back to global repo map if files in chat are disjoint from rest of repo\n        if not repo_content:\n            repo_content = self.repo_map.get_repo_map(\n                set(),\n                all_abs_files,\n                mentioned_fnames=mentioned_fnames,\n                mentioned_idents=mentioned_idents,\n            )\n\n        # fall back to completely unhinted repo\n        if not repo_content:\n            repo_content = self.repo_map.get_repo_map(\n                set(),\n                all_abs_files,\n            )\n\n        return repo_content",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::12",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 237,
      "span_ids": [
        "Coder.get_readonly_files_messages",
        "Coder.get_repo_messages"
      ],
      "start_line": 685,
      "end_line": 722,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def get_repo_messages(self):\n        repo_messages = []\n        repo_content = self.get_repo_map()\n        if repo_content:\n            repo_messages += [\n                dict(role=\"user\", content=repo_content),\n                dict(\n                    role=\"assistant\",\n                    content=\"Ok, I won't try and edit those files without asking first.\",\n                ),\n            ]\n        return repo_messages\n\n    def get_readonly_files_messages(self):\n        readonly_messages = []\n\n        # Handle non-image files\n        read_only_content = self.get_read_only_files_content()\n        if read_only_content:\n            readonly_messages += [\n                dict(\n                    role=\"user\", content=self.gpt_prompts.read_only_files_prefix + read_only_content\n                ),\n                dict(\n                    role=\"assistant\",\n                    content=\"Ok, I will use these files as references.\",\n                ),\n            ]\n\n        # Handle image files\n        images_message = self.get_images_message(self.abs_read_only_fnames)\n        if images_message is not None:\n            readonly_messages += [\n                images_message,\n                dict(role=\"assistant\", content=\"Ok, I will use these images as references.\"),\n            ]\n\n        return readonly_messages",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::13",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 226,
      "span_ids": [
        "Coder.get_chat_files_messages"
      ],
      "start_line": 724,
      "end_line": 750,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def get_chat_files_messages(self):\n        chat_files_messages = []\n        if self.abs_fnames:\n            files_content = self.gpt_prompts.files_content_prefix\n            files_content += self.get_files_content()\n            files_reply = self.gpt_prompts.files_content_assistant_reply\n        elif self.get_repo_map() and self.gpt_prompts.files_no_full_files_with_repo_map:\n            files_content = self.gpt_prompts.files_no_full_files_with_repo_map\n            files_reply = self.gpt_prompts.files_no_full_files_with_repo_map_reply\n        else:\n            files_content = self.gpt_prompts.files_no_full_files\n            files_reply = \"Ok.\"\n\n        if files_content:\n            chat_files_messages += [\n                dict(role=\"user\", content=files_content),\n                dict(role=\"assistant\", content=files_reply),\n            ]\n\n        images_message = self.get_images_message(self.abs_fnames)\n        if images_message is not None:\n            chat_files_messages += [\n                images_message,\n                dict(role=\"assistant\", content=\"Ok.\"),\n            ]\n\n        return chat_files_messages",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::14",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 370,
      "span_ids": [
        "Coder.get_images_message"
      ],
      "start_line": 752,
      "end_line": 792,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def get_images_message(self, fnames):\n        supports_images = self.main_model.info.get(\"supports_vision\")\n        supports_pdfs = self.main_model.info.get(\"supports_pdf_input\") or self.main_model.info.get(\n            \"max_pdf_size_mb\"\n        )\n\n        # https://github.com/BerriAI/litellm/pull/6928\n        supports_pdfs = supports_pdfs or \"claude-3-5-sonnet-20241022\" in self.main_model.name\n\n        if not (supports_images or supports_pdfs):\n            return None\n\n        image_messages = []\n        for fname in fnames:\n            if not is_image_file(fname):\n                continue\n\n            mime_type, _ = mimetypes.guess_type(fname)\n            if not mime_type:\n                continue\n\n            with open(fname, \"rb\") as image_file:\n                encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n            image_url = f\"data:{mime_type};base64,{encoded_string}\"\n            rel_fname = self.get_rel_fname(fname)\n\n            if mime_type.startswith(\"image/\") and supports_images:\n                image_messages += [\n                    {\"type\": \"text\", \"text\": f\"Image file: {rel_fname}\"},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url, \"detail\": \"high\"}},\n                ]\n            elif mime_type == \"application/pdf\" and supports_pdfs:\n                image_messages += [\n                    {\"type\": \"text\", \"text\": f\"PDF file: {rel_fname}\"},\n                    {\"type\": \"image_url\", \"image_url\": image_url},\n                ]\n\n        if not image_messages:\n            return None\n\n        return {\"role\": \"user\", \"content\": image_messages}",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::15",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 245,
      "span_ids": [
        "Coder.run_stream",
        "Coder.copy_context",
        "Coder.init_before_message",
        "Coder.run"
      ],
      "start_line": 794,
      "end_line": 831,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def run_stream(self, user_message):\n        self.io.user_input(user_message)\n        self.init_before_message()\n        yield from self.send_message(user_message)\n\n    def init_before_message(self):\n        self.aider_edited_files = set()\n        self.reflected_message = None\n        self.num_reflections = 0\n        self.lint_outcome = None\n        self.test_outcome = None\n        self.shell_commands = []\n        self.message_cost = 0\n\n        if self.repo:\n            self.commit_before_message.append(self.repo.get_head_commit_sha())\n\n    def run(self, with_message=None, preproc=True):\n        try:\n            if with_message:\n                self.io.user_input(with_message)\n                self.run_one(with_message, preproc)\n                return self.partial_response_content\n            while True:\n                try:\n                    if not self.io.placeholder:\n                        self.copy_context()\n                    user_message = self.get_input()\n                    self.run_one(user_message, preproc)\n                    self.show_undo_hint()\n                except KeyboardInterrupt:\n                    self.keyboard_interrupt()\n        except EOFError:\n            return\n\n    def copy_context(self):\n        if self.auto_copy_context:\n            self.commands.cmd_copy_context()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::16",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 122,
      "span_ids": [
        "Coder.get_input"
      ],
      "start_line": 833,
      "end_line": 845,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def get_input(self):\n        inchat_files = self.get_inchat_relative_files()\n        read_only_files = [self.get_rel_fname(fname) for fname in self.abs_read_only_fnames]\n        all_files = sorted(set(inchat_files + read_only_files))\n        edit_format = \"\" if self.edit_format == self.main_model.edit_format else self.edit_format\n        return self.io.get_input(\n            self.root,\n            all_files,\n            self.get_addable_relative_files(),\n            self.commands,\n            self.abs_read_only_fnames,\n            edit_format=edit_format,\n        )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::17",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 181,
      "span_ids": [
        "Coder.preproc_user_input",
        "Coder.run_one"
      ],
      "start_line": 847,
      "end_line": 879,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def preproc_user_input(self, inp):\n        if not inp:\n            return\n\n        if self.commands.is_command(inp):\n            return self.commands.run(inp)\n\n        self.check_for_file_mentions(inp)\n        inp = self.check_for_urls(inp)\n\n        return inp\n\n    def run_one(self, user_message, preproc):\n        self.init_before_message()\n\n        if preproc:\n            message = self.preproc_user_input(user_message)\n        else:\n            message = user_message\n\n        while message:\n            self.reflected_message = None\n            list(self.send_message(message))\n\n            if not self.reflected_message:\n                break\n\n            if self.num_reflections >= self.max_reflections:\n                self.io.tool_warning(f\"Only {self.max_reflections} reflections allowed, stopping.\")\n                return\n\n            self.num_reflections += 1\n            message = self.reflected_message",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::18",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 141,
      "span_ids": [
        "Coder.check_and_open_urls"
      ],
      "start_line": 881,
      "end_line": 896,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def check_and_open_urls(self, exc, friendly_msg=None):\n        \"\"\"Check exception for URLs, offer to open in a browser, with user-friendly error msgs.\"\"\"\n        text = str(exc)\n\n        if friendly_msg:\n            self.io.tool_warning(text)\n            self.io.tool_error(f\"{friendly_msg}\")\n        else:\n            self.io.tool_error(text)\n\n        url_pattern = re.compile(r\"(https?://[^\\s/$.?#].[^\\s]*)\")\n        urls = list(set(url_pattern.findall(text)))  # Use set to remove duplicates\n        for url in urls:\n            url = url.rstrip(\".',\\\"\")\n            self.io.offer_url(url)\n        return urls",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::19",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 185,
      "span_ids": [
        "Coder.check_for_urls"
      ],
      "start_line": 898,
      "end_line": 917,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def check_for_urls(self, inp: str) -> List[str]:\n        \"\"\"Check input for URLs and offer to add them to the chat.\"\"\"\n        if not self.detect_urls:\n            return inp\n\n        url_pattern = re.compile(r\"(https?://[^\\s/$.?#].[^\\s]*[^\\s,.])\")\n        urls = list(set(url_pattern.findall(inp)))  # Use set to remove duplicates\n        group = ConfirmGroup(urls)\n        for url in urls:\n            if url not in self.rejected_urls:\n                url = url.rstrip(\".',\\\"\")\n                if self.io.confirm_ask(\n                    \"Add URL to the chat?\", subject=url, group=group, allow_never=True\n                ):\n                    inp += \"\\n\\n\"\n                    inp += self.commands.cmd_web(url, return_content=True)\n                else:\n                    self.rejected_urls.add(url)\n\n        return inp",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::20",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 470,
      "span_ids": [
        "Coder.move_back_cur_messages",
        "Coder.keyboard_interrupt",
        "Coder.summarize_end",
        "Coder.summarize_start",
        "Coder.get_user_language",
        "Coder.summarize_worker"
      ],
      "start_line": 919,
      "end_line": 993,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def keyboard_interrupt(self):\n        now = time.time()\n\n        thresh = 2  # seconds\n        if self.last_keyboard_interrupt and now - self.last_keyboard_interrupt < thresh:\n            self.io.tool_warning(\"\\n\\n^C KeyboardInterrupt\")\n            self.event(\"exit\", reason=\"Control-C\")\n            sys.exit()\n\n        self.io.tool_warning(\"\\n\\n^C again to exit\")\n\n        self.last_keyboard_interrupt = now\n\n    def summarize_start(self):\n        if not self.summarizer.too_big(self.done_messages):\n            return\n\n        self.summarize_end()\n\n        if self.verbose:\n            self.io.tool_output(\"Starting to summarize chat history.\")\n\n        self.summarizer_thread = threading.Thread(target=self.summarize_worker)\n        self.summarizer_thread.start()\n\n    def summarize_worker(self):\n        try:\n            self.summarized_done_messages = self.summarizer.summarize(self.done_messages)\n        except ValueError as err:\n            self.io.tool_warning(err.args[0])\n\n        if self.verbose:\n            self.io.tool_output(\"Finished summarizing chat history.\")\n\n    def summarize_end(self):\n        if self.summarizer_thread is None:\n            return\n\n        self.summarizer_thread.join()\n        self.summarizer_thread = None\n\n        self.done_messages = self.summarized_done_messages\n        self.summarized_done_messages = []\n\n    def move_back_cur_messages(self, message):\n        self.done_messages += self.cur_messages\n        self.summarize_start()\n\n        # TODO check for impact on image messages\n        if message:\n            self.done_messages += [\n                dict(role=\"user\", content=message),\n                dict(role=\"assistant\", content=\"Ok.\"),\n            ]\n        self.cur_messages = []\n\n    def get_user_language(self):\n        if self.chat_language:\n            return self.chat_language\n\n        try:\n            lang = locale.getlocale()[0]\n            if lang:\n                return lang  # Return the full language code, including country\n        except Exception:\n            pass\n\n        for env_var in [\"LANG\", \"LANGUAGE\", \"LC_ALL\", \"LC_MESSAGES\"]:\n            lang = os.environ.get(env_var)\n            if lang:\n                return lang.split(\".\")[\n                    0\n                ]  # Return language and country, but remove encoding if present\n\n        return None",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::21",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 333,
      "span_ids": [
        "Coder.get_platform_info"
      ],
      "start_line": 995,
      "end_line": 1034,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def get_platform_info(self):\n        platform_text = f\"- Platform: {platform.platform()}\\n\"\n        shell_var = \"COMSPEC\" if os.name == \"nt\" else \"SHELL\"\n        shell_val = os.getenv(shell_var)\n        platform_text += f\"- Shell: {shell_var}={shell_val}\\n\"\n\n        user_lang = self.get_user_language()\n        if user_lang:\n            platform_text += f\"- Language: {user_lang}\\n\"\n\n        dt = datetime.now().astimezone().strftime(\"%Y-%m-%d\")\n        platform_text += f\"- Current date: {dt}\\n\"\n\n        if self.repo:\n            platform_text += \"- The user is operating inside a git repository\\n\"\n\n        if self.lint_cmds:\n            if self.auto_lint:\n                platform_text += (\n                    \"- The user's pre-commit runs these lint commands, don't suggest running\"\n                    \" them:\\n\"\n                )\n            else:\n                platform_text += \"- The user prefers these lint commands:\\n\"\n            for lang, cmd in self.lint_cmds.items():\n                if lang is None:\n                    platform_text += f\"  - {cmd}\\n\"\n                else:\n                    platform_text += f\"  - {lang}: {cmd}\\n\"\n\n        if self.test_cmd:\n            if self.auto_test:\n                platform_text += (\n                    \"- The user's pre-commit runs this test command, don't suggest running them: \"\n                )\n            else:\n                platform_text += \"- The user prefers this test command: \"\n            platform_text += self.test_cmd + \"\\n\"\n\n        return platform_text",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::22",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 219,
      "span_ids": [
        "Coder.fmt_system_prompt"
      ],
      "start_line": 1036,
      "end_line": 1062,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def fmt_system_prompt(self, prompt):\n        lazy_prompt = self.gpt_prompts.lazy_prompt if self.main_model.lazy else \"\"\n        platform_text = self.get_platform_info()\n\n        if self.suggest_shell_commands:\n            shell_cmd_prompt = self.gpt_prompts.shell_cmd_prompt.format(platform=platform_text)\n            shell_cmd_reminder = self.gpt_prompts.shell_cmd_reminder.format(platform=platform_text)\n        else:\n            shell_cmd_prompt = self.gpt_prompts.no_shell_cmd_prompt.format(platform=platform_text)\n            shell_cmd_reminder = self.gpt_prompts.no_shell_cmd_reminder.format(\n                platform=platform_text\n            )\n\n        if self.chat_language:\n            language = self.chat_language\n        else:\n            language = \"the same language they are using\"\n\n        prompt = prompt.format(\n            fence=self.fence,\n            lazy_prompt=lazy_prompt,\n            platform=platform_text,\n            shell_cmd_prompt=shell_cmd_prompt,\n            shell_cmd_reminder=shell_cmd_reminder,\n            language=language,\n        )\n        return prompt",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::23",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 752,
      "span_ids": [
        "Coder.format_chat_chunks"
      ],
      "start_line": 1064,
      "end_line": 1167,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def format_chat_chunks(self):\n        self.choose_fence()\n        main_sys = self.fmt_system_prompt(self.gpt_prompts.main_system)\n\n        example_messages = []\n        if self.main_model.examples_as_sys_msg:\n            if self.gpt_prompts.example_messages:\n                main_sys += \"\\n# Example conversations:\\n\\n\"\n            for msg in self.gpt_prompts.example_messages:\n                role = msg[\"role\"]\n                content = self.fmt_system_prompt(msg[\"content\"])\n                main_sys += f\"## {role.upper()}: {content}\\n\\n\"\n            main_sys = main_sys.strip()\n        else:\n            for msg in self.gpt_prompts.example_messages:\n                example_messages.append(\n                    dict(\n                        role=msg[\"role\"],\n                        content=self.fmt_system_prompt(msg[\"content\"]),\n                    )\n                )\n            if self.gpt_prompts.example_messages:\n                example_messages += [\n                    dict(\n                        role=\"user\",\n                        content=(\n                            \"I switched to a new code base. Please don't consider the above files\"\n                            \" or try to edit them any longer.\"\n                        ),\n                    ),\n                    dict(role=\"assistant\", content=\"Ok.\"),\n                ]\n\n        if self.gpt_prompts.system_reminder:\n            main_sys += \"\\n\" + self.fmt_system_prompt(self.gpt_prompts.system_reminder)\n\n        chunks = ChatChunks()\n\n        if self.main_model.use_system_prompt:\n            chunks.system = [\n                dict(role=\"system\", content=main_sys),\n            ]\n        else:\n            chunks.system = [\n                dict(role=\"user\", content=main_sys),\n                dict(role=\"assistant\", content=\"Ok.\"),\n            ]\n\n        chunks.examples = example_messages\n\n        self.summarize_end()\n        chunks.done = self.done_messages\n\n        chunks.repo = self.get_repo_messages()\n        chunks.readonly_files = self.get_readonly_files_messages()\n        chunks.chat_files = self.get_chat_files_messages()\n\n        if self.gpt_prompts.system_reminder:\n            reminder_message = [\n                dict(\n                    role=\"system\", content=self.fmt_system_prompt(self.gpt_prompts.system_reminder)\n                ),\n            ]\n        else:\n            reminder_message = []\n\n        chunks.cur = list(self.cur_messages)\n        chunks.reminder = []\n\n        # TODO review impact of token count on image messages\n        messages_tokens = self.main_model.token_count(chunks.all_messages())\n        reminder_tokens = self.main_model.token_count(reminder_message)\n        cur_tokens = self.main_model.token_count(chunks.cur)\n\n        if None not in (messages_tokens, reminder_tokens, cur_tokens):\n            total_tokens = messages_tokens + reminder_tokens + cur_tokens\n        else:\n            # add the reminder anyway\n            total_tokens = 0\n\n        if chunks.cur:\n            final = chunks.cur[-1]\n        else:\n            final = None\n\n        max_input_tokens = self.main_model.info.get(\"max_input_tokens\") or 0\n        # Add the reminder prompt if we still have room to include it.\n        if (\n            not max_input_tokens\n            or total_tokens < max_input_tokens\n            and self.gpt_prompts.system_reminder\n        ):\n            if self.main_model.reminder == \"sys\":\n                chunks.reminder = reminder_message\n            elif self.main_model.reminder == \"user\" and final and final[\"role\"] == \"user\":\n                # stuff it into the user message\n                new_content = (\n                    final[\"content\"]\n                    + \"\\n\\n\"\n                    + self.fmt_system_prompt(self.gpt_prompts.system_reminder)\n                )\n                chunks.cur[-1] = dict(role=final[\"role\"], content=new_content)\n\n        return chunks",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::24",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 390,
      "span_ids": [
        "Coder.format_messages",
        "Coder.warm_cache"
      ],
      "start_line": 1169,
      "end_line": 1227,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def format_messages(self):\n        chunks = self.format_chat_chunks()\n        if self.add_cache_headers:\n            chunks.add_cache_control_headers()\n\n        return chunks\n\n    def warm_cache(self, chunks):\n        if not self.add_cache_headers:\n            return\n        if not self.num_cache_warming_pings:\n            return\n\n        delay = 5 * 60 - 5\n        self.next_cache_warm = time.time() + delay\n        self.warming_pings_left = self.num_cache_warming_pings\n        self.cache_warming_chunks = chunks\n\n        if self.cache_warming_thread:\n            return\n\n        def warm_cache_worker():\n            while True:\n                time.sleep(1)\n                if self.warming_pings_left <= 0:\n                    continue\n                now = time.time()\n                if now < self.next_cache_warm:\n                    continue\n\n                self.warming_pings_left -= 1\n                self.next_cache_warm = time.time() + delay\n\n                kwargs = dict(self.main_model.extra_params) or dict()\n                kwargs[\"max_tokens\"] = 1\n\n                try:\n                    completion = litellm.completion(\n                        model=self.main_model.name,\n                        messages=self.cache_warming_chunks.cacheable_messages(),\n                        stream=False,\n                        **kwargs,\n                    )\n                except Exception as err:\n                    self.io.tool_warning(f\"Cache warming error: {str(err)}\")\n                    continue\n\n                cache_hit_tokens = getattr(\n                    completion.usage, \"prompt_cache_hit_tokens\", 0\n                ) or getattr(completion.usage, \"cache_read_input_tokens\", 0)\n\n                if self.verbose:\n                    self.io.tool_output(f\"Warmed {format_tokens(cache_hit_tokens)} cached tokens.\")\n\n        self.cache_warming_thread = threading.Timer(0, warm_cache_worker)\n        self.cache_warming_thread.daemon = True\n        self.cache_warming_thread.start()\n\n        return chunks",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::25",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 888,
      "span_ids": [
        "Coder.send_message"
      ],
      "start_line": 1229,
      "end_line": 1374,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def send_message(self, inp):\n        self.event(\"message_send_starting\")\n\n        self.cur_messages += [\n            dict(role=\"user\", content=inp),\n        ]\n\n        chunks = self.format_messages()\n        messages = chunks.all_messages()\n        self.warm_cache(chunks)\n\n        if self.verbose:\n            utils.show_messages(messages, functions=self.functions)\n\n        self.multi_response_content = \"\"\n        if self.show_pretty() and self.stream:\n            self.mdstream = self.io.get_assistant_mdstream()\n        else:\n            self.mdstream = None\n\n        retry_delay = 0.125\n\n        litellm_ex = LiteLLMExceptions()\n\n        self.usage_report = None\n        exhausted = False\n        interrupted = False\n        try:\n            while True:\n                try:\n                    yield from self.send(messages, functions=self.functions)\n                    break\n                except litellm_ex.exceptions_tuple() as err:\n                    ex_info = litellm_ex.get_ex_info(err)\n\n                    if ex_info.name == \"ContextWindowExceededError\":\n                        exhausted = True\n                        break\n\n                    should_retry = ex_info.retry\n                    if should_retry:\n                        retry_delay *= 2\n                        if retry_delay > RETRY_TIMEOUT:\n                            should_retry = False\n\n                    if not should_retry:\n                        self.mdstream = None\n                        self.check_and_open_urls(err, ex_info.description)\n                        break\n\n                    err_msg = str(err)\n                    if ex_info.description:\n                        self.io.tool_warning(err_msg)\n                        self.io.tool_error(ex_info.description)\n                    else:\n                        self.io.tool_error(err_msg)\n\n                    self.io.tool_output(f\"Retrying in {retry_delay:.1f} seconds...\")\n                    time.sleep(retry_delay)\n                    continue\n                except KeyboardInterrupt:\n                    interrupted = True\n                    break\n                except FinishReasonLength:\n                    # We hit the output limit!\n                    if not self.main_model.info.get(\"supports_assistant_prefill\"):\n                        exhausted = True\n                        break\n\n                    self.multi_response_content = self.get_multi_response_content()\n\n                    if messages[-1][\"role\"] == \"assistant\":\n                        messages[-1][\"content\"] = self.multi_response_content\n                    else:\n                        messages.append(\n                            dict(role=\"assistant\", content=self.multi_response_content, prefix=True)\n                        )\n                except Exception as err:\n                    self.mdstream = None\n                    lines = traceback.format_exception(type(err), err, err.__traceback__)\n                    self.io.tool_warning(\"\".join(lines))\n                    self.io.tool_error(str(err))\n                    self.event(\"message_send_exception\", exception=str(err))\n                    return\n        finally:\n            if self.mdstream:\n                self.live_incremental_response(True)\n                self.mdstream = None\n\n            self.partial_response_content = self.get_multi_response_content(True)\n            self.multi_response_content = \"\"\n\n        self.io.tool_output()\n\n        self.show_usage_report()\n\n        if exhausted:\n            self.show_exhausted_error()\n            self.num_exhausted_context_windows += 1\n            return\n\n        if self.partial_response_function_call:\n            args = self.parse_partial_args()\n            if args:\n                content = args.get(\"explanation\") or \"\"\n            else:\n                content = \"\"\n        elif self.partial_response_content:\n            content = self.partial_response_content\n        else:\n            content = \"\"\n\n        if not interrupted:\n            add_rel_files_message = self.check_for_file_mentions(content)\n            if add_rel_files_message:\n                if self.reflected_message:\n                    self.reflected_message += \"\\n\\n\" + add_rel_files_message\n                else:\n                    self.reflected_message = add_rel_files_message\n                return\n\n            try:\n                self.reply_completed()\n            except KeyboardInterrupt:\n                interrupted = True\n\n        if interrupted:\n            content += \"\\n^C KeyboardInterrupt\"\n            self.cur_messages += [dict(role=\"assistant\", content=content)]\n            return\n\n        edited = self.apply_updates()\n\n        self.update_cur_messages()\n\n        if edited:\n            self.aider_edited_files.update(edited)\n            saved_message = self.auto_commit(edited)\n\n            if not saved_message and hasattr(self.gpt_prompts, \"files_content_gpt_edits_no_repo\"):\n                saved_message = self.gpt_prompts.files_content_gpt_edits_no_repo\n\n            self.move_back_cur_messages(saved_message)\n\n        if self.reflected_message:\n            return\n        # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::26",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 228,
      "span_ids": [
        "Coder.send_message",
        "Coder.reply_completed"
      ],
      "start_line": 1376,
      "end_line": 1405,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def send_message(self, inp):\n        # ... other code\n\n        if edited and self.auto_lint:\n            lint_errors = self.lint_edited(edited)\n            self.auto_commit(edited, context=\"Ran the linter\")\n            self.lint_outcome = not lint_errors\n            if lint_errors:\n                ok = self.io.confirm_ask(\"Attempt to fix lint errors?\")\n                if ok:\n                    self.reflected_message = lint_errors\n                    self.update_cur_messages()\n                    return\n\n        shared_output = self.run_shell_commands()\n        if shared_output:\n            self.cur_messages += [\n                dict(role=\"user\", content=shared_output),\n                dict(role=\"assistant\", content=\"Ok\"),\n            ]\n\n        if edited and self.auto_test:\n            test_errors = self.commands.cmd_test(self.test_cmd)\n            self.test_outcome = not test_errors\n            if test_errors:\n                ok = self.io.confirm_ask(\"Attempt to fix test errors?\")\n                if ok:\n                    self.reflected_message = test_errors\n                    self.update_cur_messages()\n                    return\n\n    def reply_completed(self):\n        pass",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::27",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 496,
      "span_ids": [
        "Coder.show_exhausted_error"
      ],
      "start_line": 1407,
      "end_line": 1458,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def show_exhausted_error(self):\n        output_tokens = 0\n        if self.partial_response_content:\n            output_tokens = self.main_model.token_count(self.partial_response_content)\n        max_output_tokens = self.main_model.info.get(\"max_output_tokens\") or 0\n\n        input_tokens = self.main_model.token_count(self.format_messages().all_messages())\n        max_input_tokens = self.main_model.info.get(\"max_input_tokens\") or 0\n\n        total_tokens = input_tokens + output_tokens\n\n        fudge = 0.7\n\n        out_err = \"\"\n        if output_tokens >= max_output_tokens * fudge:\n            out_err = \" -- possibly exceeded output limit!\"\n\n        inp_err = \"\"\n        if input_tokens >= max_input_tokens * fudge:\n            inp_err = \" -- possibly exhausted context window!\"\n\n        tot_err = \"\"\n        if total_tokens >= max_input_tokens * fudge:\n            tot_err = \" -- possibly exhausted context window!\"\n\n        res = [\"\", \"\"]\n        res.append(f\"Model {self.main_model.name} has hit a token limit!\")\n        res.append(\"Token counts below are approximate.\")\n        res.append(\"\")\n        res.append(f\"Input tokens: ~{input_tokens:,} of {max_input_tokens:,}{inp_err}\")\n        res.append(f\"Output tokens: ~{output_tokens:,} of {max_output_tokens:,}{out_err}\")\n        res.append(f\"Total tokens: ~{total_tokens:,} of {max_input_tokens:,}{tot_err}\")\n\n        if output_tokens >= max_output_tokens:\n            res.append(\"\")\n            res.append(\"To reduce output tokens:\")\n            res.append(\"- Ask for smaller changes in each request.\")\n            res.append(\"- Break your code into smaller source files.\")\n            if \"diff\" not in self.main_model.edit_format:\n                res.append(\"- Use a stronger model that can return diffs.\")\n\n        if input_tokens >= max_input_tokens or total_tokens >= max_input_tokens:\n            res.append(\"\")\n            res.append(\"To reduce input tokens:\")\n            res.append(\"- Use /tokens to see token usage.\")\n            res.append(\"- Use /drop to remove unneeded files from the chat session.\")\n            res.append(\"- Use /clear to clear the chat history.\")\n            res.append(\"- Break your code into smaller source files.\")\n\n        res = \"\".join([line + \"\\n\" for line in res])\n        self.io.tool_error(res)\n        self.io.offer_url(urls.token_limits)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::28",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 153,
      "span_ids": [
        "Coder.lint_edited",
        "Coder.update_cur_messages"
      ],
      "start_line": 1460,
      "end_line": 1487,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def lint_edited(self, fnames):\n        res = \"\"\n        for fname in fnames:\n            if not fname:\n                continue\n            errors = self.linter.lint(self.abs_root_path(fname))\n\n            if errors:\n                res += \"\\n\"\n                res += errors\n                res += \"\\n\"\n\n        if res:\n            self.io.tool_warning(res)\n\n        return res\n\n    def update_cur_messages(self):\n        if self.partial_response_content:\n            self.cur_messages += [dict(role=\"assistant\", content=self.partial_response_content)]\n        if self.partial_response_function_call:\n            self.cur_messages += [\n                dict(\n                    role=\"assistant\",\n                    content=None,\n                    function_call=self.partial_response_function_call,\n                )\n            ]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::29",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 394,
      "span_ids": [
        "Coder.get_file_mentions"
      ],
      "start_line": 1489,
      "end_line": 1530,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def get_file_mentions(self, content):\n        words = set(word for word in content.split())\n\n        # drop sentence punctuation from the end\n        words = set(word.rstrip(\",.!;:?\") for word in words)\n\n        # strip away all kinds of quotes\n        quotes = \"\".join(['\"', \"'\", \"`\"])\n        words = set(word.strip(quotes) for word in words)\n\n        addable_rel_fnames = self.get_addable_relative_files()\n\n        # Get basenames of files already in chat or read-only\n        existing_basenames = {os.path.basename(f) for f in self.get_inchat_relative_files()} | {\n            os.path.basename(self.get_rel_fname(f)) for f in self.abs_read_only_fnames\n        }\n\n        mentioned_rel_fnames = set()\n        fname_to_rel_fnames = {}\n        for rel_fname in addable_rel_fnames:\n            # Skip files that share a basename with files already in chat\n            if os.path.basename(rel_fname) in existing_basenames:\n                continue\n\n            normalized_rel_fname = rel_fname.replace(\"\\\\\", \"/\")\n            normalized_words = set(word.replace(\"\\\\\", \"/\") for word in words)\n            if normalized_rel_fname in normalized_words:\n                mentioned_rel_fnames.add(rel_fname)\n\n            fname = os.path.basename(rel_fname)\n\n            # Don't add basenames that could be plain words like \"run\" or \"make\"\n            if \"/\" in fname or \"\\\\\" in fname or \".\" in fname or \"_\" in fname or \"-\" in fname:\n                if fname not in fname_to_rel_fnames:\n                    fname_to_rel_fnames[fname] = []\n                fname_to_rel_fnames[fname].append(rel_fname)\n\n        for fname, rel_fnames in fname_to_rel_fnames.items():\n            if len(rel_fnames) == 1 and fname in words:\n                mentioned_rel_fnames.add(rel_fnames[0])\n\n        return mentioned_rel_fnames",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::30",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 147,
      "span_ids": [
        "Coder.check_for_file_mentions"
      ],
      "start_line": 1532,
      "end_line": 1550,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def check_for_file_mentions(self, content):\n        mentioned_rel_fnames = self.get_file_mentions(content)\n\n        new_mentions = mentioned_rel_fnames - self.ignore_mentions\n\n        if not new_mentions:\n            return\n\n        added_fnames = []\n        group = ConfirmGroup(new_mentions)\n        for rel_fname in sorted(new_mentions):\n            if self.io.confirm_ask(f\"Add {rel_fname} to the chat?\", group=group, allow_never=True):\n                self.add_rel_fname(rel_fname)\n                added_fnames.append(rel_fname)\n            else:\n                self.ignore_mentions.add(rel_fname)\n\n        if added_fnames:\n            return prompts.added_files.format(fnames=\", \".join(added_fnames))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::31",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 356,
      "span_ids": [
        "Coder.send"
      ],
      "start_line": 1552,
      "end_line": 1607,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def send(self, messages, model=None, functions=None):\n        if not model:\n            model = self.main_model\n\n        self.partial_response_content = \"\"\n        self.partial_response_function_call = dict()\n\n        self.io.log_llm_history(\"TO LLM\", format_messages(messages))\n\n        if self.main_model.use_temperature:\n            temp = self.temperature\n        else:\n            temp = None\n\n        completion = None\n        try:\n            hash_object, completion = send_completion(\n                model.name,\n                messages,\n                functions,\n                self.stream,\n                temp,\n                extra_params=model.extra_params,\n            )\n            self.chat_completion_call_hashes.append(hash_object.hexdigest())\n\n            if self.stream:\n                yield from self.show_send_output_stream(completion)\n            else:\n                self.show_send_output(completion)\n\n            # Calculate costs for successful responses\n            self.calculate_and_show_tokens_and_cost(messages, completion)\n\n        except LiteLLMExceptions().exceptions_tuple() as err:\n            ex_info = LiteLLMExceptions().get_ex_info(err)\n            if ex_info.name == \"ContextWindowExceededError\":\n                # Still calculate costs for context window errors\n                self.calculate_and_show_tokens_and_cost(messages, completion)\n            raise\n        except KeyboardInterrupt as kbi:\n            self.keyboard_interrupt()\n            raise kbi\n        finally:\n            self.io.log_llm_history(\n                \"LLM RESPONSE\",\n                format_content(\"ASSISTANT\", self.partial_response_content),\n            )\n\n            if self.partial_response_content:\n                self.io.ai_output(self.partial_response_content)\n            elif self.partial_response_function_call:\n                # TODO: push this into subclasses\n                args = self.parse_partial_args()\n                if args:\n                    self.io.ai_output(json.dumps(args, indent=4))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::32",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 295,
      "span_ids": [
        "Coder.show_send_output"
      ],
      "start_line": 1609,
      "end_line": 1651,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def show_send_output(self, completion):\n        if self.verbose:\n            print(completion)\n\n        if not completion.choices:\n            self.io.tool_error(str(completion))\n            return\n\n        show_func_err = None\n        show_content_err = None\n        try:\n            if completion.choices[0].message.tool_calls:\n                self.partial_response_function_call = (\n                    completion.choices[0].message.tool_calls[0].function\n                )\n        except AttributeError as func_err:\n            show_func_err = func_err\n\n        try:\n            self.partial_response_content = completion.choices[0].message.content or \"\"\n        except AttributeError as content_err:\n            show_content_err = content_err\n\n        resp_hash = dict(\n            function_call=str(self.partial_response_function_call),\n            content=self.partial_response_content,\n        )\n        resp_hash = hashlib.sha1(json.dumps(resp_hash, sort_keys=True).encode())\n        self.chat_completion_response_hashes.append(resp_hash.hexdigest())\n\n        if show_func_err and show_content_err:\n            self.io.tool_error(show_func_err)\n            self.io.tool_error(show_content_err)\n            raise Exception(\"No data found in LLM response!\")\n\n        show_resp = self.render_incremental_response(True)\n        self.io.assistant_output(show_resp, pretty=self.show_pretty())\n\n        if (\n            hasattr(completion.choices[0], \"finish_reason\")\n            and completion.choices[0].finish_reason == \"length\"\n        ):\n            raise FinishReasonLength()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::33",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 311,
      "span_ids": [
        "Coder.render_incremental_response",
        "Coder.show_send_output_stream",
        "Coder.live_incremental_response"
      ],
      "start_line": 1653,
      "end_line": 1701,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def show_send_output_stream(self, completion):\n        for chunk in completion:\n            if len(chunk.choices) == 0:\n                continue\n\n            if (\n                hasattr(chunk.choices[0], \"finish_reason\")\n                and chunk.choices[0].finish_reason == \"length\"\n            ):\n                raise FinishReasonLength()\n\n            try:\n                func = chunk.choices[0].delta.function_call\n                # dump(func)\n                for k, v in func.items():\n                    if k in self.partial_response_function_call:\n                        self.partial_response_function_call[k] += v\n                    else:\n                        self.partial_response_function_call[k] = v\n            except AttributeError:\n                pass\n\n            try:\n                text = chunk.choices[0].delta.content\n                if text:\n                    self.partial_response_content += text\n            except AttributeError:\n                text = None\n\n            if self.show_pretty():\n                self.live_incremental_response(False)\n            elif text:\n                try:\n                    sys.stdout.write(text)\n                except UnicodeEncodeError:\n                    # Safely encode and decode the text\n                    safe_text = text.encode(sys.stdout.encoding, errors=\"backslashreplace\").decode(\n                        sys.stdout.encoding\n                    )\n                    sys.stdout.write(safe_text)\n                sys.stdout.flush()\n                yield text\n\n    def live_incremental_response(self, final):\n        show_resp = self.render_incremental_response(final)\n        self.mdstream.update(show_resp, final=final)\n\n    def render_incremental_response(self, final):\n        return self.get_multi_response_content()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::34",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 812,
      "span_ids": [
        "Coder.calculate_and_show_tokens_and_cost"
      ],
      "start_line": 1703,
      "end_line": 1799,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def calculate_and_show_tokens_and_cost(self, messages, completion=None):\n        prompt_tokens = 0\n        completion_tokens = 0\n        cache_hit_tokens = 0\n        cache_write_tokens = 0\n\n        if completion and hasattr(completion, \"usage\") and completion.usage is not None:\n            prompt_tokens = completion.usage.prompt_tokens\n            completion_tokens = completion.usage.completion_tokens\n            cache_hit_tokens = getattr(completion.usage, \"prompt_cache_hit_tokens\", 0) or getattr(\n                completion.usage, \"cache_read_input_tokens\", 0\n            )\n            cache_write_tokens = getattr(completion.usage, \"cache_creation_input_tokens\", 0)\n\n            if hasattr(completion.usage, \"cache_read_input_tokens\") or hasattr(\n                completion.usage, \"cache_creation_input_tokens\"\n            ):\n                self.message_tokens_sent += prompt_tokens\n                self.message_tokens_sent += cache_write_tokens\n            else:\n                self.message_tokens_sent += prompt_tokens\n\n        else:\n            prompt_tokens = self.main_model.token_count(messages)\n            completion_tokens = self.main_model.token_count(self.partial_response_content)\n            self.message_tokens_sent += prompt_tokens\n\n        self.message_tokens_received += completion_tokens\n\n        tokens_report = f\"Tokens: {format_tokens(self.message_tokens_sent)} sent\"\n\n        if cache_write_tokens:\n            tokens_report += f\", {format_tokens(cache_write_tokens)} cache write\"\n        if cache_hit_tokens:\n            tokens_report += f\", {format_tokens(cache_hit_tokens)} cache hit\"\n        tokens_report += f\", {format_tokens(self.message_tokens_received)} received.\"\n\n        if not self.main_model.info.get(\"input_cost_per_token\"):\n            self.usage_report = tokens_report\n            return\n\n        cost = 0\n\n        input_cost_per_token = self.main_model.info.get(\"input_cost_per_token\") or 0\n        output_cost_per_token = self.main_model.info.get(\"output_cost_per_token\") or 0\n        input_cost_per_token_cache_hit = (\n            self.main_model.info.get(\"input_cost_per_token_cache_hit\") or 0\n        )\n\n        # deepseek\n        # prompt_cache_hit_tokens + prompt_cache_miss_tokens\n        #    == prompt_tokens == total tokens that were sent\n        #\n        # Anthropic\n        # cache_creation_input_tokens + cache_read_input_tokens + prompt\n        #    == total tokens that were\n\n        if input_cost_per_token_cache_hit:\n            # must be deepseek\n            cost += input_cost_per_token_cache_hit * cache_hit_tokens\n            cost += (prompt_tokens - input_cost_per_token_cache_hit) * input_cost_per_token\n        else:\n            # hard code the anthropic adjustments, no-ops for other models since cache_x_tokens==0\n            cost += cache_write_tokens * input_cost_per_token * 1.25\n            cost += cache_hit_tokens * input_cost_per_token * 0.10\n            cost += prompt_tokens * input_cost_per_token\n\n        cost += completion_tokens * output_cost_per_token\n\n        self.total_cost += cost\n        self.message_cost += cost\n\n        def format_cost(value):\n            if value == 0:\n                return \"0.00\"\n            magnitude = abs(value)\n            if magnitude >= 0.01:\n                return f\"{value:.2f}\"\n            else:\n                return f\"{value:.{max(2, 2 - int(math.log10(magnitude)))}f}\"\n\n        cost_report = (\n            f\"Cost: ${format_cost(self.message_cost)} message,\"\n            f\" ${format_cost(self.total_cost)} session.\"\n        )\n\n        if self.add_cache_headers and self.stream:\n            warning = \" Use --no-stream for accurate caching costs.\"\n            self.usage_report = tokens_report + \"\\n\" + cost_report + warning\n            return\n\n        if cache_hit_tokens and cache_write_tokens:\n            sep = \"\\n\"\n        else:\n            sep = \" \"\n\n        self.usage_report = tokens_report + sep + cost_report",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::35",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 137,
      "span_ids": [
        "Coder.show_usage_report"
      ],
      "start_line": 1801,
      "end_line": 1822,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def show_usage_report(self):\n        if not self.usage_report:\n            return\n\n        self.io.tool_output(self.usage_report)\n\n        prompt_tokens = self.message_tokens_sent\n        completion_tokens = self.message_tokens_received\n        self.event(\n            \"message_send\",\n            main_model=self.main_model,\n            edit_format=self.edit_format,\n            prompt_tokens=prompt_tokens,\n            completion_tokens=completion_tokens,\n            total_tokens=prompt_tokens + completion_tokens,\n            cost=self.message_cost,\n            total_cost=self.total_cost,\n        )\n\n        self.message_cost = 0.0\n        self.message_tokens_sent = 0\n        self.message_tokens_received = 0",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::36",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 324,
      "span_ids": [
        "Coder.get_all_abs_files",
        "Coder.get_addable_relative_files",
        "Coder.is_file_safe",
        "Coder.get_multi_response_content",
        "Coder.get_all_relative_files",
        "Coder.get_rel_fname",
        "Coder.get_inchat_relative_files"
      ],
      "start_line": 1824,
      "end_line": 1868,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def get_multi_response_content(self, final=False):\n        cur = self.multi_response_content or \"\"\n        new = self.partial_response_content or \"\"\n\n        if new.rstrip() != new and not final:\n            new = new.rstrip()\n        return cur + new\n\n    def get_rel_fname(self, fname):\n        try:\n            return os.path.relpath(fname, self.root)\n        except ValueError:\n            return fname\n\n    def get_inchat_relative_files(self):\n        files = [self.get_rel_fname(fname) for fname in self.abs_fnames]\n        return sorted(set(files))\n\n    def is_file_safe(self, fname):\n        try:\n            return Path(self.abs_root_path(fname)).is_file()\n        except OSError:\n            return\n\n    def get_all_relative_files(self):\n        if self.repo:\n            files = self.repo.get_tracked_files()\n        else:\n            files = self.get_inchat_relative_files()\n\n        # This is quite slow in large repos\n        # files = [fname for fname in files if self.is_file_safe(fname)]\n\n        return sorted(set(files))\n\n    def get_all_abs_files(self):\n        files = self.get_all_relative_files()\n        files = [self.abs_root_path(path) for path in files]\n        return files\n\n    def get_addable_relative_files(self):\n        all_files = set(self.get_all_relative_files())\n        inchat_files = set(self.get_inchat_relative_files())\n        read_only_files = set(self.get_rel_fname(fname) for fname in self.abs_read_only_fnames)\n        return all_files - inchat_files - read_only_files",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::37",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 121,
      "span_ids": [
        "Coder.check_for_dirty_commit"
      ],
      "start_line": 1870,
      "end_line": 1884,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def check_for_dirty_commit(self, path):\n        if not self.repo:\n            return\n        if not self.dirty_commits:\n            return\n        if not self.repo.is_dirty(path):\n            return\n\n        # We need a committed copy of the file in order to /undo, so skip this\n        # fullp = Path(self.abs_root_path(path))\n        # if not fullp.stat().st_size:\n        #     return\n\n        self.io.tool_output(f\"Committing {path} before applying edits.\")\n        self.need_commit_before_edits.add(path)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::38",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 347,
      "span_ids": [
        "Coder.allowed_to_edit"
      ],
      "start_line": 1886,
      "end_line": 1935,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def allowed_to_edit(self, path):\n        full_path = self.abs_root_path(path)\n        if self.repo:\n            need_to_add = not self.repo.path_in_repo(path)\n        else:\n            need_to_add = False\n\n        if full_path in self.abs_fnames:\n            self.check_for_dirty_commit(path)\n            return True\n\n        if self.repo and self.repo.git_ignored_file(path):\n            self.io.tool_warning(f\"Skipping edits to {path} that matches gitignore spec.\")\n            return\n\n        if not Path(full_path).exists():\n            if not self.io.confirm_ask(\"Create new file?\", subject=path):\n                self.io.tool_output(f\"Skipping edits to {path}\")\n                return\n\n            if not self.dry_run:\n                if not utils.touch_file(full_path):\n                    self.io.tool_error(f\"Unable to create {path}, skipping edits.\")\n                    return\n\n                # Seems unlikely that we needed to create the file, but it was\n                # actually already part of the repo.\n                # But let's only add if we need to, just to be safe.\n                if need_to_add:\n                    self.repo.repo.git.add(full_path)\n\n            self.abs_fnames.add(full_path)\n            self.check_added_files()\n            return True\n\n        if not self.io.confirm_ask(\n            \"Allow edits to file that has not been added to the chat?\",\n            subject=path,\n        ):\n            self.io.tool_output(f\"Skipping edits to {path}\")\n            return\n\n        if need_to_add:\n            self.repo.repo.git.add(full_path)\n\n        self.abs_fnames.add(full_path)\n        self.check_added_files()\n        self.check_for_dirty_commit(path)\n\n        return True",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::39",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 164,
      "span_ids": [
        "Coder:76",
        "Coder.check_added_files"
      ],
      "start_line": 1937,
      "end_line": 1962,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    warning_given = False\n\n    def check_added_files(self):\n        if self.warning_given:\n            return\n\n        warn_number_of_files = 4\n        warn_number_of_tokens = 20 * 1024\n\n        num_files = len(self.abs_fnames)\n        if num_files < warn_number_of_files:\n            return\n\n        tokens = 0\n        for fname in self.abs_fnames:\n            if is_image_file(fname):\n                continue\n            content = self.io.read_text(fname)\n            tokens += self.main_model.token_count(content)\n\n        if tokens < warn_number_of_tokens:\n            return\n\n        self.io.tool_warning(\"Warning: it's best to only add files that need changes to the chat.\")\n        self.io.tool_warning(urls.edit_errors)\n        self.warning_given = True",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::40",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 131,
      "span_ids": [
        "Coder.prepare_to_edit"
      ],
      "start_line": 1964,
      "end_line": 1989,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def prepare_to_edit(self, edits):\n        res = []\n        seen = dict()\n\n        self.need_commit_before_edits = set()\n\n        for edit in edits:\n            path = edit[0]\n            if path is None:\n                res.append(edit)\n                continue\n            if path == \"python\":\n                dump(edits)\n            if path in seen:\n                allowed = seen[path]\n            else:\n                allowed = self.allowed_to_edit(path)\n                seen[path] = allowed\n\n            if allowed:\n                res.append(edit)\n\n        self.dirty_commit()\n        self.need_commit_before_edits = set()\n\n        return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::41",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 266,
      "span_ids": [
        "Coder.apply_updates"
      ],
      "start_line": 1991,
      "end_line": 2031,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def apply_updates(self):\n        edited = set()\n        try:\n            edits = self.get_edits()\n            edits = self.apply_edits_dry_run(edits)\n            edits = self.prepare_to_edit(edits)\n            edited = set(edit[0] for edit in edits)\n\n            self.apply_edits(edits)\n        except ValueError as err:\n            self.num_malformed_responses += 1\n\n            err = err.args[0]\n\n            self.io.tool_error(\"The LLM did not conform to the edit format.\")\n            self.io.tool_output(urls.edit_errors)\n            self.io.tool_output()\n            self.io.tool_output(str(err))\n\n            self.reflected_message = str(err)\n            return edited\n\n        except ANY_GIT_ERROR as err:\n            self.io.tool_error(str(err))\n            return edited\n        except Exception as err:\n            self.io.tool_error(\"Exception while updating files:\")\n            self.io.tool_error(str(err), strip=False)\n\n            traceback.print_exc()\n\n            self.reflected_message = str(err)\n            return edited\n\n        for path in edited:\n            if self.dry_run:\n                self.io.tool_output(f\"Did not apply edit to {path} (--dry-run)\")\n            else:\n                self.io.tool_output(f\"Applied edit to {path}\")\n\n        return edited",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::42",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 180,
      "span_ids": [
        "Coder.parse_partial_args",
        "Coder.get_context_from_history"
      ],
      "start_line": 2033,
      "end_line": 2068,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def parse_partial_args(self):\n        # dump(self.partial_response_function_call)\n\n        data = self.partial_response_function_call.get(\"arguments\")\n        if not data:\n            return\n\n        try:\n            return json.loads(data)\n        except JSONDecodeError:\n            pass\n\n        try:\n            return json.loads(data + \"]}\")\n        except JSONDecodeError:\n            pass\n\n        try:\n            return json.loads(data + \"}]}\")\n        except JSONDecodeError:\n            pass\n\n        try:\n            return json.loads(data + '\"}]}')\n        except JSONDecodeError:\n            pass\n\n    # commits...\n\n    def get_context_from_history(self, history):\n        context = \"\"\n        if history:\n            for msg in history:\n                context += \"\\n\" + msg[\"role\"].upper() + \": \" + msg[\"content\"] + \"\\n\"\n\n        return context",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::43",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 167,
      "span_ids": [
        "Coder.auto_commit"
      ],
      "start_line": 2070,
      "end_line": 2090,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def auto_commit(self, edited, context=None):\n        if not self.repo or not self.auto_commits or self.dry_run:\n            return\n\n        if not context:\n            context = self.get_context_from_history(self.cur_messages)\n\n        try:\n            res = self.repo.commit(fnames=edited, context=context, aider_edits=True)\n            if res:\n                self.show_auto_commit_outcome(res)\n                commit_hash, commit_message = res\n                return self.gpt_prompts.files_content_gpt_edits.format(\n                    hash=commit_hash,\n                    message=commit_message,\n                )\n\n            return self.gpt_prompts.files_content_gpt_no_edits\n        except ANY_GIT_ERROR as err:\n            self.io.tool_error(f\"Unable to commit: {str(err)}\")\n            return",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::44",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 345,
      "span_ids": [
        "Coder.get_edits",
        "Coder.apply_edits",
        "Coder.run_shell_commands",
        "Coder.show_auto_commit_outcome",
        "Coder.apply_edits_dry_run",
        "Coder.show_undo_hint",
        "Coder.dirty_commit"
      ],
      "start_line": 2092,
      "end_line": 2143,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def show_auto_commit_outcome(self, res):\n        commit_hash, commit_message = res\n        self.last_aider_commit_hash = commit_hash\n        self.aider_commit_hashes.add(commit_hash)\n        self.last_aider_commit_message = commit_message\n        if self.show_diffs:\n            self.commands.cmd_diff()\n\n    def show_undo_hint(self):\n        if not self.commit_before_message:\n            return\n        if self.commit_before_message[-1] != self.repo.get_head_commit_sha():\n            self.io.tool_output(\"You can use /undo to undo and discard each aider commit.\")\n\n    def dirty_commit(self):\n        if not self.need_commit_before_edits:\n            return\n        if not self.dirty_commits:\n            return\n        if not self.repo:\n            return\n\n        self.repo.commit(fnames=self.need_commit_before_edits)\n\n        # files changed, move cur messages back behind the files messages\n        # self.move_back_cur_messages(self.gpt_prompts.files_content_local_edits)\n        return True\n\n    def get_edits(self, mode=\"update\"):\n        return []\n\n    def apply_edits(self, edits):\n        return\n\n    def apply_edits_dry_run(self, edits):\n        return edits\n\n    def run_shell_commands(self):\n        if not self.suggest_shell_commands:\n            return \"\"\n\n        done = set()\n        group = ConfirmGroup(set(self.shell_commands))\n        accumulated_output = \"\"\n        for command in self.shell_commands:\n            if command in done:\n                continue\n            done.add(command)\n            output = self.handle_shell_commands(command, group)\n            if output:\n                accumulated_output += output + \"\\n\\n\"\n        return accumulated_output",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_coder.py::45",
    "metadata": {
      "file_path": "aider/coders/base_coder.py",
      "file_name": "base_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 311,
      "span_ids": [
        "Coder.handle_shell_commands"
      ],
      "start_line": 2145,
      "end_line": 2181,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Coder:\n\n    def handle_shell_commands(self, commands_str, group):\n        commands = commands_str.strip().splitlines()\n        command_count = sum(\n            1 for cmd in commands if cmd.strip() and not cmd.strip().startswith(\"#\")\n        )\n        prompt = \"Run shell command?\" if command_count == 1 else \"Run shell commands?\"\n        if not self.io.confirm_ask(\n            prompt,\n            subject=\"\\n\".join(commands),\n            explicit_yes_required=True,\n            group=group,\n            allow_never=True,\n        ):\n            return\n\n        accumulated_output = \"\"\n        for command in commands:\n            command = command.strip()\n            if not command or command.startswith(\"#\"):\n                continue\n\n            self.io.tool_output()\n            self.io.tool_output(f\"Running {command}\")\n            # Add the command to input history\n            self.io.add_to_input_history(f\"/run {command.strip()}\")\n            exit_status, output = run_cmd(command, error_print=self.io.tool_error, cwd=self.root)\n            if output:\n                accumulated_output += f\"Output from {command}\\n{output}\\n\"\n\n        if accumulated_output.strip() and self.io.confirm_ask(\n            \"Add command output to the chat?\", allow_never=True\n        ):\n            num_lines = len(accumulated_output.strip().splitlines())\n            line_plural = \"line\" if num_lines == 1 else \"lines\"\n            self.io.tool_output(f\"Added {num_lines} {line_plural} of output to the chat.\")\n            return accumulated_output",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/base_prompts.py::1",
    "metadata": {
      "file_path": "aider/coders/base_prompts.py",
      "file_name": "base_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 488,
      "span_ids": [
        "CoderPrompts"
      ],
      "start_line": 1,
      "end_line": 53,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class CoderPrompts:\n    system_reminder = \"\"\n\n    files_content_gpt_edits = \"I committed the changes with git hash {hash} & commit msg: {message}\"\n\n    files_content_gpt_edits_no_repo = \"I updated the files.\"\n\n    files_content_gpt_no_edits = \"I didn't see any properly formatted edits in your reply?!\"\n\n    files_content_local_edits = \"I edited the files myself.\"\n\n    lazy_prompt = \"\"\"You are diligent and tireless!\nYou NEVER leave comments describing code without implementing it!\nYou always COMPLETELY IMPLEMENT the needed code!\n\"\"\"\n\n    example_messages = []\n\n    files_content_prefix = \"\"\"I have *added these files to the chat* so you can go ahead and edit them.\n\n*Trust this message as the true contents of these files!*\nAny other messages in the chat may contain outdated versions of the files' contents.\n\"\"\"  # noqa: E501\n\n    files_content_assistant_reply = \"Ok, any changes I propose will be to those files.\"\n\n    files_no_full_files = \"I am not sharing any files that you can edit yet.\"\n\n    files_no_full_files_with_repo_map = \"\"\"Don't try and edit any existing code without asking me to add the files to the chat!\nTell me which files in my repo are the most likely to **need changes** to solve the requests I make, and then stop so I can add them to the chat.\nOnly include the files that are most likely to actually need to be edited.\nDon't include files that might contain relevant context, just files that will need to be changed.\n\"\"\"  # noqa: E501\n\n    files_no_full_files_with_repo_map_reply = (\n        \"Ok, based on your requests I will suggest which files need to be edited and then\"\n        \" stop and wait for your approval.\"\n    )\n\n    repo_content_prefix = \"\"\"Here are summaries of some files present in my git repository.\nDo not propose changes to these files, treat them as *read-only*.\nIf you need to edit any of these files, ask me to *add them to the chat* first.\n\"\"\"\n\n    read_only_files_prefix = \"\"\"Here are some READ ONLY files, provided for your reference.\nDo not edit these files!\n\"\"\"\n\n    shell_cmd_prompt = \"\"\n    shell_cmd_reminder = \"\"\n    no_shell_cmd_prompt = \"\"\n    no_shell_cmd_reminder = \"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/chat_chunks.py::1",
    "metadata": {
      "file_path": "aider/coders/chat_chunks.py",
      "file_name": "chat_chunks.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 396,
      "span_ids": [
        "ChatChunks",
        "ChatChunks.add_cache_control",
        "ChatChunks.all_messages",
        "ChatChunks.add_cache_control_headers",
        "ChatChunks.cacheable_messages",
        "imports"
      ],
      "start_line": 1,
      "end_line": 65,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from dataclasses import dataclass, field\nfrom typing import List\n\n\n@dataclass\nclass ChatChunks:\n    system: List = field(default_factory=list)\n    examples: List = field(default_factory=list)\n    done: List = field(default_factory=list)\n    repo: List = field(default_factory=list)\n    readonly_files: List = field(default_factory=list)\n    chat_files: List = field(default_factory=list)\n    cur: List = field(default_factory=list)\n    reminder: List = field(default_factory=list)\n\n    def all_messages(self):\n        return (\n            self.system\n            + self.examples\n            + self.readonly_files\n            + self.repo\n            + self.done\n            + self.chat_files\n            + self.cur\n            + self.reminder\n        )\n\n    def add_cache_control_headers(self):\n        if self.examples:\n            self.add_cache_control(self.examples)\n        else:\n            self.add_cache_control(self.system)\n\n        if self.repo:\n            # this will mark both the readonly_files and repomap chunk as cacheable\n            self.add_cache_control(self.repo)\n        else:\n            # otherwise, just cache readonly_files if there are any\n            self.add_cache_control(self.readonly_files)\n\n        self.add_cache_control(self.chat_files)\n\n    def add_cache_control(self, messages):\n        if not messages:\n            return\n\n        content = messages[-1][\"content\"]\n        if type(content) is str:\n            content = dict(\n                type=\"text\",\n                text=content,\n            )\n        content[\"cache_control\"] = {\"type\": \"ephemeral\"}\n\n        messages[-1][\"content\"] = [content]\n\n    def cacheable_messages(self):\n        messages = self.all_messages()\n        for i, message in enumerate(reversed(messages)):\n            if isinstance(message.get(\"content\"), list) and message[\"content\"][0].get(\n                \"cache_control\"\n            ):\n                return messages[: len(messages) - i]\n        return messages",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_coder.py::1",
    "metadata": {
      "file_path": "aider/coders/editblock_coder.py",
      "file_name": "editblock_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 233,
      "span_ids": [
        "EditBlockCoder",
        "imports",
        "EditBlockCoder.get_edits",
        "EditBlockCoder.apply_edits_dry_run"
      ],
      "start_line": 1,
      "end_line": 39,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import difflib\nimport math\nimport re\nimport sys\nfrom difflib import SequenceMatcher\nfrom pathlib import Path\n\nfrom aider import utils\n\nfrom ..dump import dump  # noqa: F401\nfrom .base_coder import Coder\nfrom .editblock_prompts import EditBlockPrompts\n\n\nclass EditBlockCoder(Coder):\n    \"\"\"A coder that uses search/replace blocks for code modifications.\"\"\"\n\n    edit_format = \"diff\"\n    gpt_prompts = EditBlockPrompts()\n\n    def get_edits(self):\n        content = self.partial_response_content\n\n        # might raise ValueError for malformed ORIG/UPD blocks\n        edits = list(\n            find_original_update_blocks(\n                content,\n                self.fence,\n                self.get_inchat_relative_files(),\n            )\n        )\n\n        self.shell_commands += [edit[1] for edit in edits if edit[0] is None]\n        edits = [edit for edit in edits if edit[0] is not None]\n\n        return edits\n\n    def apply_edits_dry_run(self, edits):\n        return self.apply_edits(edits, dry_run=True)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_coder.py::2",
    "metadata": {
      "file_path": "aider/coders/editblock_coder.py",
      "file_name": "editblock_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 617,
      "span_ids": [
        "EditBlockCoder.apply_edits"
      ],
      "start_line": 41,
      "end_line": 124,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class EditBlockCoder(Coder):\n\n    def apply_edits(self, edits, dry_run=False):\n        failed = []\n        passed = []\n        updated_edits = []\n\n        for edit in edits:\n            path, original, updated = edit\n            full_path = self.abs_root_path(path)\n            new_content = None\n\n            if Path(full_path).exists():\n                content = self.io.read_text(full_path)\n                new_content = do_replace(full_path, content, original, updated, self.fence)\n\n            # If the edit failed, and\n            # this is not a \"create a new file\" with an empty original...\n            # https://github.com/Aider-AI/aider/issues/2258\n            if not new_content and original.strip():\n                # try patching any of the other files in the chat\n                for full_path in self.abs_fnames:\n                    content = self.io.read_text(full_path)\n                    new_content = do_replace(full_path, content, original, updated, self.fence)\n                    if new_content:\n                        path = self.get_rel_fname(full_path)\n                        break\n\n            updated_edits.append((path, original, updated))\n\n            if new_content:\n                if not dry_run:\n                    self.io.write_text(full_path, new_content)\n                passed.append(edit)\n            else:\n                failed.append(edit)\n\n        if dry_run:\n            return updated_edits\n\n        if not failed:\n            return\n\n        blocks = \"block\" if len(failed) == 1 else \"blocks\"\n\n        res = f\"# {len(failed)} SEARCH/REPLACE {blocks} failed to match!\\n\"\n        for edit in failed:\n            path, original, updated = edit\n\n            full_path = self.abs_root_path(path)\n            content = self.io.read_text(full_path)\n\n            res += f\"\"\"\n## SearchReplaceNoExactMatch: This SEARCH block failed to exactly match lines in {path}\n<<<<<<< SEARCH\n{original}=======\n{updated}>>>>>>> REPLACE\n\n\"\"\"\n            did_you_mean = find_similar_lines(original, content)\n            if did_you_mean:\n                res += f\"\"\"Did you mean to match some of these actual lines from {path}?\n\n{self.fence[0]}\n{did_you_mean}\n{self.fence[1]}\n\n\"\"\"\n\n            if updated in content and updated:\n                res += f\"\"\"Are you sure you need this SEARCH/REPLACE block?\nThe REPLACE lines are already in {path}!\n\n\"\"\"\n        res += (\n            \"The SEARCH section must exactly match an existing block of lines including all white\"\n            \" space, comments, indentation, docstrings, etc\\n\"\n        )\n        if passed:\n            pblocks = \"block\" if len(passed) == 1 else \"blocks\"\n            res += f\"\"\"\n# The other {len(passed)} SEARCH/REPLACE {pblocks} were applied successfully.\nDon't re-send them.\nJust reply with fixed versions of the {blocks} above that failed to match.\n\"\"\"\n        raise ValueError(res)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_coder.py::3",
    "metadata": {
      "file_path": "aider/coders/editblock_coder.py",
      "file_name": "editblock_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 217,
      "span_ids": [
        "prep",
        "perfect_or_whitespace",
        "perfect_replace"
      ],
      "start_line": 127,
      "end_line": 154,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def prep(content):\n    if content and not content.endswith(\"\\n\"):\n        content += \"\\n\"\n    lines = content.splitlines(keepends=True)\n    return content, lines\n\n\ndef perfect_or_whitespace(whole_lines, part_lines, replace_lines):\n    # Try for a perfect match\n    res = perfect_replace(whole_lines, part_lines, replace_lines)\n    if res:\n        return res\n\n    # Try being flexible about leading whitespace\n    res = replace_part_with_missing_leading_whitespace(whole_lines, part_lines, replace_lines)\n    if res:\n        return res\n\n\ndef perfect_replace(whole_lines, part_lines, replace_lines):\n    part_tup = tuple(part_lines)\n    part_len = len(part_lines)\n\n    for i in range(len(whole_lines) - part_len + 1):\n        whole_tup = tuple(whole_lines[i : i + part_len])\n        if part_tup == whole_tup:\n            res = whole_lines[:i] + replace_lines + whole_lines[i + part_len :]\n            return \"\".join(res)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_coder.py::4",
    "metadata": {
      "file_path": "aider/coders/editblock_coder.py",
      "file_name": "editblock_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 245,
      "span_ids": [
        "replace_most_similar_chunk"
      ],
      "start_line": 157,
      "end_line": 187,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def replace_most_similar_chunk(whole, part, replace):\n    \"\"\"Best efforts to find the `part` lines in `whole` and replace them with `replace`\"\"\"\n\n    whole, whole_lines = prep(whole)\n    part, part_lines = prep(part)\n    replace, replace_lines = prep(replace)\n\n    res = perfect_or_whitespace(whole_lines, part_lines, replace_lines)\n    if res:\n        return res\n\n    # drop leading empty line, GPT sometimes adds them spuriously (issue #25)\n    if len(part_lines) > 2 and not part_lines[0].strip():\n        skip_blank_line_part_lines = part_lines[1:]\n        res = perfect_or_whitespace(whole_lines, skip_blank_line_part_lines, replace_lines)\n        if res:\n            return res\n\n    # Try to handle when it elides code with ...\n    try:\n        res = try_dotdotdots(whole, part, replace)\n        if res:\n            return res\n    except ValueError:\n        pass\n\n    return\n    # Try fuzzy matching\n    res = replace_closest_edit_distance(whole_lines, part, part_lines, replace_lines)\n    if res:\n        return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_coder.py::5",
    "metadata": {
      "file_path": "aider/coders/editblock_coder.py",
      "file_name": "editblock_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 386,
      "span_ids": [
        "try_dotdotdots"
      ],
      "start_line": 190,
      "end_line": 240,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def try_dotdotdots(whole, part, replace):\n    \"\"\"\n    See if the edit block has ... lines.\n    If not, return none.\n\n    If yes, try and do a perfect edit with the ... chunks.\n    If there's a mismatch or otherwise imperfect edit, raise ValueError.\n\n    If perfect edit succeeds, return the updated whole.\n    \"\"\"\n\n    dots_re = re.compile(r\"(^\\s*\\.\\.\\.\\n)\", re.MULTILINE | re.DOTALL)\n\n    part_pieces = re.split(dots_re, part)\n    replace_pieces = re.split(dots_re, replace)\n\n    if len(part_pieces) != len(replace_pieces):\n        raise ValueError(\"Unpaired ... in SEARCH/REPLACE block\")\n\n    if len(part_pieces) == 1:\n        # no dots in this edit block, just return None\n        return\n\n    # Compare odd strings in part_pieces and replace_pieces\n    all_dots_match = all(part_pieces[i] == replace_pieces[i] for i in range(1, len(part_pieces), 2))\n\n    if not all_dots_match:\n        raise ValueError(\"Unmatched ... in SEARCH/REPLACE block\")\n\n    part_pieces = [part_pieces[i] for i in range(0, len(part_pieces), 2)]\n    replace_pieces = [replace_pieces[i] for i in range(0, len(replace_pieces), 2)]\n\n    pairs = zip(part_pieces, replace_pieces)\n    for part, replace in pairs:\n        if not part and not replace:\n            continue\n\n        if not part and replace:\n            if not whole.endswith(\"\\n\"):\n                whole += \"\\n\"\n            whole += replace\n            continue\n\n        if whole.count(part) == 0:\n            raise ValueError\n        if whole.count(part) > 1:\n            raise ValueError\n\n        whole = whole.replace(part, replace, 1)\n\n    return whole",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_coder.py::6",
    "metadata": {
      "file_path": "aider/coders/editblock_coder.py",
      "file_name": "editblock_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 324,
      "span_ids": [
        "replace_part_with_missing_leading_whitespace"
      ],
      "start_line": 243,
      "end_line": 273,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def replace_part_with_missing_leading_whitespace(whole_lines, part_lines, replace_lines):\n    # GPT often messes up leading whitespace.\n    # It usually does it uniformly across the ORIG and UPD blocks.\n    # Either omitting all leading whitespace, or including only some of it.\n\n    # Outdent everything in part_lines and replace_lines by the max fixed amount possible\n    leading = [len(p) - len(p.lstrip()) for p in part_lines if p.strip()] + [\n        len(p) - len(p.lstrip()) for p in replace_lines if p.strip()\n    ]\n\n    if leading and min(leading):\n        num_leading = min(leading)\n        part_lines = [p[num_leading:] if p.strip() else p for p in part_lines]\n        replace_lines = [p[num_leading:] if p.strip() else p for p in replace_lines]\n\n    # can we find an exact match not including the leading whitespace\n    num_part_lines = len(part_lines)\n\n    for i in range(len(whole_lines) - num_part_lines + 1):\n        add_leading = match_but_for_leading_whitespace(\n            whole_lines[i : i + num_part_lines], part_lines\n        )\n\n        if add_leading is None:\n            continue\n\n        replace_lines = [add_leading + rline if rline.strip() else rline for rline in replace_lines]\n        whole_lines = whole_lines[:i] + replace_lines + whole_lines[i + num_part_lines :]\n        return \"\".join(whole_lines)\n\n    return None",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_coder.py::7",
    "metadata": {
      "file_path": "aider/coders/editblock_coder.py",
      "file_name": "editblock_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 127,
      "span_ids": [
        "match_but_for_leading_whitespace"
      ],
      "start_line": 276,
      "end_line": 293,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def match_but_for_leading_whitespace(whole_lines, part_lines):\n    num = len(whole_lines)\n\n    # does the non-whitespace all agree?\n    if not all(whole_lines[i].lstrip() == part_lines[i].lstrip() for i in range(num)):\n        return\n\n    # are they all offset the same?\n    add = set(\n        whole_lines[i][: len(whole_lines[i]) - len(part_lines[i])]\n        for i in range(num)\n        if whole_lines[i].strip()\n    )\n\n    if len(add) != 1:\n        return\n\n    return add.pop()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_coder.py::8",
    "metadata": {
      "file_path": "aider/coders/editblock_coder.py",
      "file_name": "editblock_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 250,
      "span_ids": [
        "replace_closest_edit_distance"
      ],
      "start_line": 296,
      "end_line": 329,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def replace_closest_edit_distance(whole_lines, part, part_lines, replace_lines):\n    similarity_thresh = 0.8\n\n    max_similarity = 0\n    most_similar_chunk_start = -1\n    most_similar_chunk_end = -1\n\n    scale = 0.1\n    min_len = math.floor(len(part_lines) * (1 - scale))\n    max_len = math.ceil(len(part_lines) * (1 + scale))\n\n    for length in range(min_len, max_len):\n        for i in range(len(whole_lines) - length + 1):\n            chunk = whole_lines[i : i + length]\n            chunk = \"\".join(chunk)\n\n            similarity = SequenceMatcher(None, chunk, part).ratio()\n\n            if similarity > max_similarity and similarity:\n                max_similarity = similarity\n                most_similar_chunk_start = i\n                most_similar_chunk_end = i + length\n\n    if max_similarity < similarity_thresh:\n        return\n\n    modified_whole = (\n        whole_lines[:most_similar_chunk_start]\n        + replace_lines\n        + whole_lines[most_similar_chunk_end:]\n    )\n    modified_whole = \"\".join(modified_whole)\n\n    return modified_whole",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_coder.py::9",
    "metadata": {
      "file_path": "aider/coders/editblock_coder.py",
      "file_name": "editblock_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 189,
      "span_ids": [
        "impl",
        "strip_quoted_wrapping"
      ],
      "start_line": 332,
      "end_line": 361,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "DEFAULT_FENCE = (\"`\" * 3, \"`\" * 3)\n\n\ndef strip_quoted_wrapping(res, fname=None, fence=DEFAULT_FENCE):\n    \"\"\"\n    Given an input string which may have extra \"wrapping\" around it, remove the wrapping.\n    For example:\n\n    filename.ext\n    ```\n    We just want this content\n    Not the filename and triple quotes\n    ```\n    \"\"\"\n    if not res:\n        return res\n\n    res = res.splitlines()\n\n    if fname and res[0].strip().endswith(Path(fname).name):\n        res = res[1:]\n\n    if res[0].startswith(fence[0]) and res[-1].startswith(fence[1]):\n        res = res[1:-1]\n\n    res = \"\\n\".join(res)\n    if res and res[-1] != \"\\n\":\n        res += \"\\n\"\n\n    return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_coder.py::10",
    "metadata": {
      "file_path": "aider/coders/editblock_coder.py",
      "file_name": "editblock_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 147,
      "span_ids": [
        "do_replace"
      ],
      "start_line": 364,
      "end_line": 383,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def do_replace(fname, content, before_text, after_text, fence=None):\n    before_text = strip_quoted_wrapping(before_text, fname, fence)\n    after_text = strip_quoted_wrapping(after_text, fname, fence)\n    fname = Path(fname)\n\n    # does it want to make a new file?\n    if not fname.exists() and not before_text.strip():\n        fname.touch()\n        content = \"\"\n\n    if content is None:\n        return\n\n    if not before_text.strip():\n        # append to existing file, or start a new file\n        new_content = content + after_text\n    else:\n        new_content = replace_most_similar_chunk(content, before_text, after_text)\n\n    return new_content",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_coder.py::11",
    "metadata": {
      "file_path": "aider/coders/editblock_coder.py",
      "file_name": "editblock_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 245,
      "span_ids": [
        "strip_filename",
        "impl:3"
      ],
      "start_line": 386,
      "end_line": 424,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "HEAD = r\"^<{5,9} SEARCH\\s*$\"\nDIVIDER = r\"^={5,9}\\s*$\"\nUPDATED = r\"^>{5,9} REPLACE\\s*$\"\n\nHEAD_ERR = \"<<<<<<< SEARCH\"\nDIVIDER_ERR = \"=======\"\nUPDATED_ERR = \">>>>>>> REPLACE\"\n\nseparators = \"|\".join([HEAD, DIVIDER, UPDATED])\n\nsplit_re = re.compile(r\"^((?:\" + separators + r\")[ ]*\\n)\", re.MULTILINE | re.DOTALL)\n\n\nmissing_filename_err = (\n    \"Bad/missing filename. The filename must be alone on the line before the opening fence\"\n    \" {fence[0]}\"\n)\n\n\ndef strip_filename(filename, fence):\n    filename = filename.strip()\n\n    if filename == \"...\":\n        return\n\n    start_fence = fence[0]\n    if filename.startswith(start_fence):\n        return\n\n    filename = filename.rstrip(\":\")\n    filename = filename.lstrip(\"#\")\n    filename = filename.strip()\n    filename = filename.strip(\"`\")\n    filename = filename.strip(\"*\")\n\n    # https://github.com/Aider-AI/aider/issues/1158\n    # filename = filename.replace(\"\\\\_\", \"_\")\n\n    return filename",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_coder.py::12",
    "metadata": {
      "file_path": "aider/coders/editblock_coder.py",
      "file_name": "editblock_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 674,
      "span_ids": [
        "find_original_update_blocks"
      ],
      "start_line": 427,
      "end_line": 516,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def find_original_update_blocks(content, fence=DEFAULT_FENCE, valid_fnames=None):\n    lines = content.splitlines(keepends=True)\n    i = 0\n    current_filename = None\n\n    head_pattern = re.compile(HEAD)\n    divider_pattern = re.compile(DIVIDER)\n    updated_pattern = re.compile(UPDATED)\n\n    while i < len(lines):\n        line = lines[i]\n\n        # Check for shell code blocks\n        shell_starts = [\n            \"```bash\",\n            \"```sh\",\n            \"```shell\",\n            \"```cmd\",\n            \"```batch\",\n            \"```powershell\",\n            \"```ps1\",\n            \"```zsh\",\n            \"```fish\",\n            \"```ksh\",\n            \"```csh\",\n            \"```tcsh\",\n        ]\n        next_is_editblock = i + 1 < len(lines) and head_pattern.match(lines[i + 1].strip())\n\n        if any(line.strip().startswith(start) for start in shell_starts) and not next_is_editblock:\n            shell_content = []\n            i += 1\n            while i < len(lines) and not lines[i].strip().startswith(\"```\"):\n                shell_content.append(lines[i])\n                i += 1\n            if i < len(lines) and lines[i].strip().startswith(\"```\"):\n                i += 1  # Skip the closing ```\n\n            yield None, \"\".join(shell_content)\n            continue\n\n        # Check for SEARCH/REPLACE blocks\n        if head_pattern.match(line.strip()):\n            try:\n                # if next line after HEAD exists and is DIVIDER, it's a new file\n                if i + 1 < len(lines) and divider_pattern.match(lines[i + 1].strip()):\n                    filename = find_filename(lines[max(0, i - 3) : i], fence, None)\n                else:\n                    filename = find_filename(lines[max(0, i - 3) : i], fence, valid_fnames)\n\n                if not filename:\n                    if current_filename:\n                        filename = current_filename\n                    else:\n                        raise ValueError(missing_filename_err.format(fence=fence))\n\n                current_filename = filename\n\n                original_text = []\n                i += 1\n                while i < len(lines) and not divider_pattern.match(lines[i].strip()):\n                    original_text.append(lines[i])\n                    i += 1\n\n                if i >= len(lines) or not divider_pattern.match(lines[i].strip()):\n                    raise ValueError(f\"Expected `{DIVIDER_ERR}`\")\n\n                updated_text = []\n                i += 1\n                while i < len(lines) and not (\n                    updated_pattern.match(lines[i].strip())\n                    or divider_pattern.match(lines[i].strip())\n                ):\n                    updated_text.append(lines[i])\n                    i += 1\n\n                if i >= len(lines) or not (\n                    updated_pattern.match(lines[i].strip())\n                    or divider_pattern.match(lines[i].strip())\n                ):\n                    raise ValueError(f\"Expected `{UPDATED_ERR}` or `{DIVIDER_ERR}`\")\n\n                yield filename, \"\".join(original_text), \"\".join(updated_text)\n\n            except ValueError as e:\n                processed = \"\".join(lines[: i + 1])\n                err = e.args[0]\n                raise ValueError(f\"{processed}\\n^^^ {err}\")\n\n        i += 1",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_coder.py::13",
    "metadata": {
      "file_path": "aider/coders/editblock_coder.py",
      "file_name": "editblock_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 347,
      "span_ids": [
        "find_filename"
      ],
      "start_line": 519,
      "end_line": 580,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def find_filename(lines, fence, valid_fnames):\n    \"\"\"\n    Deepseek Coder v2 has been doing this:\n\n\n     ```python\n    word_count.py\n    ```\n    ```python\n    <<<<<<< SEARCH\n    ...\n\n    This is a more flexible search back for filenames.\n    \"\"\"\n\n    if valid_fnames is None:\n        valid_fnames = []\n\n    # Go back through the 3 preceding lines\n    lines.reverse()\n    lines = lines[:3]\n\n    filenames = []\n    for line in lines:\n        # If we find a filename, done\n        filename = strip_filename(line, fence)\n        if filename:\n            filenames.append(filename)\n\n        # Only continue as long as we keep seeing fences\n        if not line.startswith(fence[0]):\n            break\n\n    if not filenames:\n        return\n\n    # pick the *best* filename found\n\n    # Check for exact match first\n    for fname in filenames:\n        if fname in valid_fnames:\n            return fname\n\n    # Check for partial match (basename match)\n    for fname in filenames:\n        for vfn in valid_fnames:\n            if fname == Path(vfn).name:\n                return vfn\n\n    # Perform fuzzy matching with valid_fnames\n    for fname in filenames:\n        close_matches = difflib.get_close_matches(fname, valid_fnames, n=1, cutoff=0.8)\n        if len(close_matches) == 1:\n            return close_matches[0]\n\n    # If no fuzzy match, look for a file w/extension\n    for fname in filenames:\n        if \".\" in fname:\n            return fname\n\n    if filenames:\n        return filenames[0]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_coder.py::14",
    "metadata": {
      "file_path": "aider/coders/editblock_coder.py",
      "file_name": "editblock_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 227,
      "span_ids": [
        "find_similar_lines"
      ],
      "start_line": 583,
      "end_line": 609,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def find_similar_lines(search_lines, content_lines, threshold=0.6):\n    search_lines = search_lines.splitlines()\n    content_lines = content_lines.splitlines()\n\n    best_ratio = 0\n    best_match = None\n\n    for i in range(len(content_lines) - len(search_lines) + 1):\n        chunk = content_lines[i : i + len(search_lines)]\n        ratio = SequenceMatcher(None, search_lines, chunk).ratio()\n        if ratio > best_ratio:\n            best_ratio = ratio\n            best_match = chunk\n            best_match_i = i\n\n    if best_ratio < threshold:\n        return \"\"\n\n    if best_match[0] == search_lines[0] and best_match[-1] == search_lines[-1]:\n        return \"\\n\".join(best_match)\n\n    N = 5\n    best_match_end = min(len(content_lines), best_match_i + len(search_lines) + N)\n    best_match_i = max(0, best_match_i - N)\n\n    best = content_lines[best_match_i:best_match_end]\n    return \"\\n\".join(best)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_coder.py::15",
    "metadata": {
      "file_path": "aider/coders/editblock_coder.py",
      "file_name": "editblock_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 147,
      "span_ids": [
        "impl:21",
        "main"
      ],
      "start_line": 612,
      "end_line": 639,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main():\n    history_md = Path(sys.argv[1]).read_text()\n    if not history_md:\n        return\n\n    messages = utils.split_chat_history_markdown(history_md)\n\n    for msg in messages:\n        msg = msg[\"content\"]\n        edits = list(find_original_update_blocks(msg))\n\n        for fname, before, after in edits:\n            # Compute diff\n            diff = difflib.unified_diff(\n                before.splitlines(keepends=True),\n                after.splitlines(keepends=True),\n                fromfile=\"before\",\n                tofile=\"after\",\n            )\n            diff = \"\".join(diff)\n            dump(before)\n            dump(after)\n            dump(diff)\n\n\nif __name__ == \"__main__\":\n    main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_fenced_coder.py::1",
    "metadata": {
      "file_path": "aider/coders/editblock_fenced_coder.py",
      "file_name": "editblock_fenced_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 86,
      "span_ids": [
        "imports",
        "EditBlockFencedCoder"
      ],
      "start_line": 1,
      "end_line": 10,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from ..dump import dump  # noqa: F401\nfrom .editblock_coder import EditBlockCoder\nfrom .editblock_fenced_prompts import EditBlockFencedPrompts\n\n\nclass EditBlockFencedCoder(EditBlockCoder):\n    \"\"\"A coder that uses fenced search/replace blocks for code modifications.\"\"\"\n    edit_format = \"diff-fenced\"\n    gpt_prompts = EditBlockFencedPrompts()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_fenced_prompts.py::1",
    "metadata": {
      "file_path": "aider/coders/editblock_fenced_prompts.py",
      "file_name": "editblock_fenced_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 472,
      "span_ids": [
        "EditBlockFencedPrompts",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 96,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# flake8: noqa: E501\n\nfrom .editblock_prompts import EditBlockPrompts\n\n\nclass EditBlockFencedPrompts(EditBlockPrompts):\n    example_messages = [\n        dict(\n            role=\"user\",\n            content=\"Change get_factorial() to use math.factorial\",\n        ),\n        dict(\n            role=\"assistant\",\n            content=\"\"\"To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n{fence[0]}\nmathweb/flask/app.py\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n{fence[1]}\n\n{fence[0]}\nmathweb/flask/app.py\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n{fence[1]}\n\n{fence[0]}\nmathweb/flask/app.py\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n{fence[1]}\n<<<<<<< HEAD\n\"\"\",\n        ),\n        dict(\n            role=\"user\",\n            content=\"Refactor hello() into its own file.\",\n        ),\n        dict(\n            role=\"assistant\",\n            content=\"\"\"To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\n{fence[0]}\nhello.py\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n{fence[1]}\n\n{fence[0]}\nmain.py\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n{fence[1]}\n\"\"\",\n        ),\n    ]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_func_coder.py::1",
    "metadata": {
      "file_path": "aider/coders/editblock_func_coder.py",
      "file_name": "editblock_func_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 560,
      "span_ids": [
        "imports",
        "EditBlockFunctionCoder.render_incremental_response",
        "EditBlockFunctionCoder.__init__",
        "EditBlockFunctionCoder"
      ],
      "start_line": 1,
      "end_line": 93,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import json\n\nfrom ..dump import dump  # noqa: F401\nfrom .base_coder import Coder\nfrom .editblock_coder import do_replace\nfrom .editblock_func_prompts import EditBlockFunctionPrompts\n\n\nclass EditBlockFunctionCoder(Coder):\n    functions = [\n        dict(\n            name=\"replace_lines\",\n            description=\"create or update one or more files\",\n            parameters=dict(\n                type=\"object\",\n                required=[\"explanation\", \"edits\"],\n                properties=dict(\n                    explanation=dict(\n                        type=\"string\",\n                        description=(\n                            \"Step by step plan for the changes to be made to the code (future\"\n                            \" tense, markdown format)\"\n                        ),\n                    ),\n                    edits=dict(\n                        type=\"array\",\n                        items=dict(\n                            type=\"object\",\n                            required=[\"path\", \"original_lines\", \"updated_lines\"],\n                            properties=dict(\n                                path=dict(\n                                    type=\"string\",\n                                    description=\"Path of file to edit\",\n                                ),\n                                original_lines=dict(\n                                    type=\"array\",\n                                    items=dict(\n                                        type=\"string\",\n                                    ),\n                                    description=(\n                                        \"A unique stretch of lines from the original file,\"\n                                        \" including all whitespace, without skipping any lines\"\n                                    ),\n                                ),\n                                updated_lines=dict(\n                                    type=\"array\",\n                                    items=dict(\n                                        type=\"string\",\n                                    ),\n                                    description=\"New content to replace the `original_lines` with\",\n                                ),\n                            ),\n                        ),\n                    ),\n                ),\n            ),\n        ),\n    ]\n\n    def __init__(self, code_format, *args, **kwargs):\n        raise RuntimeError(\"Deprecated, needs to be refactored to support get_edits/apply_edits\")\n        self.code_format = code_format\n\n        if code_format == \"string\":\n            original_lines = dict(\n                type=\"string\",\n                description=(\n                    \"A unique stretch of lines from the original file, including all\"\n                    \" whitespace and newlines, without skipping any lines\"\n                ),\n            )\n            updated_lines = dict(\n                type=\"string\",\n                description=\"New content to replace the `original_lines` with\",\n            )\n\n            self.functions[0][\"parameters\"][\"properties\"][\"edits\"][\"items\"][\"properties\"][\n                \"original_lines\"\n            ] = original_lines\n            self.functions[0][\"parameters\"][\"properties\"][\"edits\"][\"items\"][\"properties\"][\n                \"updated_lines\"\n            ] = updated_lines\n\n        self.gpt_prompts = EditBlockFunctionPrompts()\n        super().__init__(*args, **kwargs)\n\n    def render_incremental_response(self, final=False):\n        if self.partial_response_content:\n            return self.partial_response_content\n\n        args = self.parse_partial_args()\n        res = json.dumps(args, indent=4)\n        return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_func_coder.py::2",
    "metadata": {
      "file_path": "aider/coders/editblock_func_coder.py",
      "file_name": "editblock_func_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 342,
      "span_ids": [
        "get_arg",
        "EditBlockFunctionCoder._update_files"
      ],
      "start_line": 95,
      "end_line": 142,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class EditBlockFunctionCoder(Coder):\n\n    def _update_files(self):\n        name = self.partial_response_function_call.get(\"name\")\n\n        if name and name != \"replace_lines\":\n            raise ValueError(f'Unknown function_call name=\"{name}\", use name=\"replace_lines\"')\n\n        args = self.parse_partial_args()\n        if not args:\n            return\n\n        edits = args.get(\"edits\", [])\n\n        edited = set()\n        for edit in edits:\n            path = get_arg(edit, \"path\")\n            original = get_arg(edit, \"original_lines\")\n            updated = get_arg(edit, \"updated_lines\")\n\n            # gpt-3.5 returns lists even when instructed to return a string!\n            if self.code_format == \"list\" or type(original) is list:\n                original = \"\\n\".join(original)\n            if self.code_format == \"list\" or type(updated) is list:\n                updated = \"\\n\".join(updated)\n\n            if original and not original.endswith(\"\\n\"):\n                original += \"\\n\"\n            if updated and not updated.endswith(\"\\n\"):\n                updated += \"\\n\"\n\n            full_path = self.allowed_to_edit(path)\n            if not full_path:\n                continue\n            content = self.io.read_text(full_path)\n            content = do_replace(full_path, content, original, updated)\n            if content:\n                self.io.write_text(full_path, content)\n                edited.add(path)\n                continue\n            self.io.tool_error(f\"Failed to apply edit to {path}\")\n\n        return edited\n\n\ndef get_arg(edit, arg):\n    if arg not in edit:\n        raise ValueError(f\"Missing `{arg}` parameter: {edit}\")\n    return edit[arg]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_func_prompts.py::1",
    "metadata": {
      "file_path": "aider/coders/editblock_func_prompts.py",
      "file_name": "editblock_func_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 200,
      "span_ids": [
        "EditBlockFunctionPrompts",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 28,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# flake8: noqa: E501\n\nfrom .base_prompts import CoderPrompts\n\n\nclass EditBlockFunctionPrompts(CoderPrompts):\n    main_system = \"\"\"Act as an expert software developer.\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nOnce you understand the request you MUST use the `replace_lines` function to edit the files to make the needed changes.\n\"\"\"\n\n    system_reminder = \"\"\"\nONLY return code using the `replace_lines` function.\nNEVER return code outside the `replace_lines` function.\n\"\"\"\n\n    files_content_prefix = \"Here is the current content of the files:\\n\"\n    files_no_full_files = \"I am not sharing any files yet.\"\n\n    redacted_edit_message = \"No changes are needed.\"\n\n    repo_content_prefix = (\n        \"Below here are summaries of other files! Do not propose changes to these *read-only*\"\n        \" files without asking me first.\\n\"\n    )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_prompts.py::1",
    "metadata": {
      "file_path": "aider/coders/editblock_prompts.py",
      "file_name": "editblock_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 543,
      "span_ids": [
        "EditBlockPrompts",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 57,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# flake8: noqa: E501\n\nfrom .base_prompts import CoderPrompts\n\n\nclass EditBlockPrompts(CoderPrompts):\n    main_system = \"\"\"Act as an expert software developer.\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\n{lazy_prompt}\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in {language}.\n\nOnce you understand the request you MUST:\n\n1. Decide if you need to propose *SEARCH/REPLACE* edits to any files that haven't been added to the chat. You can create new files without asking!\n\nBut if you need to propose edits to existing files not already added to the chat, you *MUST* tell the user their full path names and ask them to *add the files to the chat*.\nEnd your reply and wait for their approval.\nYou can keep asking if you then decide you need to edit more files.\n\n2. Think step-by-step and explain the needed changes in a few short sentences.\n\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\n\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n{shell_cmd_prompt}\n\"\"\"\n\n    shell_cmd_prompt = \"\"\"\n4. *Concisely* suggest any shell commands the user might want to run in ```bash blocks.\n\nJust suggest shell commands this way, not example code.\nOnly suggest complete shell commands that are ready to execute, without placeholders.\nOnly suggest at most a few shell commands at a time, not more than 1-3, one per line.\nDo not suggest multi-line shell commands.\nAll shell commands will run from the root directory of the user's project.\n\nUse the appropriate shell based on the user's system info:\n{platform}\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\"\"\"\n\n    no_shell_cmd_prompt = \"\"\"\nKeep in mind these details about the user's platform and environment:\n{platform}\n\"\"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_prompts.py::2",
    "metadata": {
      "file_path": "aider/coders/editblock_prompts.py",
      "file_name": "editblock_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 450,
      "span_ids": [
        "EditBlockPrompts"
      ],
      "start_line": 58,
      "end_line": 145,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class EditBlockPrompts(CoderPrompts):\n    example_messages = [\n        dict(\n            role=\"user\",\n            content=\"Change get_factorial() to use math.factorial\",\n        ),\n        dict(\n            role=\"assistant\",\n            content=\"\"\"To make this change we need to modify `mathweb/flask/app.py` to:\n\n1. Import the math package.\n2. Remove the existing factorial() function.\n3. Update get_factorial() to call math.factorial instead.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nmathweb/flask/app.py\n{fence[0]}python\n<<<<<<< SEARCH\nfrom flask import Flask\n=======\nimport math\nfrom flask import Flask\n>>>>>>> REPLACE\n{fence[1]}\n\nmathweb/flask/app.py\n{fence[0]}python\n<<<<<<< SEARCH\ndef factorial(n):\n    \"compute factorial\"\n\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n\n=======\n>>>>>>> REPLACE\n{fence[1]}\n\nmathweb/flask/app.py\n{fence[0]}python\n<<<<<<< SEARCH\n    return str(factorial(n))\n=======\n    return str(math.factorial(n))\n>>>>>>> REPLACE\n{fence[1]}\n\"\"\",\n        ),\n        dict(\n            role=\"user\",\n            content=\"Refactor hello() into its own file.\",\n        ),\n        dict(\n            role=\"assistant\",\n            content=\"\"\"To make this change we need to modify `main.py` and make a new file `hello.py`:\n\n1. Make a new hello.py file with hello() in it.\n2. Remove hello() from main.py and replace it with an import.\n\nHere are the *SEARCH/REPLACE* blocks:\n\nhello.py\n{fence[0]}python\n<<<<<<< SEARCH\n=======\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n>>>>>>> REPLACE\n{fence[1]}\n\nmain.py\n{fence[0]}python\n<<<<<<< SEARCH\ndef hello():\n    \"print a greeting\"\n\n    print(\"hello\")\n=======\nfrom hello import hello\n>>>>>>> REPLACE\n{fence[1]}\n\"\"\",\n        ),\n    ]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_prompts.py::3",
    "metadata": {
      "file_path": "aider/coders/editblock_prompts.py",
      "file_name": "editblock_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 624,
      "span_ids": [
        "EditBlockPrompts"
      ],
      "start_line": 147,
      "end_line": 192,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class EditBlockPrompts(CoderPrompts):\n\n    system_reminder = \"\"\"# *SEARCH/REPLACE block* Rules:\n\nEvery *SEARCH/REPLACE block* must use this format:\n1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.\n2. The opening fence and code language, eg: {fence[0]}python\n3. The start of search block: <<<<<<< SEARCH\n4. A contiguous chunk of lines to search for in the existing source code\n5. The dividing line: =======\n6. The lines to replace into the source code\n7. The end of the replace block: >>>>>>> REPLACE\n8. The closing fence: {fence[1]}\n\nUse the *FULL* file path, as shown to you by the user.\n\nEvery *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.\nIf the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.\n\n*SEARCH/REPLACE* blocks will *only* replace the first match occurrence.\nIncluding multiple unique *SEARCH/REPLACE* blocks if needed.\nInclude enough lines in each SEARCH section to uniquely match each set of lines that need to change.\n\nKeep *SEARCH/REPLACE* blocks concise.\nBreak large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.\nInclude just the changing lines, and a few surrounding lines if needed for uniqueness.\nDo not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.\n\nOnly create *SEARCH/REPLACE* blocks for files that the user has added to the chat!\n\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nPay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.\n\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\n- A new file path, including dir name if needed\n- An empty `SEARCH` section\n- The new file's contents in the `REPLACE` section\n\nTo rename files which have been added to the chat, use shell commands at the end of your response.\n\nIf the user just says something like \"ok\" or \"go ahead\" or \"do that\" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.\nThe user will say when they've applied your edits. If they haven't explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.\n\n{lazy_prompt}\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n{shell_cmd_reminder}\n\"\"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editblock_prompts.py::4",
    "metadata": {
      "file_path": "aider/coders/editblock_prompts.py",
      "file_name": "editblock_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 138,
      "span_ids": [
        "EditBlockPrompts"
      ],
      "start_line": 194,
      "end_line": 204,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class EditBlockPrompts(CoderPrompts):\n\n    shell_cmd_reminder = \"\"\"\nExamples of when to suggest shell commands:\n\n- If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.\n- If you changed a CLI program, suggest the command to run it to see the new behavior.\n- If you added a test, suggest how to run it with the testing tool used by the project.\n- Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.\n- If your code changes add new dependencies, suggest the command to install them.\n- Etc.\n\"\"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editor_editblock_coder.py::1",
    "metadata": {
      "file_path": "aider/coders/editor_editblock_coder.py",
      "file_name": "editor_editblock_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 72,
      "span_ids": [
        "EditorEditBlockCoder",
        "imports"
      ],
      "start_line": 1,
      "end_line": 9,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from .editblock_coder import EditBlockCoder\nfrom .editor_editblock_prompts import EditorEditBlockPrompts\n\n\nclass EditorEditBlockCoder(EditBlockCoder):\n    \"A coder that uses search/replace blocks, focused purely on editing files.\"\n    edit_format = \"editor-diff\"\n    gpt_prompts = EditorEditBlockPrompts()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editor_editblock_prompts.py::1",
    "metadata": {
      "file_path": "aider/coders/editor_editblock_prompts.py",
      "file_name": "editor_editblock_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 121,
      "span_ids": [
        "docstring",
        "EditorEditBlockPrompts"
      ],
      "start_line": 1,
      "end_line": 17,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# flake8: noqa: E501\n\nfrom .editblock_prompts import EditBlockPrompts\n\n\nclass EditorEditBlockPrompts(EditBlockPrompts):\n    main_system = \"\"\"Act as an expert software developer who edits source code.\n{lazy_prompt}\nDescribe each change with a *SEARCH/REPLACE block* per the examples below.\nAll changes to files must use this *SEARCH/REPLACE block* format.\nONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!\n\"\"\"\n\n    shell_cmd_prompt = \"\"\n    no_shell_cmd_prompt = \"\"\n    shell_cmd_reminder = \"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editor_whole_coder.py::1",
    "metadata": {
      "file_path": "aider/coders/editor_whole_coder.py",
      "file_name": "editor_whole_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 72,
      "span_ids": [
        "imports",
        "EditorWholeFileCoder"
      ],
      "start_line": 1,
      "end_line": 9,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from .editor_whole_prompts import EditorWholeFilePrompts\nfrom .wholefile_coder import WholeFileCoder\n\n\nclass EditorWholeFileCoder(WholeFileCoder):\n    \"A coder that operates on entire files, focused purely on editing files.\"\n    edit_format = \"editor-whole\"\n    gpt_prompts = EditorWholeFilePrompts()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/editor_whole_prompts.py::1",
    "metadata": {
      "file_path": "aider/coders/editor_whole_prompts.py",
      "file_name": "editor_whole_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 67,
      "span_ids": [
        "EditorWholeFilePrompts",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 11,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# flake8: noqa: E501\n\nfrom .wholefile_prompts import WholeFilePrompts\n\n\nclass EditorWholeFilePrompts(WholeFilePrompts):\n    main_system = \"\"\"Act as an expert software developer and make changes to source code.\n{lazy_prompt}\nOutput a copy of each file that needs changes.\n\"\"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/help_coder.py::1",
    "metadata": {
      "file_path": "aider/coders/help_coder.py",
      "file_name": "help_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 88,
      "span_ids": [
        "HelpCoder.get_edits",
        "imports",
        "HelpCoder.apply_edits",
        "HelpCoder"
      ],
      "start_line": 1,
      "end_line": 16,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from ..dump import dump  # noqa: F401\nfrom .base_coder import Coder\nfrom .help_prompts import HelpPrompts\n\n\nclass HelpCoder(Coder):\n    \"\"\"Interactive help and documentation about aider.\"\"\"\n    edit_format = \"help\"\n    gpt_prompts = HelpPrompts()\n\n    def get_edits(self, mode=\"update\"):\n        return []\n\n    def apply_edits(self, edits):\n        pass",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/help_prompts.py::1",
    "metadata": {
      "file_path": "aider/coders/help_prompts.py",
      "file_name": "help_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 347,
      "span_ids": [
        "HelpPrompts",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 47,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# flake8: noqa: E501\n\nfrom .base_prompts import CoderPrompts\n\n\nclass HelpPrompts(CoderPrompts):\n    main_system = \"\"\"You are an expert on the AI coding tool called Aider.\nAnswer the user's questions about how to use aider.\n\nThe user is currently chatting with you using aider, to write and edit code.\n\nUse the provided aider documentation *if it is relevant to the user's question*.\n\nInclude a bulleted list of urls to the aider docs that might be relevant for the user to read.\nInclude *bare* urls. *Do not* make [markdown links](http://...).\nFor example:\n- https://aider.chat/docs/usage.html\n- https://aider.chat/docs/faq.html\n\nIf you don't know the answer, say so and suggest some relevant aider doc urls.\n\nIf asks for something that isn't possible with aider, be clear about that.\nDon't suggest a solution that isn't supported.\n\nBe helpful but concise.\n\nUnless the question indicates otherwise, assume the user wants to use aider as a CLI tool.\n\nKeep this info about the user's system in mind:\n{platform}\n\"\"\"\n\n    example_messages = []\n    system_reminder = \"\"\n\n    files_content_prefix = \"\"\"These are some files we have been discussing that we may want to edit after you answer my questions:\n\"\"\"\n\n    files_no_full_files = \"I am not sharing any files with you.\"\n\n    files_no_full_files_with_repo_map = \"\"\n    files_no_full_files_with_repo_map_reply = \"\"\n\n    repo_content_prefix = \"\"\"Here are summaries of some files present in my git repository.\nWe may look at these in more detail after you answer my questions.\n\"\"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/search_replace.py::1",
    "metadata": {
      "file_path": "aider/coders/search_replace.py",
      "file_name": "search_replace.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 605,
      "span_ids": [
        "RelativeIndenter",
        "RelativeIndenter.__init__",
        "RelativeIndenter.select_unique_marker",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 104,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "#!/usr/bin/env python\n\nimport sys\nfrom pathlib import Path\n\ntry:\n    import git\nexcept ImportError:\n    git = None\n\nfrom diff_match_patch import diff_match_patch\nfrom tqdm import tqdm\n\nfrom aider.dump import dump\nfrom aider.utils import GitTemporaryDirectory\n\n\nclass RelativeIndenter:\n    \"\"\"Rewrites text files to have relative indentation, which involves\n    reformatting the leading white space on lines.  This format makes\n    it easier to search and apply edits to pairs of code blocks which\n    may differ significantly in their overall level of indentation.\n\n    It removes leading white space which is shared with the preceding\n    line.\n\n    Original:\n    ```\n            Foo # indented 8\n                Bar # indented 4 more than the previous line\n                Baz # same indent as the previous line\n                Fob # same indent as the previous line\n    ```\n\n    Becomes:\n    ```\n            Foo # indented 8\n        Bar # indented 4 more than the previous line\n    Baz # same indent as the previous line\n    Fob # same indent as the previous line\n    ```\n\n    If the current line is *less* indented then the previous line,\n    uses a unicode character to indicate outdenting.\n\n    Original\n    ```\n            Foo\n                Bar\n                Baz\n            Fob # indented 4 less than the previous line\n    ```\n\n    Becomes:\n    ```\n            Foo\n        Bar\n    Baz\n    \u2190\u2190\u2190\u2190Fob # indented 4 less than the previous line\n    ```\n\n    This is a similar original to the last one, but every line has\n    been uniformly outdented:\n    ```\n    Foo\n        Bar\n        Baz\n    Fob # indented 4 less than the previous line\n    ```\n\n    It becomes this result, which is very similar to the previous\n    result.  Only the white space on the first line differs.  From the\n    word Foo onwards, it is identical to the previous result.\n    ```\n    Foo\n        Bar\n    Baz\n    \u2190\u2190\u2190\u2190Fob # indented 4 less than the previous line\n    ```\n\n    \"\"\"\n\n    def __init__(self, texts):\n        \"\"\"\n        Based on the texts, choose a unicode character that isn't in any of them.\n        \"\"\"\n\n        chars = set()\n        for text in texts:\n            chars.update(text)\n\n        ARROW = \"\u2190\"\n        if ARROW not in chars:\n            self.marker = ARROW\n        else:\n            self.marker = self.select_unique_marker(chars)\n\n    def select_unique_marker(self, chars):\n        for codepoint in range(0x10FFFF, 0x10000, -1):\n            marker = chr(codepoint)\n            if marker not in chars:\n                return marker\n\n        raise ValueError(\"Could not find a unique marker\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/search_replace.py::2",
    "metadata": {
      "file_path": "aider/coders/search_replace.py",
      "file_name": "search_replace.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 215,
      "span_ids": [
        "RelativeIndenter.make_relative"
      ],
      "start_line": 106,
      "end_line": 138,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class RelativeIndenter:\n\n    def make_relative(self, text):\n        \"\"\"\n        Transform text to use relative indents.\n        \"\"\"\n\n        if self.marker in text:\n            raise ValueError(\"Text already contains the outdent marker: {self.marker}\")\n\n        lines = text.splitlines(keepends=True)\n\n        output = []\n        prev_indent = \"\"\n        for line in lines:\n            line_without_end = line.rstrip(\"\\n\\r\")\n\n            len_indent = len(line_without_end) - len(line_without_end.lstrip())\n            indent = line[:len_indent]\n            change = len_indent - len(prev_indent)\n            if change > 0:\n                cur_indent = indent[-change:]\n            elif change < 0:\n                cur_indent = self.marker * -change\n            else:\n                cur_indent = \"\"\n\n            out_line = cur_indent + \"\\n\" + line[len_indent:]\n            # dump(len_indent, change, out_line)\n            # print(out_line)\n            output.append(out_line)\n            prev_indent = indent\n\n        res = \"\".join(output)\n        return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/search_replace.py::3",
    "metadata": {
      "file_path": "aider/coders/search_replace.py",
      "file_name": "search_replace.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 208,
      "span_ids": [
        "RelativeIndenter.make_absolute"
      ],
      "start_line": 140,
      "end_line": 171,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class RelativeIndenter:\n\n    def make_absolute(self, text):\n        \"\"\"\n        Transform text from relative back to absolute indents.\n        \"\"\"\n        lines = text.splitlines(keepends=True)\n\n        output = []\n        prev_indent = \"\"\n        for i in range(0, len(lines), 2):\n            dent = lines[i].rstrip(\"\\r\\n\")\n            non_indent = lines[i + 1]\n\n            if dent.startswith(self.marker):\n                len_outdent = len(dent)\n                cur_indent = prev_indent[:-len_outdent]\n            else:\n                cur_indent = prev_indent + dent\n\n            if not non_indent.rstrip(\"\\r\\n\"):\n                out_line = non_indent  # don't indent a blank line\n            else:\n                out_line = cur_indent + non_indent\n\n            output.append(out_line)\n            prev_indent = cur_indent\n\n        res = \"\".join(output)\n        if self.marker in res:\n            # dump(res)\n            raise ValueError(\"Error transforming text back to absolute indents\")\n\n        return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/search_replace.py::4",
    "metadata": {
      "file_path": "aider/coders/search_replace.py",
      "file_name": "search_replace.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 415,
      "span_ids": [
        "RelativeIndenter.make_absolute",
        "map_patches"
      ],
      "start_line": 174,
      "end_line": 226,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# The patches are created to change S->R.\n# So all the patch offsets are relative to S.\n# But O has a lot more content. So all the offsets are very wrong.\n#\n# But patch_apply() seems to imply that once patch N is located,\n# then it adjusts the offset of the next patch.\n#\n# This is great, because once we sync up after a big gap the nearby\n# patches are close to being located right.\n# Except when indentation has been changed by GPT.\n#\n# It would help to use the diff trick to build map_S_offset_to_O_offset().\n# Then update all the S offsets in the S->R patches to be O offsets.\n# Do we also need to update the R offsets?\n#\n# What if this gets funky/wrong?\n#\n\n\ndef map_patches(texts, patches, debug):\n    search_text, replace_text, original_text = texts\n\n    dmp = diff_match_patch()\n    dmp.Diff_Timeout = 5\n\n    diff_s_o = dmp.diff_main(search_text, original_text)\n    # diff_r_s = dmp.diff_main(replace_text, search_text)\n\n    # dmp.diff_cleanupSemantic(diff_s_o)\n    # dmp.diff_cleanupEfficiency(diff_s_o)\n\n    if debug:\n        html = dmp.diff_prettyHtml(diff_s_o)\n        Path(\"tmp.html\").write_text(html)\n\n        dump(len(search_text))\n        dump(len(original_text))\n\n    for patch in patches:\n        start1 = patch.start1\n        start2 = patch.start2\n\n        patch.start1 = dmp.diff_xIndex(diff_s_o, start1)\n        patch.start2 = dmp.diff_xIndex(diff_s_o, start2)\n\n        if debug:\n            print()\n            print(start1, repr(search_text[start1 : start1 + 50]))\n            print(patch.start1, repr(original_text[patch.start1 : patch.start1 + 50]))\n            print(patch.diffs)\n            print()\n\n    return patches",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/search_replace.py::5",
    "metadata": {
      "file_path": "aider/coders/search_replace.py",
      "file_name": "search_replace.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 172,
      "span_ids": [
        "impl:5",
        "line_unpad",
        "relative_indent",
        "line_pad",
        "impl:7"
      ],
      "start_line": 229,
      "end_line": 271,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "example = \"\"\"Left\nLeft\n    4 in\n    4 in\n        8 in\n    4 in\nLeft\n\"\"\"\n\n\"\"\"\nri = RelativeIndenter([example])\ndump(example)\n\nrel_example = ri.make_relative(example)\ndump(repr(rel_example))\n\nabs_example = ri.make_absolute(rel_example)\ndump(abs_example)\n\n\nsys.exit()\n\"\"\"\n\n\ndef relative_indent(texts):\n    ri = RelativeIndenter(texts)\n    texts = list(map(ri.make_relative, texts))\n\n    return ri, texts\n\n\nline_padding = 100\n\n\ndef line_pad(text):\n    padding = \"\\n\" * line_padding\n    return padding + text + padding\n\n\ndef line_unpad(text):\n    if set(text[:line_padding] + text[-line_padding:]) != set(\"\\n\"):\n        return\n    return text[line_padding:-line_padding]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/search_replace.py::6",
    "metadata": {
      "file_path": "aider/coders/search_replace.py",
      "file_name": "search_replace.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 466,
      "span_ids": [
        "lines_to_chars",
        "dmp_apply"
      ],
      "start_line": 274,
      "end_line": 349,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def dmp_apply(texts, remap=True):\n    debug = False\n    # debug = True\n\n    search_text, replace_text, original_text = texts\n\n    dmp = diff_match_patch()\n    dmp.Diff_Timeout = 5\n    # dmp.Diff_EditCost = 16\n\n    if remap:\n        dmp.Match_Threshold = 0.95\n        dmp.Match_Distance = 500\n        dmp.Match_MaxBits = 128\n        dmp.Patch_Margin = 32\n    else:\n        dmp.Match_Threshold = 0.5\n        dmp.Match_Distance = 100_000\n        dmp.Match_MaxBits = 32\n        dmp.Patch_Margin = 8\n\n    diff = dmp.diff_main(search_text, replace_text, None)\n    dmp.diff_cleanupSemantic(diff)\n    dmp.diff_cleanupEfficiency(diff)\n\n    patches = dmp.patch_make(search_text, diff)\n\n    if debug:\n        html = dmp.diff_prettyHtml(diff)\n        Path(\"tmp.search_replace_diff.html\").write_text(html)\n\n        for d in diff:\n            print(d[0], repr(d[1]))\n\n        for patch in patches:\n            start1 = patch.start1\n            print()\n            print(start1, repr(search_text[start1 : start1 + 10]))\n            print(start1, repr(replace_text[start1 : start1 + 10]))\n            print(patch.diffs)\n\n        # dump(original_text)\n        # dump(search_text)\n\n    if remap:\n        patches = map_patches(texts, patches, debug)\n\n    patches_text = dmp.patch_toText(patches)\n\n    new_text, success = dmp.patch_apply(patches, original_text)\n\n    all_success = False not in success\n\n    if debug:\n        # dump(new_text)\n        print(patches_text)\n\n        # print(new_text)\n        dump(success)\n        dump(all_success)\n\n        # print(new_text)\n\n    if not all_success:\n        return\n\n    return new_text\n\n\ndef lines_to_chars(lines, mapping):\n    new_text = []\n    for char in lines:\n        new_text.append(mapping[ord(char)])\n\n    new_text = \"\".join(new_text)\n    return new_text",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/search_replace.py::7",
    "metadata": {
      "file_path": "aider/coders/search_replace.py",
      "file_name": "search_replace.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 461,
      "span_ids": [
        "dmp_lines_apply"
      ],
      "start_line": 352,
      "end_line": 417,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def dmp_lines_apply(texts, remap=True):\n    debug = False\n    # debug = True\n\n    for t in texts:\n        assert t.endswith(\"\\n\"), t\n\n    search_text, replace_text, original_text = texts\n\n    dmp = diff_match_patch()\n    dmp.Diff_Timeout = 5\n    # dmp.Diff_EditCost = 16\n\n    dmp.Match_Threshold = 0.1\n    dmp.Match_Distance = 100_000\n    dmp.Match_MaxBits = 32\n    dmp.Patch_Margin = 1\n\n    all_text = search_text + replace_text + original_text\n    all_lines, _, mapping = dmp.diff_linesToChars(all_text, \"\")\n    assert len(all_lines) == len(all_text.splitlines())\n\n    search_num = len(search_text.splitlines())\n    replace_num = len(replace_text.splitlines())\n    original_num = len(original_text.splitlines())\n\n    search_lines = all_lines[:search_num]\n    replace_lines = all_lines[search_num : search_num + replace_num]\n    original_lines = all_lines[search_num + replace_num :]\n\n    assert len(search_lines) == search_num\n    assert len(replace_lines) == replace_num\n    assert len(original_lines) == original_num\n\n    diff_lines = dmp.diff_main(search_lines, replace_lines, None)\n    dmp.diff_cleanupSemantic(diff_lines)\n    dmp.diff_cleanupEfficiency(diff_lines)\n\n    patches = dmp.patch_make(search_lines, diff_lines)\n\n    if debug:\n        diff = list(diff_lines)\n        dmp.diff_charsToLines(diff, mapping)\n        # dump(diff)\n        html = dmp.diff_prettyHtml(diff)\n        Path(\"tmp.search_replace_diff.html\").write_text(html)\n\n        for d in diff:\n            print(d[0], repr(d[1]))\n\n    new_lines, success = dmp.patch_apply(patches, original_lines)\n    new_text = lines_to_chars(new_lines, mapping)\n\n    all_success = False not in success\n\n    if debug:\n        # print(new_text)\n        dump(success)\n        dump(all_success)\n\n        # print(new_text)\n\n    if not all_success:\n        return\n\n    return new_text",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/search_replace.py::8",
    "metadata": {
      "file_path": "aider/coders/search_replace.py",
      "file_name": "search_replace.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 259,
      "span_ids": [
        "search_and_replace",
        "diff_lines"
      ],
      "start_line": 420,
      "end_line": 459,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def diff_lines(search_text, replace_text):\n    dmp = diff_match_patch()\n    dmp.Diff_Timeout = 5\n    # dmp.Diff_EditCost = 16\n    search_lines, replace_lines, mapping = dmp.diff_linesToChars(search_text, replace_text)\n\n    diff_lines = dmp.diff_main(search_lines, replace_lines, None)\n    dmp.diff_cleanupSemantic(diff_lines)\n    dmp.diff_cleanupEfficiency(diff_lines)\n\n    diff = list(diff_lines)\n    dmp.diff_charsToLines(diff, mapping)\n    # dump(diff)\n\n    udiff = []\n    for d, lines in diff:\n        if d < 0:\n            d = \"-\"\n        elif d > 0:\n            d = \"+\"\n        else:\n            d = \" \"\n        for line in lines.splitlines(keepends=True):\n            udiff.append(d + line)\n\n    return udiff\n\n\ndef search_and_replace(texts):\n    search_text, replace_text, original_text = texts\n\n    num = original_text.count(search_text)\n    # if num > 1:\n    #    raise SearchTextNotUnique()\n    if num == 0:\n        return\n\n    new_text = original_text.replace(search_text, replace_text)\n\n    return new_text",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/search_replace.py::9",
    "metadata": {
      "file_path": "aider/coders/search_replace.py",
      "file_name": "search_replace.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 232,
      "span_ids": [
        "git_cherry_pick_osr_onto_o"
      ],
      "start_line": 462,
      "end_line": 496,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def git_cherry_pick_osr_onto_o(texts):\n    search_text, replace_text, original_text = texts\n\n    with GitTemporaryDirectory() as dname:\n        repo = git.Repo(dname)\n\n        fname = Path(dname) / \"file.txt\"\n\n        # Make O->S->R\n        fname.write_text(original_text)\n        repo.git.add(str(fname))\n        repo.git.commit(\"-m\", \"original\")\n        original_hash = repo.head.commit.hexsha\n\n        fname.write_text(search_text)\n        repo.git.add(str(fname))\n        repo.git.commit(\"-m\", \"search\")\n\n        fname.write_text(replace_text)\n        repo.git.add(str(fname))\n        repo.git.commit(\"-m\", \"replace\")\n        replace_hash = repo.head.commit.hexsha\n\n        # go back to O\n        repo.git.checkout(original_hash)\n\n        # cherry pick R onto original\n        try:\n            repo.git.cherry_pick(replace_hash, \"--minimal\")\n        except (git.exc.ODBError, git.exc.GitError):\n            # merge conflicts!\n            return\n\n        new_text = fname.read_text()\n        return new_text",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/search_replace.py::10",
    "metadata": {
      "file_path": "aider/coders/search_replace.py",
      "file_name": "search_replace.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 236,
      "span_ids": [
        "git_cherry_pick_sr_onto_so"
      ],
      "start_line": 499,
      "end_line": 535,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def git_cherry_pick_sr_onto_so(texts):\n    search_text, replace_text, original_text = texts\n\n    with GitTemporaryDirectory() as dname:\n        repo = git.Repo(dname)\n\n        fname = Path(dname) / \"file.txt\"\n\n        fname.write_text(search_text)\n        repo.git.add(str(fname))\n        repo.git.commit(\"-m\", \"search\")\n        search_hash = repo.head.commit.hexsha\n\n        # make search->replace\n        fname.write_text(replace_text)\n        repo.git.add(str(fname))\n        repo.git.commit(\"-m\", \"replace\")\n        replace_hash = repo.head.commit.hexsha\n\n        # go back to search,\n        repo.git.checkout(search_hash)\n\n        # make search->original\n        fname.write_text(original_text)\n        repo.git.add(str(fname))\n        repo.git.commit(\"-m\", \"original\")\n\n        # cherry pick replace onto original\n        try:\n            repo.git.cherry_pick(replace_hash, \"--minimal\")\n        except (git.exc.ODBError, git.exc.GitError):\n            # merge conflicts!\n            return\n\n        new_text = fname.read_text()\n\n        return new_text",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/search_replace.py::11",
    "metadata": {
      "file_path": "aider/coders/search_replace.py",
      "file_name": "search_replace.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 374,
      "span_ids": [
        "reverse_lines",
        "impl:9",
        "SearchTextNotUnique",
        "flexible_search_and_replace"
      ],
      "start_line": 538,
      "end_line": 597,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class SearchTextNotUnique(ValueError):\n    pass\n\n\nall_preprocs = [\n    # (strip_blank_lines, relative_indent, reverse_lines)\n    (False, False, False),\n    (True, False, False),\n    (False, True, False),\n    (True, True, False),\n    # (False, False, True),\n    # (True, False, True),\n    # (False, True, True),\n    # (True, True, True),\n]\n\nalways_relative_indent = [\n    (False, True, False),\n    (True, True, False),\n    # (False, True, True),\n    # (True, True, True),\n]\n\neditblock_strategies = [\n    (search_and_replace, all_preprocs),\n    (git_cherry_pick_osr_onto_o, all_preprocs),\n    (dmp_lines_apply, all_preprocs),\n]\n\nnever_relative = [\n    (False, False),\n    (True, False),\n]\n\nudiff_strategies = [\n    (search_and_replace, all_preprocs),\n    (git_cherry_pick_osr_onto_o, all_preprocs),\n    (dmp_lines_apply, all_preprocs),\n]\n\n\ndef flexible_search_and_replace(texts, strategies):\n    \"\"\"Try a series of search/replace methods, starting from the most\n    literal interpretation of search_text. If needed, progress to more\n    flexible methods, which can accommodate divergence between\n    search_text and original_text and yet still achieve the desired\n    edits.\n    \"\"\"\n\n    for strategy, preprocs in strategies:\n        for preproc in preprocs:\n            res = try_strategy(texts, strategy, preproc)\n            if res:\n                return res\n\n\ndef reverse_lines(text):\n    lines = text.splitlines(keepends=True)\n    lines.reverse()\n    return \"\".join(lines)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/search_replace.py::12",
    "metadata": {
      "file_path": "aider/coders/search_replace.py",
      "file_name": "search_replace.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 193,
      "span_ids": [
        "try_strategy",
        "strip_blank_lines",
        "read_text"
      ],
      "start_line": 600,
      "end_line": 633,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def try_strategy(texts, strategy, preproc):\n    preproc_strip_blank_lines, preproc_relative_indent, preproc_reverse = preproc\n    ri = None\n\n    if preproc_strip_blank_lines:\n        texts = strip_blank_lines(texts)\n    if preproc_relative_indent:\n        ri, texts = relative_indent(texts)\n    if preproc_reverse:\n        texts = list(map(reverse_lines, texts))\n\n    res = strategy(texts)\n\n    if res and preproc_reverse:\n        res = reverse_lines(res)\n\n    if res and preproc_relative_indent:\n        try:\n            res = ri.make_absolute(res)\n        except ValueError:\n            return\n\n    return res\n\n\ndef strip_blank_lines(texts):\n    # strip leading and trailing blank lines\n    texts = [text.strip(\"\\n\") + \"\\n\" for text in texts]\n    return texts\n\n\ndef read_text(fname):\n    text = Path(fname).read_text()\n    return text",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/search_replace.py::13",
    "metadata": {
      "file_path": "aider/coders/search_replace.py",
      "file_name": "search_replace.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 439,
      "span_ids": [
        "proc"
      ],
      "start_line": 636,
      "end_line": 706,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def proc(dname):\n    dname = Path(dname)\n\n    try:\n        search_text = read_text(dname / \"search\")\n        replace_text = read_text(dname / \"replace\")\n        original_text = read_text(dname / \"original\")\n    except FileNotFoundError:\n        return\n\n    ####\n\n    texts = search_text, replace_text, original_text\n\n    strategies = [\n        # (search_and_replace, all_preprocs),\n        # (git_cherry_pick_osr_onto_o, all_preprocs),\n        # (git_cherry_pick_sr_onto_so, all_preprocs),\n        # (dmp_apply, all_preprocs),\n        (dmp_lines_apply, all_preprocs),\n    ]\n\n    _strategies = editblock_strategies  # noqa: F841\n\n    short_names = dict(\n        search_and_replace=\"sr\",\n        git_cherry_pick_osr_onto_o=\"cp_o\",\n        git_cherry_pick_sr_onto_so=\"cp_so\",\n        dmp_apply=\"dmp\",\n        dmp_lines_apply=\"dmpl\",\n    )\n\n    patched = dict()\n    for strategy, preprocs in strategies:\n        for preproc in preprocs:\n            method = strategy.__name__\n            method = short_names[method]\n\n            strip_blank, rel_indent, rev_lines = preproc\n            if strip_blank or rel_indent:\n                method += \"_\"\n            if strip_blank:\n                method += \"s\"\n            if rel_indent:\n                method += \"i\"\n            if rev_lines:\n                method += \"r\"\n\n            res = try_strategy(texts, strategy, preproc)\n            patched[method] = res\n\n    results = []\n    for method, res in patched.items():\n        out_fname = dname / f\"original.{method}\"\n        if out_fname.exists():\n            out_fname.unlink()\n\n        if res:\n            out_fname.write_text(res)\n\n            correct = (dname / \"correct\").read_text()\n            if res == correct:\n                res = \"pass\"\n            else:\n                res = \"WRONG\"\n        else:\n            res = \"fail\"\n\n        results.append((method, res))\n\n    return results",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/search_replace.py::14",
    "metadata": {
      "file_path": "aider/coders/search_replace.py",
      "file_name": "search_replace.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 110,
      "span_ids": [
        "colorize_result"
      ],
      "start_line": 709,
      "end_line": 715,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def colorize_result(result):\n    colors = {\n        \"pass\": \"\\033[102;30mpass\\033[0m\",  # Green background, black text\n        \"WRONG\": \"\\033[101;30mWRONG\\033[0m\",  # Red background, black text\n        \"fail\": \"\\033[103;30mfail\\033[0m\",  # Yellow background, black text\n    }\n    return colors.get(result, result)  # Default to original result if not found\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/search_replace.py::15",
    "metadata": {
      "file_path": "aider/coders/search_replace.py",
      "file_name": "search_replace.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 437,
      "span_ids": [
        "main",
        "impl:19"
      ],
      "start_line": 718,
      "end_line": 774,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main(dnames):\n    all_results = []\n    for dname in tqdm(dnames):\n        dname = Path(dname)\n        results = proc(dname)\n        for method, res in results:\n            all_results.append((dname, method, res))\n            # print(dname, method, colorize_result(res))\n\n    # Create a 2D table with directories along the right and methods along the top\n    # Collect all unique methods and directories\n    methods = []\n    for _, method, _ in all_results:\n        if method not in methods:\n            methods.append(method)\n\n    directories = dnames\n\n    # Sort directories by decreasing number of 'pass' results\n    pass_counts = {\n        dname: sum(\n            res == \"pass\" for dname_result, _, res in all_results if str(dname) == str(dname_result)\n        )\n        for dname in directories\n    }\n    directories.sort(key=lambda dname: pass_counts[dname], reverse=True)\n\n    # Create a results matrix\n    results_matrix = {dname: {method: \"\" for method in methods} for dname in directories}\n\n    # Populate the results matrix\n    for dname, method, res in all_results:\n        results_matrix[str(dname)][method] = res\n\n    # Print the 2D table\n    # Print the header\n    print(\"{:<20}\".format(\"Directory\"), end=\"\")\n    for method in methods:\n        print(\"{:<9}\".format(method), end=\"\")\n    print()\n\n    # Print the rows with colorized results\n    for dname in directories:\n        print(\"{:<20}\".format(Path(dname).name), end=\"\")\n        for method in methods:\n            res = results_matrix[dname][method]\n            colorized_res = colorize_result(res)\n            res_l = 9 + len(colorized_res) - len(res)\n            fmt = \"{:<\" + str(res_l) + \"}\"\n            print(fmt.format(colorized_res), end=\"\")\n        print()\n\n\nif __name__ == \"__main__\":\n    status = main(sys.argv[1:])\n    sys.exit(status)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/single_wholefile_func_coder.py::1",
    "metadata": {
      "file_path": "aider/coders/single_wholefile_func_coder.py",
      "file_name": "single_wholefile_func_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 360,
      "span_ids": [
        "SingleWholeFileFunctionCoder.__init__",
        "SingleWholeFileFunctionCoder.update_cur_messages",
        "SingleWholeFileFunctionCoder",
        "SingleWholeFileFunctionCoder.render_incremental_response",
        "imports"
      ],
      "start_line": 1,
      "end_line": 64,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from aider import diffs\n\nfrom ..dump import dump  # noqa: F401\nfrom .base_coder import Coder\nfrom .single_wholefile_func_prompts import SingleWholeFileFunctionPrompts\n\n\nclass SingleWholeFileFunctionCoder(Coder):\n    edit_format = \"func\"\n\n    functions = [\n        dict(\n            name=\"write_file\",\n            description=\"write new content into the file\",\n            # strict=True,\n            parameters=dict(\n                type=\"object\",\n                properties=dict(\n                    explanation=dict(\n                        type=\"string\",\n                        description=(\n                            \"Step by step plan for the changes to be made to the code (future\"\n                            \" tense, markdown format)\"\n                        ),\n                    ),\n                    content=dict(\n                        type=\"string\",\n                        description=\"Content to write to the file\",\n                    ),\n                ),\n                required=[\"explanation\", \"content\"],\n                additionalProperties=False,\n            ),\n        ),\n    ]\n\n    def __init__(self, *args, **kwargs):\n        self.gpt_prompts = SingleWholeFileFunctionPrompts()\n        super().__init__(*args, **kwargs)\n\n    def update_cur_messages(self, edited):\n        if edited:\n            self.cur_messages += [\n                dict(role=\"assistant\", content=self.gpt_prompts.redacted_edit_message)\n            ]\n        else:\n            self.cur_messages += [dict(role=\"assistant\", content=self.partial_response_content)]\n\n    def render_incremental_response(self, final=False):\n        res = \"\"\n        if self.partial_response_content:\n            res += self.partial_response_content\n\n        args = self.parse_partial_args()\n\n        if not args:\n            return \"\"\n\n        for k, v in args.items():\n            res += \"\\n\"\n            res += f\"{k}:\\n\"\n            res += v\n\n        return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/single_wholefile_func_coder.py::2",
    "metadata": {
      "file_path": "aider/coders/single_wholefile_func_coder.py",
      "file_name": "single_wholefile_func_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 225,
      "span_ids": [
        "SingleWholeFileFunctionCoder.live_diffs",
        "SingleWholeFileFunctionCoder.apply_edits",
        "SingleWholeFileFunctionCoder.get_edits"
      ],
      "start_line": 66,
      "end_line": 103,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class SingleWholeFileFunctionCoder(Coder):\n\n    def live_diffs(self, fname, content, final):\n        lines = content.splitlines(keepends=True)\n\n        # ending an existing block\n        full_path = self.abs_root_path(fname)\n\n        content = self.io.read_text(full_path)\n        if content is None:\n            orig_lines = []\n        else:\n            orig_lines = content.splitlines()\n\n        show_diff = diffs.diff_partial_update(\n            orig_lines,\n            lines,\n            final,\n            fname=fname,\n        ).splitlines()\n\n        return \"\\n\".join(show_diff)\n\n    def get_edits(self):\n        chat_files = self.get_inchat_relative_files()\n        assert len(chat_files) == 1, chat_files\n\n        args = self.parse_partial_args()\n        if not args:\n            return []\n\n        res = chat_files[0], args[\"content\"]\n        dump(res)\n        return [res]\n\n    def apply_edits(self, edits):\n        for path, content in edits:\n            full_path = self.abs_root_path(path)\n            self.io.write_text(full_path, content)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/single_wholefile_func_prompts.py::1",
    "metadata": {
      "file_path": "aider/coders/single_wholefile_func_prompts.py",
      "file_name": "single_wholefile_func_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 203,
      "span_ids": [
        "docstring",
        "SingleWholeFileFunctionPrompts"
      ],
      "start_line": 1,
      "end_line": 28,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# flake8: noqa: E501\n\nfrom .base_prompts import CoderPrompts\n\n\nclass SingleWholeFileFunctionPrompts(CoderPrompts):\n    main_system = \"\"\"Act as an expert software developer.\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nOnce you understand the request you MUST use the `write_file` function to update the file to make the changes.\n\"\"\"\n\n    system_reminder = \"\"\"\nONLY return code using the `write_file` function.\nNEVER return code outside the `write_file` function.\n\"\"\"\n\n    files_content_prefix = \"Here is the current content of the file:\\n\"\n    files_no_full_files = \"I am not sharing any files yet.\"\n\n    redacted_edit_message = \"No changes are needed.\"\n\n    # TODO: should this be present for using this with gpt-4?\n    repo_content_prefix = None\n\n    # TODO: fix the chat history, except we can't keep the whole file\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/udiff_coder.py::1",
    "metadata": {
      "file_path": "aider/coders/udiff_coder.py",
      "file_name": "udiff_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 291,
      "span_ids": [
        "imports"
      ],
      "start_line": 1,
      "end_line": 43,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import difflib\nfrom itertools import groupby\nfrom pathlib import Path\n\nfrom ..dump import dump  # noqa: F401\nfrom .base_coder import Coder\nfrom .search_replace import (\n    SearchTextNotUnique,\n    all_preprocs,\n    diff_lines,\n    flexible_search_and_replace,\n    search_and_replace,\n)\nfrom .udiff_prompts import UnifiedDiffPrompts\n\nno_match_error = \"\"\"UnifiedDiffNoMatch: hunk failed to apply!\n\n{path} does not contain lines that match the diff you provided!\nTry again.\nDO NOT skip blank lines, comments, docstrings, etc!\nThe diff needs to apply cleanly to the lines in {path}!\n\n{path} does not contain these {num_lines} exact lines in a row:\n```\n{original}```\n\"\"\"\n\n\nnot_unique_error = \"\"\"UnifiedDiffNotUnique: hunk failed to apply!\n\n{path} contains multiple sets of lines that match the diff you provided!\nTry again.\nUse additional ` ` lines to provide context that uniquely indicates which code needs to be changed.\nThe diff needs to apply to a unique set of lines in {path}!\n\n{path} contains multiple copies of these {num_lines} lines:\n```\n{original}```\n\"\"\"\n\nother_hunks_applied = (\n    \"Note: some hunks did apply successfully. See the updated source code shown above.\\n\\n\"\n)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/udiff_coder.py::2",
    "metadata": {
      "file_path": "aider/coders/udiff_coder.py",
      "file_name": "udiff_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 131,
      "span_ids": [
        "UnifiedDiffCoder",
        "UnifiedDiffCoder.get_edits"
      ],
      "start_line": 46,
      "end_line": 66,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class UnifiedDiffCoder(Coder):\n    \"\"\"A coder that uses unified diff format for code modifications.\"\"\"\n    edit_format = \"udiff\"\n    gpt_prompts = UnifiedDiffPrompts()\n\n    def get_edits(self):\n        content = self.partial_response_content\n\n        # might raise ValueError for malformed ORIG/UPD blocks\n        raw_edits = list(find_diffs(content))\n\n        last_path = None\n        edits = []\n        for path, hunk in raw_edits:\n            if path:\n                last_path = path\n            else:\n                path = last_path\n            edits.append((path, hunk))\n\n        return edits",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/udiff_coder.py::3",
    "metadata": {
      "file_path": "aider/coders/udiff_coder.py",
      "file_name": "udiff_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 286,
      "span_ids": [
        "UnifiedDiffCoder.apply_edits"
      ],
      "start_line": 68,
      "end_line": 117,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class UnifiedDiffCoder(Coder):\n\n    def apply_edits(self, edits):\n        seen = set()\n        uniq = []\n        for path, hunk in edits:\n            hunk = normalize_hunk(hunk)\n            if not hunk:\n                continue\n\n            this = [path + \"\\n\"] + hunk\n            this = \"\".join(this)\n\n            if this in seen:\n                continue\n            seen.add(this)\n\n            uniq.append((path, hunk))\n\n        errors = []\n        for path, hunk in uniq:\n            full_path = self.abs_root_path(path)\n            content = self.io.read_text(full_path)\n\n            original, _ = hunk_to_before_after(hunk)\n\n            try:\n                content = do_replace(full_path, content, hunk)\n            except SearchTextNotUnique:\n                errors.append(\n                    not_unique_error.format(\n                        path=path, original=original, num_lines=len(original.splitlines())\n                    )\n                )\n                continue\n\n            if not content:\n                errors.append(\n                    no_match_error.format(\n                        path=path, original=original, num_lines=len(original.splitlines())\n                    )\n                )\n                continue\n\n            # SUCCESS!\n            self.io.write_text(full_path, content)\n\n        if errors:\n            errors = \"\\n\\n\".join(errors)\n            if len(errors) < len(uniq):\n                errors += other_hunks_applied\n            raise ValueError(errors)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/udiff_coder.py::4",
    "metadata": {
      "file_path": "aider/coders/udiff_coder.py",
      "file_name": "udiff_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 163,
      "span_ids": [
        "collapse_repeats",
        "do_replace"
      ],
      "start_line": 120,
      "end_line": 147,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def do_replace(fname, content, hunk):\n    fname = Path(fname)\n\n    before_text, after_text = hunk_to_before_after(hunk)\n\n    # does it want to make a new file?\n    if not fname.exists() and not before_text.strip():\n        fname.touch()\n        content = \"\"\n\n    if content is None:\n        return\n\n    # TODO: handle inserting into new file\n    if not before_text.strip():\n        # append to existing file, or start a new file\n        new_content = content + after_text\n        return new_content\n\n    new_content = None\n\n    new_content = apply_hunk(content, hunk)\n    if new_content:\n        return new_content\n\n\ndef collapse_repeats(s):\n    return \"\".join(k for k, g in groupby(s))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/udiff_coder.py::5",
    "metadata": {
      "file_path": "aider/coders/udiff_coder.py",
      "file_name": "udiff_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 299,
      "span_ids": [
        "apply_hunk"
      ],
      "start_line": 150,
      "end_line": 197,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def apply_hunk(content, hunk):\n    before_text, after_text = hunk_to_before_after(hunk)\n\n    res = directly_apply_hunk(content, hunk)\n    if res:\n        return res\n\n    hunk = make_new_lines_explicit(content, hunk)\n\n    # just consider space vs not-space\n    ops = \"\".join([line[0] for line in hunk])\n    ops = ops.replace(\"-\", \"x\")\n    ops = ops.replace(\"+\", \"x\")\n    ops = ops.replace(\"\\n\", \" \")\n\n    cur_op = \" \"\n    section = []\n    sections = []\n\n    for i in range(len(ops)):\n        op = ops[i]\n        if op != cur_op:\n            sections.append(section)\n            section = []\n            cur_op = op\n        section.append(hunk[i])\n\n    sections.append(section)\n    if cur_op != \" \":\n        sections.append([])\n\n    all_done = True\n    for i in range(2, len(sections), 2):\n        preceding_context = sections[i - 2]\n        changes = sections[i - 1]\n        following_context = sections[i]\n\n        res = apply_partial_hunk(content, preceding_context, changes, following_context)\n        if res:\n            content = res\n        else:\n            all_done = False\n            # FAILED!\n            # this_hunk = preceding_context + changes + following_context\n            break\n\n    if all_done:\n        return content",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/udiff_coder.py::6",
    "metadata": {
      "file_path": "aider/coders/udiff_coder.py",
      "file_name": "udiff_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 260,
      "span_ids": [
        "make_new_lines_explicit",
        "flexi_just_search_and_replace"
      ],
      "start_line": 200,
      "end_line": 239,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def flexi_just_search_and_replace(texts):\n    strategies = [\n        (search_and_replace, all_preprocs),\n    ]\n\n    return flexible_search_and_replace(texts, strategies)\n\n\ndef make_new_lines_explicit(content, hunk):\n    before, after = hunk_to_before_after(hunk)\n\n    diff = diff_lines(before, content)\n\n    back_diff = []\n    for line in diff:\n        if line[0] == \"+\":\n            continue\n        # if line[0] == \"-\":\n        #    line = \"+\" + line[1:]\n\n        back_diff.append(line)\n\n    new_before = directly_apply_hunk(before, back_diff)\n    if not new_before:\n        return hunk\n\n    if len(new_before.strip()) < 10:\n        return hunk\n\n    before = before.splitlines(keepends=True)\n    new_before = new_before.splitlines(keepends=True)\n    after = after.splitlines(keepends=True)\n\n    if len(new_before) < len(before) * 0.66:\n        return hunk\n\n    new_hunk = difflib.unified_diff(new_before, after, n=max(len(new_before), len(after)))\n    new_hunk = list(new_hunk)[3:]\n\n    return new_hunk",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/udiff_coder.py::7",
    "metadata": {
      "file_path": "aider/coders/udiff_coder.py",
      "file_name": "udiff_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 120,
      "span_ids": [
        "cleanup_pure_whitespace_lines",
        "normalize_hunk"
      ],
      "start_line": 242,
      "end_line": 257,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def cleanup_pure_whitespace_lines(lines):\n    res = [\n        line if line.strip() else line[-(len(line) - len(line.rstrip(\"\\r\\n\")))] for line in lines\n    ]\n    return res\n\n\ndef normalize_hunk(hunk):\n    before, after = hunk_to_before_after(hunk, lines=True)\n\n    before = cleanup_pure_whitespace_lines(before)\n    after = cleanup_pure_whitespace_lines(after)\n\n    diff = difflib.unified_diff(before, after, n=max(len(before), len(after)))\n    diff = list(diff)[3:]\n    return diff",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/udiff_coder.py::8",
    "metadata": {
      "file_path": "aider/coders/udiff_coder.py",
      "file_name": "udiff_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 144,
      "span_ids": [
        "directly_apply_hunk"
      ],
      "start_line": 260,
      "end_line": 278,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def directly_apply_hunk(content, hunk):\n    before, after = hunk_to_before_after(hunk)\n\n    if not before:\n        return\n\n    before_lines, _ = hunk_to_before_after(hunk, lines=True)\n    before_lines = \"\".join([line.strip() for line in before_lines])\n\n    # Refuse to do a repeated search and replace on a tiny bit of non-whitespace context\n    if len(before_lines) < 10 and content.count(before) > 1:\n        return\n\n    try:\n        new_content = flexi_just_search_and_replace([before, after, content])\n    except SearchTextNotUnique:\n        new_content = None\n\n    return new_content",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/udiff_coder.py::9",
    "metadata": {
      "file_path": "aider/coders/udiff_coder.py",
      "file_name": "udiff_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 198,
      "span_ids": [
        "apply_partial_hunk"
      ],
      "start_line": 281,
      "end_line": 308,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def apply_partial_hunk(content, preceding_context, changes, following_context):\n    len_prec = len(preceding_context)\n    len_foll = len(following_context)\n\n    use_all = len_prec + len_foll\n\n    # if there is a - in the hunk, we can go all the way to `use=0`\n    for drop in range(use_all + 1):\n        use = use_all - drop\n\n        for use_prec in range(len_prec, -1, -1):\n            if use_prec > use:\n                continue\n\n            use_foll = use - use_prec\n            if use_foll > len_foll:\n                continue\n\n            if use_prec:\n                this_prec = preceding_context[-use_prec:]\n            else:\n                this_prec = []\n\n            this_foll = following_context[:use_foll]\n\n            res = directly_apply_hunk(content, this_prec + changes + this_foll)\n            if res:\n                return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/udiff_coder.py::10",
    "metadata": {
      "file_path": "aider/coders/udiff_coder.py",
      "file_name": "udiff_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 162,
      "span_ids": [
        "find_diffs"
      ],
      "start_line": 311,
      "end_line": 333,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def find_diffs(content):\n    # We can always fence with triple-quotes, because all the udiff content\n    # is prefixed with +/-/space.\n\n    if not content.endswith(\"\\n\"):\n        content = content + \"\\n\"\n\n    lines = content.splitlines(keepends=True)\n    line_num = 0\n    edits = []\n    while line_num < len(lines):\n        while line_num < len(lines):\n            line = lines[line_num]\n            if line.startswith(\"```diff\"):\n                line_num, these_edits = process_fenced_block(lines, line_num + 1)\n                edits += these_edits\n                break\n            line_num += 1\n\n    # For now, just take 1!\n    # edits = edits[:1]\n\n    return edits",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/udiff_coder.py::11",
    "metadata": {
      "file_path": "aider/coders/udiff_coder.py",
      "file_name": "udiff_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 323,
      "span_ids": [
        "process_fenced_block"
      ],
      "start_line": 336,
      "end_line": 390,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def process_fenced_block(lines, start_line_num):\n    for line_num in range(start_line_num, len(lines)):\n        line = lines[line_num]\n        if line.startswith(\"```\"):\n            break\n\n    block = lines[start_line_num:line_num]\n    block.append(\"@@ @@\")\n\n    if block[0].startswith(\"--- \") and block[1].startswith(\"+++ \"):\n        # Extract the file path, considering that it might contain spaces\n        fname = block[1][4:].strip()\n        block = block[2:]\n    else:\n        fname = None\n\n    edits = []\n\n    keeper = False\n    hunk = []\n    op = \" \"\n    for line in block:\n        hunk.append(line)\n        if len(line) < 2:\n            continue\n\n        if line.startswith(\"+++ \") and hunk[-2].startswith(\"--- \"):\n            if hunk[-3] == \"\\n\":\n                hunk = hunk[:-3]\n            else:\n                hunk = hunk[:-2]\n\n            edits.append((fname, hunk))\n            hunk = []\n            keeper = False\n\n            fname = line[4:].strip()\n            continue\n\n        op = line[0]\n        if op in \"-+\":\n            keeper = True\n            continue\n        if op != \"@\":\n            continue\n        if not keeper:\n            hunk = []\n            continue\n\n        hunk = hunk[:-1]\n        edits.append((fname, hunk))\n        hunk = []\n        keeper = False\n\n    return line_num + 1, edits",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/udiff_coder.py::12",
    "metadata": {
      "file_path": "aider/coders/udiff_coder.py",
      "file_name": "udiff_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 136,
      "span_ids": [
        "hunk_to_before_after"
      ],
      "start_line": 393,
      "end_line": 420,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def hunk_to_before_after(hunk, lines=False):\n    before = []\n    after = []\n    op = \" \"\n    for line in hunk:\n        if len(line) < 2:\n            op = \" \"\n            line = line\n        else:\n            op = line[0]\n            line = line[1:]\n\n        if op == \" \":\n            before.append(line)\n            after.append(line)\n        elif op == \"-\":\n            before.append(line)\n        elif op == \"+\":\n            after.append(line)\n\n    if lines:\n        return before, after\n\n    before = \"\".join(before)\n    after = \"\".join(after)\n\n    return before, after",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/udiff_prompts.py::1",
    "metadata": {
      "file_path": "aider/coders/udiff_prompts.py",
      "file_name": "udiff_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 462,
      "span_ids": [
        "UnifiedDiffPrompts",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 73,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# flake8: noqa: E501\n\nfrom .base_prompts import CoderPrompts\n\n\nclass UnifiedDiffPrompts(CoderPrompts):\n    main_system = \"\"\"Act as an expert software developer.\n{lazy_prompt}\nAlways use best practices when coding.\nRespect and use existing conventions, libraries, etc that are already present in the code base.\n\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in {language}.\n\nFor each file that needs to be changed, write out the changes similar to a unified diff like `diff -U0` would produce.\n\"\"\"\n\n    example_messages = [\n        dict(\n            role=\"user\",\n            content=\"Replace is_prime with a call to sympy.\",\n        ),\n        dict(\n            role=\"assistant\",\n            content=\"\"\"Ok, I will:\n\n1. Add an imports of sympy.\n2. Remove the is_prime() function.\n3. Replace the existing call to is_prime() with a call to sympy.isprime().\n\nHere are the diffs for those changes:\n\n```diff\n--- mathweb/flask/app.py\n+++ mathweb/flask/app.py\n@@ ... @@\n-class MathWeb:\n+import sympy\n+\n+class MathWeb:\n@@ ... @@\n-def is_prime(x):\n-    if x < 2:\n-        return False\n-    for i in range(2, int(math.sqrt(x)) + 1):\n-        if x % i == 0:\n-            return False\n-    return True\n@@ ... @@\n-@app.route('/prime/<int:n>')\n-def nth_prime(n):\n-    count = 0\n-    num = 1\n-    while count < n:\n-        num += 1\n-        if is_prime(num):\n-            count += 1\n-    return str(num)\n+@app.route('/prime/<int:n>')\n+def nth_prime(n):\n+    count = 0\n+    num = 1\n+    while count < n:\n+        num += 1\n+        if sympy.isprime(num):\n+            count += 1\n+    return str(num)\n```\n\"\"\",\n        ),\n    ]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/udiff_prompts.py::2",
    "metadata": {
      "file_path": "aider/coders/udiff_prompts.py",
      "file_name": "udiff_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 368,
      "span_ids": [
        "UnifiedDiffPrompts"
      ],
      "start_line": 75,
      "end_line": 111,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class UnifiedDiffPrompts(CoderPrompts):\n\n    system_reminder = \"\"\"# File editing rules:\n\nReturn edits similar to unified diffs that `diff -U0` would produce.\n\nMake sure you include the first 2 lines with the file paths.\nDon't include timestamps with the file paths.\n\nStart each hunk of changes with a `@@ ... @@` line.\nDon't include line numbers like `diff -U0` does.\nThe user's patch tool doesn't need them.\n\nThe user's patch tool needs CORRECT patches that apply cleanly against the current contents of the file!\nThink carefully and make sure you include and mark all lines that need to be removed or changed as `-` lines.\nMake sure you mark all new or modified lines with `+`.\nDon't leave out any lines or the diff patch won't apply correctly.\n\nIndentation matters in the diffs!\n\nStart a new hunk for each section of the file that needs changes.\n\nOnly output hunks that specify changes with `+` or `-` lines.\nSkip any hunks that are entirely unchanging ` ` lines.\n\nOutput hunks in whatever order makes the most sense.\nHunks don't need to be in any particular order.\n\nWhen editing a function, method, loop, etc use a hunk to replace the *entire* code block.\nDelete the entire existing version with `-` lines and then add a new, updated version with `+` lines.\nThis will help you generate correct code and correct diffs.\n\nTo move code within a file, use 2 hunks: 1 to delete it from its current location, 1 to insert it in the new location.\n\nTo make a new file, show a diff from `--- /dev/null` to `+++ path/to/new/file.ext`.\n\n{lazy_prompt}\n\"\"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/wholefile_coder.py::1",
    "metadata": {
      "file_path": "aider/coders/wholefile_coder.py",
      "file_name": "wholefile_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 817,
      "span_ids": [
        "WholeFileCoder",
        "imports",
        "WholeFileCoder.render_incremental_response",
        "WholeFileCoder.get_edits"
      ],
      "start_line": 1,
      "end_line": 122,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from pathlib import Path\n\nfrom aider import diffs\n\nfrom ..dump import dump  # noqa: F401\nfrom .base_coder import Coder\nfrom .wholefile_prompts import WholeFilePrompts\n\n\nclass WholeFileCoder(Coder):\n    \"\"\"A coder that operates on entire files for code modifications.\"\"\"\n\n    edit_format = \"whole\"\n    gpt_prompts = WholeFilePrompts()\n\n    def render_incremental_response(self, final):\n        try:\n            return self.get_edits(mode=\"diff\")\n        except ValueError:\n            return self.get_multi_response_content()\n\n    def get_edits(self, mode=\"update\"):\n        content = self.get_multi_response_content()\n\n        chat_files = self.get_inchat_relative_files()\n\n        output = []\n        lines = content.splitlines(keepends=True)\n\n        edits = []\n\n        saw_fname = None\n        fname = None\n        fname_source = None\n        new_lines = []\n        for i, line in enumerate(lines):\n            if line.startswith(self.fence[0]) or line.startswith(self.fence[1]):\n                if fname is not None:\n                    # ending an existing block\n                    saw_fname = None\n\n                    full_path = self.abs_root_path(fname)\n\n                    if mode == \"diff\":\n                        output += self.do_live_diff(full_path, new_lines, True)\n                    else:\n                        edits.append((fname, fname_source, new_lines))\n\n                    fname = None\n                    fname_source = None\n                    new_lines = []\n                    continue\n\n                # fname==None ... starting a new block\n                if i > 0:\n                    fname_source = \"block\"\n                    fname = lines[i - 1].strip()\n                    fname = fname.strip(\"*\")  # handle **filename.py**\n                    fname = fname.rstrip(\":\")\n                    fname = fname.strip(\"`\")\n                    fname = fname.lstrip(\"#\")\n                    fname = fname.strip()\n\n                    # Issue #1232\n                    if len(fname) > 250:\n                        fname = \"\"\n\n                    # Did gpt prepend a bogus dir? It especially likes to\n                    # include the path/to prefix from the one-shot example in\n                    # the prompt.\n                    if fname and fname not in chat_files and Path(fname).name in chat_files:\n                        fname = Path(fname).name\n                if not fname:  # blank line? or ``` was on first line i==0\n                    if saw_fname:\n                        fname = saw_fname\n                        fname_source = \"saw\"\n                    elif len(chat_files) == 1:\n                        fname = chat_files[0]\n                        fname_source = \"chat\"\n                    else:\n                        # TODO: sense which file it is by diff size\n                        raise ValueError(\n                            f\"No filename provided before {self.fence[0]} in file listing\"\n                        )\n\n            elif fname is not None:\n                new_lines.append(line)\n            else:\n                for word in line.strip().split():\n                    word = word.rstrip(\".:,;!\")\n                    for chat_file in chat_files:\n                        quoted_chat_file = f\"`{chat_file}`\"\n                        if word == quoted_chat_file:\n                            saw_fname = chat_file\n\n                output.append(line)\n\n        if mode == \"diff\":\n            if fname is not None:\n                # ending an existing block\n                full_path = (Path(self.root) / fname).absolute()\n                output += self.do_live_diff(full_path, new_lines, False)\n            return \"\\n\".join(output)\n\n        if fname:\n            edits.append((fname, fname_source, new_lines))\n\n        seen = set()\n        refined_edits = []\n        # process from most reliable filename, to least reliable\n        for source in (\"block\", \"saw\", \"chat\"):\n            for fname, fname_source, new_lines in edits:\n                if fname_source != source:\n                    continue\n                # if a higher priority source already edited the file, skip\n                if fname in seen:\n                    continue\n\n                seen.add(fname)\n                refined_edits.append((fname, fname_source, new_lines))\n\n        return refined_edits",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/wholefile_coder.py::2",
    "metadata": {
      "file_path": "aider/coders/wholefile_coder.py",
      "file_name": "wholefile_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 161,
      "span_ids": [
        "WholeFileCoder.do_live_diff",
        "WholeFileCoder.apply_edits"
      ],
      "start_line": 124,
      "end_line": 145,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class WholeFileCoder(Coder):\n\n    def apply_edits(self, edits):\n        for path, fname_source, new_lines in edits:\n            full_path = self.abs_root_path(path)\n            new_lines = \"\".join(new_lines)\n            self.io.write_text(full_path, new_lines)\n\n    def do_live_diff(self, full_path, new_lines, final):\n        if Path(full_path).exists():\n            orig_lines = self.io.read_text(full_path)\n            if orig_lines is not None:\n                orig_lines = orig_lines.splitlines(keepends=True)\n\n                show_diff = diffs.diff_partial_update(\n                    orig_lines,\n                    new_lines,\n                    final=final,\n                ).splitlines()\n                return show_diff\n\n        output = [\"```\"] + new_lines + [\"```\"]\n        return output",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/wholefile_func_coder.py::1",
    "metadata": {
      "file_path": "aider/coders/wholefile_func_coder.py",
      "file_name": "wholefile_func_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 338,
      "span_ids": [
        "imports",
        "WholeFileFunctionCoder.__init__",
        "WholeFileFunctionCoder",
        "WholeFileFunctionCoder.update_cur_messages"
      ],
      "start_line": 1,
      "end_line": 58,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from aider import diffs\n\nfrom ..dump import dump  # noqa: F401\nfrom .base_coder import Coder\nfrom .wholefile_func_prompts import WholeFileFunctionPrompts\n\n\nclass WholeFileFunctionCoder(Coder):\n    functions = [\n        dict(\n            name=\"write_file\",\n            description=\"create or update one or more files\",\n            parameters=dict(\n                type=\"object\",\n                required=[\"explanation\", \"files\"],\n                properties=dict(\n                    explanation=dict(\n                        type=\"string\",\n                        description=(\n                            \"Step by step plan for the changes to be made to the code (future\"\n                            \" tense, markdown format)\"\n                        ),\n                    ),\n                    files=dict(\n                        type=\"array\",\n                        items=dict(\n                            type=\"object\",\n                            required=[\"path\", \"content\"],\n                            properties=dict(\n                                path=dict(\n                                    type=\"string\",\n                                    description=\"Path of file to write\",\n                                ),\n                                content=dict(\n                                    type=\"string\",\n                                    description=\"Content to write to the file\",\n                                ),\n                            ),\n                        ),\n                    ),\n                ),\n            ),\n        ),\n    ]\n\n    def __init__(self, *args, **kwargs):\n        raise RuntimeError(\"Deprecated, needs to be refactored to support get_edits/apply_edits\")\n\n        self.gpt_prompts = WholeFileFunctionPrompts()\n        super().__init__(*args, **kwargs)\n\n    def update_cur_messages(self, edited):\n        if edited:\n            self.cur_messages += [\n                dict(role=\"assistant\", content=self.gpt_prompts.redacted_edit_message)\n            ]\n        else:\n            self.cur_messages += [dict(role=\"assistant\", content=self.partial_response_content)]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/wholefile_func_coder.py::2",
    "metadata": {
      "file_path": "aider/coders/wholefile_func_coder.py",
      "file_name": "wholefile_func_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 164,
      "span_ids": [
        "WholeFileFunctionCoder.render_incremental_response"
      ],
      "start_line": 60,
      "end_line": 87,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class WholeFileFunctionCoder(Coder):\n\n    def render_incremental_response(self, final=False):\n        if self.partial_response_content:\n            return self.partial_response_content\n\n        args = self.parse_partial_args()\n\n        if not args:\n            return\n\n        explanation = args.get(\"explanation\")\n        files = args.get(\"files\", [])\n\n        res = \"\"\n        if explanation:\n            res += f\"{explanation}\\n\\n\"\n\n        for i, file_upd in enumerate(files):\n            path = file_upd.get(\"path\")\n            if not path:\n                continue\n            content = file_upd.get(\"content\")\n            if not content:\n                continue\n\n            this_final = (i < len(files) - 1) or final\n            res += self.live_diffs(path, content, this_final)\n\n        return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/wholefile_func_coder.py::3",
    "metadata": {
      "file_path": "aider/coders/wholefile_func_coder.py",
      "file_name": "wholefile_func_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 119,
      "span_ids": [
        "WholeFileFunctionCoder.live_diffs"
      ],
      "start_line": 89,
      "end_line": 108,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class WholeFileFunctionCoder(Coder):\n\n    def live_diffs(self, fname, content, final):\n        lines = content.splitlines(keepends=True)\n\n        # ending an existing block\n        full_path = self.abs_root_path(fname)\n\n        content = self.io.read_text(full_path)\n        if content is None:\n            orig_lines = []\n        else:\n            orig_lines = content.splitlines()\n\n        show_diff = diffs.diff_partial_update(\n            orig_lines,\n            lines,\n            final,\n            fname=fname,\n        ).splitlines()\n\n        return \"\\n\".join(show_diff)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/wholefile_func_coder.py::4",
    "metadata": {
      "file_path": "aider/coders/wholefile_func_coder.py",
      "file_name": "wholefile_func_coder.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 165,
      "span_ids": [
        "WholeFileFunctionCoder._update_files"
      ],
      "start_line": 110,
      "end_line": 135,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class WholeFileFunctionCoder(Coder):\n\n    def _update_files(self):\n        name = self.partial_response_function_call.get(\"name\")\n        if name and name != \"write_file\":\n            raise ValueError(f'Unknown function_call name=\"{name}\", use name=\"write_file\"')\n\n        args = self.parse_partial_args()\n        if not args:\n            return\n\n        files = args.get(\"files\", [])\n\n        edited = set()\n        for file_upd in files:\n            path = file_upd.get(\"path\")\n            if not path:\n                raise ValueError(f\"Missing path parameter: {file_upd}\")\n\n            content = file_upd.get(\"content\")\n            if not content:\n                raise ValueError(f\"Missing content parameter: {file_upd}\")\n\n            if self.allowed_to_edit(path, content):\n                edited.add(path)\n\n        return edited",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/wholefile_func_prompts.py::1",
    "metadata": {
      "file_path": "aider/coders/wholefile_func_prompts.py",
      "file_name": "wholefile_func_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 203,
      "span_ids": [
        "docstring",
        "WholeFileFunctionPrompts"
      ],
      "start_line": 1,
      "end_line": 28,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# flake8: noqa: E501\n\nfrom .base_prompts import CoderPrompts\n\n\nclass WholeFileFunctionPrompts(CoderPrompts):\n    main_system = \"\"\"Act as an expert software developer.\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nOnce you understand the request you MUST use the `write_file` function to edit the files to make the needed changes.\n\"\"\"\n\n    system_reminder = \"\"\"\nONLY return code using the `write_file` function.\nNEVER return code outside the `write_file` function.\n\"\"\"\n\n    files_content_prefix = \"Here is the current content of the files:\\n\"\n    files_no_full_files = \"I am not sharing any files yet.\"\n\n    redacted_edit_message = \"No changes are needed.\"\n\n    # TODO: should this be present for using this with gpt-4?\n    repo_content_prefix = None\n\n    # TODO: fix the chat history, except we can't keep the whole file\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "coders/wholefile_prompts.py::1",
    "metadata": {
      "file_path": "aider/coders/wholefile_prompts.py",
      "file_name": "wholefile_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 461,
      "span_ids": [
        "WholeFilePrompts",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 68,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# flake8: noqa: E501\n\nfrom .base_prompts import CoderPrompts\n\n\nclass WholeFilePrompts(CoderPrompts):\n    main_system = \"\"\"Act as an expert software developer.\nTake requests for changes to the supplied code.\nIf the request is ambiguous, ask questions.\n\nAlways reply to the user in {language}.\n\n{lazy_prompt}\nOnce you understand the request you MUST:\n1. Determine if any code changes are needed.\n2. Explain any needed changes.\n3. If changes are needed, output a copy of each file that needs changes.\n\"\"\"\n\n    example_messages = [\n        dict(\n            role=\"user\",\n            content=\"Change the greeting to be more casual\",\n        ),\n        dict(\n            role=\"assistant\",\n            content=\"\"\"Ok, I will:\n\n1. Switch the greeting text from \"Hello\" to \"Hey\".\n\nshow_greeting.py\n{fence[0]}\nimport sys\n\ndef greeting(name):\n    print(f\"Hey {{name}}\")\n\nif __name__ == '__main__':\n    greeting(sys.argv[1])\n{fence[1]}\n\"\"\",\n        ),\n    ]\n\n    system_reminder = \"\"\"To suggest changes to a file you MUST return the entire content of the updated file.\nYou MUST use this *file listing* format:\n\npath/to/filename.js\n{fence[0]}\n// entire file content ...\n// ... goes in between\n{fence[1]}\n\nEvery *file listing* MUST use this format:\n- First line: the filename with any originally provided path; no extra markup, punctuation, comments, etc. **JUST** the filename with path.\n- Second line: opening {fence[0]}\n- ... entire content of the file ...\n- Final line: closing {fence[1]}\n\nTo suggest changes to a file you MUST return a *file listing* that contains the entire content of the file.\n*NEVER* skip, omit or elide content from a *file listing* using \"...\" or by adding comments like \"... rest of code...\"!\nCreate a new file you MUST return a *file listing* which includes an appropriate filename, including any appropriate path.\n\n{lazy_prompt}\n\"\"\"\n\n    redacted_edit_message = \"No changes are needed.\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::1",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 182,
      "span_ids": [
        "imports",
        "SwitchCoder",
        "SwitchCoder.__init__"
      ],
      "start_line": 1,
      "end_line": 31,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import glob\nimport os\nimport re\nimport subprocess\nimport sys\nimport tempfile\nfrom collections import OrderedDict\nfrom os.path import expanduser\nfrom pathlib import Path\n\nimport pyperclip\nfrom PIL import Image, ImageGrab\nfrom prompt_toolkit.completion import Completion, PathCompleter\nfrom prompt_toolkit.document import Document\n\nfrom aider import models, prompts, voice\nfrom aider.editor import pipe_editor\nfrom aider.format_settings import format_settings\nfrom aider.help import Help, install_help_extra\nfrom aider.llm import litellm\nfrom aider.repo import ANY_GIT_ERROR\nfrom aider.run_cmd import run_cmd\nfrom aider.scrape import Scraper, install_playwright\nfrom aider.utils import is_image_file\n\nfrom .dump import dump  # noqa: F401\n\n\nclass SwitchCoder(Exception):\n    def __init__(self, **kwargs):\n        self.kwargs = kwargs",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::2",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 261,
      "span_ids": [
        "Commands.cmd_model",
        "Commands.clone",
        "Commands.__init__",
        "Commands"
      ],
      "start_line": 34,
      "end_line": 86,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n    voice = None\n    scraper = None\n\n    def clone(self):\n        return Commands(\n            self.io,\n            None,\n            voice_language=self.voice_language,\n            verify_ssl=self.verify_ssl,\n            args=self.args,\n            parser=self.parser,\n            verbose=self.verbose,\n            editor=self.editor,\n        )\n\n    def __init__(\n        self,\n        io,\n        coder,\n        voice_language=None,\n        voice_input_device=None,\n        voice_format=None,\n        verify_ssl=True,\n        args=None,\n        parser=None,\n        verbose=False,\n        editor=None,\n    ):\n        self.io = io\n        self.coder = coder\n        self.parser = parser\n        self.args = args\n        self.verbose = verbose\n\n        self.verify_ssl = verify_ssl\n        if voice_language == \"auto\":\n            voice_language = None\n\n        self.voice_language = voice_language\n        self.voice_format = voice_format\n        self.voice_input_device = voice_input_device\n\n        self.help = None\n        self.editor = editor\n\n    def cmd_model(self, args):\n        \"Switch to a new LLM\"\n\n        model_name = args.strip()\n        model = models.Model(model_name)\n        models.sanity_check_models(self.io, model)\n        raise SwitchCoder(main_model=model)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::3",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 439,
      "span_ids": [
        "Commands.cmd_chat_mode"
      ],
      "start_line": 88,
      "end_line": 149,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_chat_mode(self, args):\n        \"Switch to a new chat mode\"\n\n        from aider import coders\n\n        ef = args.strip()\n        valid_formats = OrderedDict(\n            sorted(\n                (\n                    coder.edit_format,\n                    coder.__doc__.strip().split(\"\\n\")[0] if coder.__doc__ else \"No description\",\n                )\n                for coder in coders.__all__\n                if getattr(coder, \"edit_format\", None)\n            )\n        )\n\n        show_formats = OrderedDict(\n            [\n                (\"help\", \"Get help about using aider (usage, config, troubleshoot).\"),\n                (\"ask\", \"Ask questions about your code without making any changes.\"),\n                (\"code\", \"Ask for changes to your code (using the best edit format).\"),\n                (\n                    \"architect\",\n                    (\n                        \"Work with an architect model to design code changes, and an editor to make\"\n                        \" them.\"\n                    ),\n                ),\n            ]\n        )\n\n        if ef not in valid_formats and ef not in show_formats:\n            if ef:\n                self.io.tool_error(f'Chat mode \"{ef}\" should be one of these:\\n')\n            else:\n                self.io.tool_output(\"Chat mode should be one of these:\\n\")\n\n            max_format_length = max(len(format) for format in valid_formats.keys())\n            for format, description in show_formats.items():\n                self.io.tool_output(f\"- {format:<{max_format_length}} : {description}\")\n\n            self.io.tool_output(\"\\nOr a valid edit format:\\n\")\n            for format, description in valid_formats.items():\n                if format not in show_formats:\n                    self.io.tool_output(f\"- {format:<{max_format_length}} : {description}\")\n\n            return\n\n        summarize_from_coder = True\n        edit_format = ef\n\n        if ef == \"code\":\n            edit_format = self.coder.main_model.edit_format\n            summarize_from_coder = False\n        elif ef == \"ask\":\n            summarize_from_coder = False\n\n        raise SwitchCoder(\n            edit_format=edit_format,\n            summarize_from_coder=summarize_from_coder,\n        )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::4",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 292,
      "span_ids": [
        "Commands.cmd_web",
        "Commands.completions_model",
        "Commands.cmd_models"
      ],
      "start_line": 151,
      "end_line": 193,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def completions_model(self):\n        models = litellm.model_cost.keys()\n        return models\n\n    def cmd_models(self, args):\n        \"Search the list of available models\"\n\n        args = args.strip()\n\n        if args:\n            models.print_matching_models(self.io, args)\n        else:\n            self.io.tool_output(\"Please provide a partial model name to search for.\")\n\n    def cmd_web(self, args, return_content=False):\n        \"Scrape a webpage, convert to markdown and send in a message\"\n\n        url = args.strip()\n        if not url:\n            self.io.tool_error(\"Please provide a URL to scrape.\")\n            return\n\n        self.io.tool_output(f\"Scraping {url}...\")\n        if not self.scraper:\n            res = install_playwright(self.io)\n            if not res:\n                self.io.tool_warning(\"Unable to initialize playwright.\")\n\n            self.scraper = Scraper(\n                print_error=self.io.tool_error, playwright_available=res, verify_ssl=self.verify_ssl\n            )\n\n        content = self.scraper.scrape(url) or \"\"\n        content = f\"Here is the content of {url}:\\n\\n\" + content\n        if return_content:\n            return content\n\n        self.io.tool_output(\"... added to chat.\")\n\n        self.coder.cur_messages += [\n            dict(role=\"user\", content=content),\n            dict(role=\"assistant\", content=\"Ok.\"),\n        ]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::5",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 382,
      "span_ids": [
        "Commands.is_command",
        "Commands.do_run",
        "Commands.matching_commands",
        "Commands.get_raw_completions",
        "Commands.get_completions",
        "Commands.get_commands"
      ],
      "start_line": 195,
      "end_line": 250,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def is_command(self, inp):\n        return inp[0] in \"/!\"\n\n    def get_raw_completions(self, cmd):\n        assert cmd.startswith(\"/\")\n        cmd = cmd[1:]\n        cmd = cmd.replace(\"-\", \"_\")\n\n        raw_completer = getattr(self, f\"completions_raw_{cmd}\", None)\n        return raw_completer\n\n    def get_completions(self, cmd):\n        assert cmd.startswith(\"/\")\n        cmd = cmd[1:]\n\n        cmd = cmd.replace(\"-\", \"_\")\n        fun = getattr(self, f\"completions_{cmd}\", None)\n        if not fun:\n            return\n        return sorted(fun())\n\n    def get_commands(self):\n        commands = []\n        for attr in dir(self):\n            if not attr.startswith(\"cmd_\"):\n                continue\n            cmd = attr[4:]\n            cmd = cmd.replace(\"_\", \"-\")\n            commands.append(\"/\" + cmd)\n\n        return commands\n\n    def do_run(self, cmd_name, args):\n        cmd_name = cmd_name.replace(\"-\", \"_\")\n        cmd_method_name = f\"cmd_{cmd_name}\"\n        cmd_method = getattr(self, cmd_method_name, None)\n        if not cmd_method:\n            self.io.tool_output(f\"Error: Command {cmd_name} not found.\")\n            return\n\n        try:\n            return cmd_method(args)\n        except ANY_GIT_ERROR as err:\n            self.io.tool_error(f\"Unable to complete {cmd_name}: {err}\")\n\n    def matching_commands(self, inp):\n        words = inp.strip().split()\n        if not words:\n            return\n\n        first_word = words[0]\n        rest_inp = inp[len(words[0]) :].strip()\n\n        all_commands = self.get_commands()\n        matching_commands = [cmd for cmd in all_commands if cmd.startswith(first_word)]\n        return matching_commands, first_word, rest_inp",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::6",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 195,
      "span_ids": [
        "Commands.run"
      ],
      "start_line": 252,
      "end_line": 272,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def run(self, inp):\n        if inp.startswith(\"!\"):\n            self.coder.event(\"command_run\")\n            return self.do_run(\"run\", inp[1:])\n\n        res = self.matching_commands(inp)\n        if res is None:\n            return\n        matching_commands, first_word, rest_inp = res\n        if len(matching_commands) == 1:\n            command = matching_commands[0][1:]\n            self.coder.event(f\"command_{command}\")\n            return self.do_run(command, rest_inp)\n        elif first_word in matching_commands:\n            command = first_word[1:]\n            self.coder.event(f\"command_{command}\")\n            return self.do_run(command, rest_inp)\n        elif len(matching_commands) > 1:\n            self.io.tool_error(f\"Ambiguous command: {', '.join(matching_commands)}\")\n        else:\n            self.io.tool_error(f\"Invalid command: {first_word}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::7",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 166,
      "span_ids": [
        "Commands.cmd_commit",
        "Commands.run",
        "Commands.raw_cmd_commit"
      ],
      "start_line": 274,
      "end_line": 294,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    # any method called cmd_xxx becomes a command automatically.\n    # each one must take an args param.\n\n    def cmd_commit(self, args=None):\n        \"Commit edits to the repo made outside the chat (commit message optional)\"\n        try:\n            self.raw_cmd_commit(args)\n        except ANY_GIT_ERROR as err:\n            self.io.tool_error(f\"Unable to complete commit: {err}\")\n\n    def raw_cmd_commit(self, args=None):\n        if not self.coder.repo:\n            self.io.tool_error(\"No git repository found.\")\n            return\n\n        if not self.coder.repo.is_dirty():\n            self.io.tool_warning(\"No more changes to commit.\")\n            return\n\n        commit_message = args.strip() if args else None\n        self.coder.repo.commit(message=commit_message)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::8",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 380,
      "span_ids": [
        "Commands.cmd_lint"
      ],
      "start_line": 296,
      "end_line": 349,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_lint(self, args=\"\", fnames=None):\n        \"Lint and fix in-chat files or all dirty files if none in chat\"\n\n        if not self.coder.repo:\n            self.io.tool_error(\"No git repository found.\")\n            return\n\n        if not fnames:\n            fnames = self.coder.get_inchat_relative_files()\n\n        # If still no files, get all dirty files in the repo\n        if not fnames and self.coder.repo:\n            fnames = self.coder.repo.get_dirty_files()\n\n        if not fnames:\n            self.io.tool_warning(\"No dirty files to lint.\")\n            return\n\n        fnames = [self.coder.abs_root_path(fname) for fname in fnames]\n\n        lint_coder = None\n        for fname in fnames:\n            try:\n                errors = self.coder.linter.lint(fname)\n            except FileNotFoundError as err:\n                self.io.tool_error(f\"Unable to lint {fname}\")\n                self.io.tool_output(str(err))\n                continue\n\n            if not errors:\n                continue\n\n            self.io.tool_output(errors)\n            if not self.io.confirm_ask(f\"Fix lint errors in {fname}?\", default=\"y\"):\n                continue\n\n            # Commit everything before we start fixing lint errors\n            if self.coder.repo.is_dirty() and self.coder.dirty_commits:\n                self.cmd_commit(\"\")\n\n            if not lint_coder:\n                lint_coder = self.coder.clone(\n                    # Clear the chat history, fnames\n                    cur_messages=[],\n                    done_messages=[],\n                    fnames=None,\n                )\n\n            lint_coder.add_rel_fname(fname)\n            lint_coder.run(errors)\n            lint_coder.abs_fnames = set()\n\n        if lint_coder and self.coder.repo.is_dirty() and self.coder.auto_commits:\n            self.cmd_commit(\"\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::9",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 126,
      "span_ids": [
        "Commands._drop_all_files",
        "Commands.cmd_reset",
        "Commands._clear_chat_history",
        "Commands.cmd_clear"
      ],
      "start_line": 351,
      "end_line": 368,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_clear(self, args):\n        \"Clear the chat history\"\n\n        self._clear_chat_history()\n\n    def _drop_all_files(self):\n        self.coder.abs_fnames = set()\n        self.coder.abs_read_only_fnames = set()\n\n    def _clear_chat_history(self):\n        self.coder.done_messages = []\n        self.coder.cur_messages = []\n\n    def cmd_reset(self, args):\n        \"Drop all files and clear the chat history\"\n        self._drop_all_files()\n        self._clear_chat_history()\n        self.io.tool_output(\"All files dropped and chat history cleared.\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::10",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 822,
      "span_ids": [
        "Commands.cmd_tokens"
      ],
      "start_line": 370,
      "end_line": 459,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_tokens(self, args):\n        \"Report on the number of tokens used by the current chat context\"\n\n        res = []\n\n        self.coder.choose_fence()\n\n        # system messages\n        main_sys = self.coder.fmt_system_prompt(self.coder.gpt_prompts.main_system)\n        main_sys += \"\\n\" + self.coder.fmt_system_prompt(self.coder.gpt_prompts.system_reminder)\n        msgs = [\n            dict(role=\"system\", content=main_sys),\n            dict(\n                role=\"system\",\n                content=self.coder.fmt_system_prompt(self.coder.gpt_prompts.system_reminder),\n            ),\n        ]\n\n        tokens = self.coder.main_model.token_count(msgs)\n        res.append((tokens, \"system messages\", \"\"))\n\n        # chat history\n        msgs = self.coder.done_messages + self.coder.cur_messages\n        if msgs:\n            tokens = self.coder.main_model.token_count(msgs)\n            res.append((tokens, \"chat history\", \"use /clear to clear\"))\n\n        # repo map\n        other_files = set(self.coder.get_all_abs_files()) - set(self.coder.abs_fnames)\n        if self.coder.repo_map:\n            repo_content = self.coder.repo_map.get_repo_map(self.coder.abs_fnames, other_files)\n            if repo_content:\n                tokens = self.coder.main_model.token_count(repo_content)\n                res.append((tokens, \"repository map\", \"use --map-tokens to resize\"))\n\n        fence = \"`\" * 3\n\n        # files\n        for fname in self.coder.abs_fnames:\n            relative_fname = self.coder.get_rel_fname(fname)\n            content = self.io.read_text(fname)\n            if is_image_file(relative_fname):\n                tokens = self.coder.main_model.token_count_for_image(fname)\n            else:\n                # approximate\n                content = f\"{relative_fname}\\n{fence}\\n\" + content + \"{fence}\\n\"\n                tokens = self.coder.main_model.token_count(content)\n            res.append((tokens, f\"{relative_fname}\", \"/drop to remove\"))\n\n        # read-only files\n        for fname in self.coder.abs_read_only_fnames:\n            relative_fname = self.coder.get_rel_fname(fname)\n            content = self.io.read_text(fname)\n            if content is not None and not is_image_file(relative_fname):\n                # approximate\n                content = f\"{relative_fname}\\n{fence}\\n\" + content + \"{fence}\\n\"\n                tokens = self.coder.main_model.token_count(content)\n                res.append((tokens, f\"{relative_fname} (read-only)\", \"/drop to remove\"))\n\n        self.io.tool_output(\n            f\"Approximate context window usage for {self.coder.main_model.name}, in tokens:\"\n        )\n        self.io.tool_output()\n\n        width = 8\n        cost_width = 9\n\n        def fmt(v):\n            return format(int(v), \",\").rjust(width)\n\n        col_width = max(len(row[1]) for row in res)\n\n        cost_pad = \" \" * cost_width\n        total = 0\n        total_cost = 0.0\n        for tk, msg, tip in res:\n            total += tk\n            cost = tk * (self.coder.main_model.info.get(\"input_cost_per_token\") or 0)\n            total_cost += cost\n            msg = msg.ljust(col_width)\n            self.io.tool_output(f\"${cost:7.4f} {fmt(tk)} {msg} {tip}\")  # noqa: E231\n\n        self.io.tool_output(\"=\" * (width + cost_width + 1))\n        self.io.tool_output(f\"${total_cost:7.4f} {fmt(total)} tokens total\")  # noqa: E231\n\n        limit = self.coder.main_model.info.get(\"max_input_tokens\") or 0\n        if not limit:\n            return\n\n        remaining = limit - total\n        # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::11",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 206,
      "span_ids": [
        "Commands.cmd_undo",
        "Commands.cmd_tokens"
      ],
      "start_line": 460,
      "end_line": 479,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_tokens(self, args):\n        # ... other code\n        if remaining > 1024:\n            self.io.tool_output(f\"{cost_pad}{fmt(remaining)} tokens remaining in context window\")\n        elif remaining > 0:\n            self.io.tool_error(\n                f\"{cost_pad}{fmt(remaining)} tokens remaining in context window (use /drop or\"\n                \" /clear to make space)\"\n            )\n        else:\n            self.io.tool_error(\n                f\"{cost_pad}{fmt(remaining)} tokens remaining, window exhausted (use /drop or\"\n                \" /clear to make space)\"\n            )\n        self.io.tool_output(f\"{cost_pad}{fmt(limit)} tokens max context window size\")\n\n    def cmd_undo(self, args):\n        \"Undo the last git commit if it was done by aider\"\n        try:\n            self.raw_cmd_undo(args)\n        except ANY_GIT_ERROR as err:\n            self.io.tool_error(f\"Unable to complete undo: {err}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::12",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 770,
      "span_ids": [
        "Commands.raw_cmd_undo"
      ],
      "start_line": 481,
      "end_line": 574,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def raw_cmd_undo(self, args):\n        if not self.coder.repo:\n            self.io.tool_error(\"No git repository found.\")\n            return\n\n        last_commit = self.coder.repo.get_head_commit()\n        if not last_commit or not last_commit.parents:\n            self.io.tool_error(\"This is the first commit in the repository. Cannot undo.\")\n            return\n\n        last_commit_hash = self.coder.repo.get_head_commit_sha(short=True)\n        last_commit_message = self.coder.repo.get_head_commit_message(\"(unknown)\").strip()\n        if last_commit_hash not in self.coder.aider_commit_hashes:\n            self.io.tool_error(\"The last commit was not made by aider in this chat session.\")\n            self.io.tool_output(\n                \"You could try `/git reset --hard HEAD^` but be aware that this is a destructive\"\n                \" command!\"\n            )\n            return\n\n        if len(last_commit.parents) > 1:\n            self.io.tool_error(\n                f\"The last commit {last_commit.hexsha} has more than 1 parent, can't undo.\"\n            )\n            return\n\n        prev_commit = last_commit.parents[0]\n        changed_files_last_commit = [item.a_path for item in last_commit.diff(prev_commit)]\n\n        for fname in changed_files_last_commit:\n            if self.coder.repo.repo.is_dirty(path=fname):\n                self.io.tool_error(\n                    f\"The file {fname} has uncommitted changes. Please stash them before undoing.\"\n                )\n                return\n\n            # Check if the file was in the repo in the previous commit\n            try:\n                prev_commit.tree[fname]\n            except KeyError:\n                self.io.tool_error(\n                    f\"The file {fname} was not in the repository in the previous commit. Cannot\"\n                    \" undo safely.\"\n                )\n                return\n\n        local_head = self.coder.repo.repo.git.rev_parse(\"HEAD\")\n        current_branch = self.coder.repo.repo.active_branch.name\n        try:\n            remote_head = self.coder.repo.repo.git.rev_parse(f\"origin/{current_branch}\")\n            has_origin = True\n        except ANY_GIT_ERROR:\n            has_origin = False\n\n        if has_origin:\n            if local_head == remote_head:\n                self.io.tool_error(\n                    \"The last commit has already been pushed to the origin. Undoing is not\"\n                    \" possible.\"\n                )\n                return\n\n        # Reset only the files which are part of `last_commit`\n        restored = set()\n        unrestored = set()\n        for file_path in changed_files_last_commit:\n            try:\n                self.coder.repo.repo.git.checkout(\"HEAD~1\", file_path)\n                restored.add(file_path)\n            except ANY_GIT_ERROR:\n                unrestored.add(file_path)\n\n        if unrestored:\n            self.io.tool_error(f\"Error restoring {file_path}, aborting undo.\")\n            self.io.tool_output(\"Restored files:\")\n            for file in restored:\n                self.io.tool_output(f\"  {file}\")\n            self.io.tool_output(\"Unable to restore files:\")\n            for file in unrestored:\n                self.io.tool_output(f\"  {file}\")\n            return\n\n        # Move the HEAD back before the latest commit\n        self.coder.repo.repo.git.reset(\"--soft\", \"HEAD~1\")\n\n        self.io.tool_output(f\"Removed: {last_commit_hash} {last_commit_message}\")\n\n        # Get the current HEAD after undo\n        current_head_hash = self.coder.repo.get_head_commit_sha(short=True)\n        current_head_message = self.coder.repo.get_head_commit_message(\"(unknown)\").strip()\n        self.io.tool_output(f\"Now at:  {current_head_hash} {current_head_message}\")\n\n        if self.coder.main_model.send_undo_reply:\n            return prompts.undo_command_reply",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::13",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 310,
      "span_ids": [
        "Commands.cmd_diff",
        "Commands.raw_cmd_diff",
        "Commands.quote_fname"
      ],
      "start_line": 576,
      "end_line": 619,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_diff(self, args=\"\"):\n        \"Display the diff of changes since the last message\"\n        try:\n            self.raw_cmd_diff(args)\n        except ANY_GIT_ERROR as err:\n            self.io.tool_error(f\"Unable to complete diff: {err}\")\n\n    def raw_cmd_diff(self, args=\"\"):\n        if not self.coder.repo:\n            self.io.tool_error(\"No git repository found.\")\n            return\n\n        current_head = self.coder.repo.get_head_commit_sha()\n        if current_head is None:\n            self.io.tool_error(\"Unable to get current commit. The repository might be empty.\")\n            return\n\n        if len(self.coder.commit_before_message) < 2:\n            commit_before_message = current_head + \"^\"\n        else:\n            commit_before_message = self.coder.commit_before_message[-2]\n\n        if not commit_before_message or commit_before_message == current_head:\n            self.io.tool_warning(\"No changes to display since the last message.\")\n            return\n\n        self.io.tool_output(f\"Diff since {commit_before_message[:7]}...\")\n\n        if self.coder.pretty:\n            run_cmd(f\"git diff {commit_before_message}\")\n            return\n\n        diff = self.coder.repo.diff_commits(\n            self.coder.pretty,\n            commit_before_message,\n            \"HEAD\",\n        )\n\n        self.io.print(diff)\n\n    def quote_fname(self, fname):\n        if \" \" in fname and '\"' not in fname:\n            fname = f'\"{fname}\"'\n        return fname",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::14",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 384,
      "span_ids": [
        "Commands.completions_raw_read_only"
      ],
      "start_line": 621,
      "end_line": 676,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def completions_raw_read_only(self, document, complete_event):\n        # Get the text before the cursor\n        text = document.text_before_cursor\n\n        # Skip the first word and the space after it\n        after_command = text.split()[-1]\n\n        # Create a new Document object with the text after the command\n        new_document = Document(after_command, cursor_position=len(after_command))\n\n        def get_paths():\n            return [self.coder.root] if self.coder.root else None\n\n        path_completer = PathCompleter(\n            get_paths=get_paths,\n            only_directories=False,\n            expanduser=True,\n        )\n\n        # Adjust the start_position to replace all of 'after_command'\n        adjusted_start_position = -len(after_command)\n\n        # Collect all completions\n        all_completions = []\n\n        # Iterate over the completions and modify them\n        for completion in path_completer.get_completions(new_document, complete_event):\n            quoted_text = self.quote_fname(after_command + completion.text)\n            all_completions.append(\n                Completion(\n                    text=quoted_text,\n                    start_position=adjusted_start_position,\n                    display=completion.display,\n                    style=completion.style,\n                    selected_style=completion.selected_style,\n                )\n            )\n\n        # Add completions from the 'add' command\n        add_completions = self.completions_add()\n        for completion in add_completions:\n            if after_command in completion:\n                all_completions.append(\n                    Completion(\n                        text=completion,\n                        start_position=adjusted_start_position,\n                        display=completion,\n                    )\n                )\n\n        # Sort all completions based on their text\n        sorted_completions = sorted(all_completions, key=lambda c: c.text)\n\n        # Yield the sorted completions\n        for completion in sorted_completions:\n            yield completion",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::15",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 279,
      "span_ids": [
        "Commands.completions_add",
        "Commands.glob_filtered_to_repo"
      ],
      "start_line": 678,
      "end_line": 716,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def completions_add(self):\n        files = set(self.coder.get_all_relative_files())\n        files = files - set(self.coder.get_inchat_relative_files())\n        files = [self.quote_fname(fn) for fn in files]\n        return files\n\n    def glob_filtered_to_repo(self, pattern):\n        if not pattern.strip():\n            return []\n        try:\n            if os.path.isabs(pattern):\n                # Handle absolute paths\n                raw_matched_files = [Path(pattern)]\n            else:\n                try:\n                    raw_matched_files = list(Path(self.coder.root).glob(pattern))\n                except (IndexError, AttributeError):\n                    raw_matched_files = []\n        except ValueError as err:\n            self.io.tool_error(f\"Error matching {pattern}: {err}\")\n            raw_matched_files = []\n\n        matched_files = []\n        for fn in raw_matched_files:\n            matched_files += expand_subdir(fn)\n\n        matched_files = [\n            fn.relative_to(self.coder.root)\n            for fn in matched_files\n            if fn.is_relative_to(self.coder.root)\n        ]\n\n        # if repo, filter against it\n        if self.coder.repo:\n            git_files = self.coder.repo.get_tracked_files()\n            matched_files = [fn for fn in matched_files if str(fn) in git_files]\n\n        res = list(map(str, matched_files))\n        return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::16",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 786,
      "span_ids": [
        "Commands.cmd_add"
      ],
      "start_line": 718,
      "end_line": 807,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_add(self, args):\n        \"Add files to the chat so aider can edit them or review them in detail\"\n\n        all_matched_files = set()\n\n        filenames = parse_quoted_filenames(args)\n        for word in filenames:\n            if Path(word).is_absolute():\n                fname = Path(word)\n            else:\n                fname = Path(self.coder.root) / word\n\n            if self.coder.repo and self.coder.repo.ignored_file(fname):\n                self.io.tool_warning(f\"Skipping {fname} due to aiderignore or --subtree-only.\")\n                continue\n\n            if fname.exists():\n                if fname.is_file():\n                    all_matched_files.add(str(fname))\n                    continue\n                # an existing dir, escape any special chars so they won't be globs\n                word = re.sub(r\"([\\*\\?\\[\\]])\", r\"[\\1]\", word)\n\n            matched_files = self.glob_filtered_to_repo(word)\n            if matched_files:\n                all_matched_files.update(matched_files)\n                continue\n\n            if \"*\" in str(fname) or \"?\" in str(fname):\n                self.io.tool_error(\n                    f\"No match, and cannot create file with wildcard characters: {fname}\"\n                )\n                continue\n\n            if fname.exists() and fname.is_dir() and self.coder.repo:\n                self.io.tool_error(f\"Directory {fname} is not in git.\")\n                self.io.tool_output(f\"You can add to git with: /git add {fname}\")\n                continue\n\n            if self.io.confirm_ask(f\"No files matched '{word}'. Do you want to create {fname}?\"):\n                try:\n                    fname.touch()\n                    all_matched_files.add(str(fname))\n                except OSError as e:\n                    self.io.tool_error(f\"Error creating file {fname}: {e}\")\n\n        for matched_file in sorted(all_matched_files):\n            abs_file_path = self.coder.abs_root_path(matched_file)\n\n            if not abs_file_path.startswith(self.coder.root) and not is_image_file(matched_file):\n                self.io.tool_error(\n                    f\"Can not add {abs_file_path}, which is not within {self.coder.root}\"\n                )\n                continue\n\n            if self.coder.repo and self.coder.repo.git_ignored_file(matched_file):\n                self.io.tool_error(f\"Can't add {matched_file} which is in gitignore\")\n                continue\n\n            if abs_file_path in self.coder.abs_fnames:\n                self.io.tool_error(f\"{matched_file} is already in the chat as an editable file\")\n                continue\n            elif abs_file_path in self.coder.abs_read_only_fnames:\n                if self.coder.repo and self.coder.repo.path_in_repo(matched_file):\n                    self.coder.abs_read_only_fnames.remove(abs_file_path)\n                    self.coder.abs_fnames.add(abs_file_path)\n                    self.io.tool_output(\n                        f\"Moved {matched_file} from read-only to editable files in the chat\"\n                    )\n                else:\n                    self.io.tool_error(\n                        f\"Cannot add {matched_file} as it's not part of the repository\"\n                    )\n            else:\n                if is_image_file(matched_file) and not self.coder.main_model.info.get(\n                    \"supports_vision\"\n                ):\n                    self.io.tool_error(\n                        f\"Cannot add image file {matched_file} as the\"\n                        f\" {self.coder.main_model.name} does not support images.\"\n                    )\n                    continue\n                content = self.io.read_text(abs_file_path)\n                if content is None:\n                    self.io.tool_error(f\"Unable to read {matched_file}\")\n                else:\n                    self.coder.abs_fnames.add(abs_file_path)\n                    fname = self.coder.get_rel_fname(abs_file_path)\n                    self.io.tool_output(f\"Added {fname} to the chat\")\n                    self.coder.check_added_files()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::17",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 479,
      "span_ids": [
        "Commands.completions_drop",
        "Commands.cmd_drop"
      ],
      "start_line": 809,
      "end_line": 864,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def completions_drop(self):\n        files = self.coder.get_inchat_relative_files()\n        read_only_files = [self.coder.get_rel_fname(fn) for fn in self.coder.abs_read_only_fnames]\n        all_files = files + read_only_files\n        all_files = [self.quote_fname(fn) for fn in all_files]\n        return all_files\n\n    def cmd_drop(self, args=\"\"):\n        \"Remove files from the chat session to free up context space\"\n\n        if not args.strip():\n            self.io.tool_output(\"Dropping all files from the chat session.\")\n            self._drop_all_files()\n            return\n\n        filenames = parse_quoted_filenames(args)\n        for word in filenames:\n            # Expand tilde in the path\n            expanded_word = os.path.expanduser(word)\n\n            # Handle read-only files with substring matching and samefile check\n            read_only_matched = []\n            for f in self.coder.abs_read_only_fnames:\n                if expanded_word in f:\n                    read_only_matched.append(f)\n                    continue\n\n                # Try samefile comparison for relative paths\n                try:\n                    abs_word = os.path.abspath(expanded_word)\n                    if os.path.samefile(abs_word, f):\n                        read_only_matched.append(f)\n                except (FileNotFoundError, OSError):\n                    continue\n\n            for matched_file in read_only_matched:\n                self.coder.abs_read_only_fnames.remove(matched_file)\n                self.io.tool_output(f\"Removed read-only file {matched_file} from the chat\")\n\n            # For editable files, use glob if word contains glob chars, otherwise use substring\n            if any(c in expanded_word for c in \"*?[]\"):\n                matched_files = self.glob_filtered_to_repo(expanded_word)\n            else:\n                # Use substring matching like we do for read-only files\n                matched_files = [\n                    self.coder.get_rel_fname(f) for f in self.coder.abs_fnames if expanded_word in f\n                ]\n\n            if not matched_files:\n                matched_files.append(expanded_word)\n\n            for matched_file in matched_files:\n                abs_fname = self.coder.abs_root_path(matched_file)\n                if abs_fname in self.coder.abs_fnames:\n                    self.coder.abs_fnames.remove(abs_fname)\n                    self.io.tool_output(f\"Removed {matched_file} from the chat\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::18",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 264,
      "span_ids": [
        "Commands.cmd_git",
        "Commands.cmd_test"
      ],
      "start_line": 866,
      "end_line": 910,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_git(self, args):\n        \"Run a git command (output excluded from chat)\"\n        combined_output = None\n        try:\n            args = \"git \" + args\n            env = dict(subprocess.os.environ)\n            env[\"GIT_EDITOR\"] = \"true\"\n            result = subprocess.run(\n                args,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                text=True,\n                env=env,\n                shell=True,\n                encoding=self.io.encoding,\n                errors=\"replace\",\n            )\n            combined_output = result.stdout\n        except Exception as e:\n            self.io.tool_error(f\"Error running /git command: {e}\")\n\n        if combined_output is None:\n            return\n\n        self.io.tool_output(combined_output)\n\n    def cmd_test(self, args):\n        \"Run a shell command and add the output to the chat on non-zero exit code\"\n        if not args and self.coder.test_cmd:\n            args = self.coder.test_cmd\n\n        if not args:\n            return\n\n        if not callable(args):\n            if type(args) is not str:\n                raise ValueError(repr(args))\n            return self.cmd_run(args, True)\n\n        errors = args()\n        if not errors:\n            return\n\n        self.io.tool_output(errors)\n        return errors",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::19",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 288,
      "span_ids": [
        "Commands.cmd_run"
      ],
      "start_line": 912,
      "end_line": 946,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_run(self, args, add_on_nonzero_exit=False):\n        \"Run a shell command and optionally add the output to the chat (alias: !)\"\n        exit_status, combined_output = run_cmd(\n            args, verbose=self.verbose, error_print=self.io.tool_error, cwd=self.coder.root\n        )\n\n        if combined_output is None:\n            return\n\n        # Calculate token count of output\n        token_count = self.coder.main_model.token_count(combined_output)\n        k_tokens = token_count / 1000\n\n        if add_on_nonzero_exit:\n            add = exit_status != 0\n        else:\n            add = self.io.confirm_ask(f\"Add {k_tokens:.1f}k tokens of command output to the chat?\")\n\n        if add:\n            num_lines = len(combined_output.strip().splitlines())\n            line_plural = \"line\" if num_lines == 1 else \"lines\"\n            self.io.tool_output(f\"Added {num_lines} {line_plural} of output to the chat.\")\n\n            msg = prompts.run_output.format(\n                command=args,\n                output=combined_output,\n            )\n\n            self.coder.cur_messages += [\n                dict(role=\"user\", content=msg),\n                dict(role=\"assistant\", content=\"Ok.\"),\n            ]\n\n            if add and exit_status != 0:\n                self.io.placeholder = \"What's wrong? Fix\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::20",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 347,
      "span_ids": [
        "Commands.cmd_ls",
        "Commands.cmd_quit",
        "Commands.cmd_exit"
      ],
      "start_line": 948,
      "end_line": 994,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_exit(self, args):\n        \"Exit the application\"\n        self.coder.event(\"exit\", reason=\"/exit\")\n        sys.exit()\n\n    def cmd_quit(self, args):\n        \"Exit the application\"\n        self.cmd_exit(args)\n\n    def cmd_ls(self, args):\n        \"List all known files and indicate which are included in the chat session\"\n\n        files = self.coder.get_all_relative_files()\n\n        other_files = []\n        chat_files = []\n        read_only_files = []\n        for file in files:\n            abs_file_path = self.coder.abs_root_path(file)\n            if abs_file_path in self.coder.abs_fnames:\n                chat_files.append(file)\n            else:\n                other_files.append(file)\n\n        # Add read-only files\n        for abs_file_path in self.coder.abs_read_only_fnames:\n            rel_file_path = self.coder.get_rel_fname(abs_file_path)\n            read_only_files.append(rel_file_path)\n\n        if not chat_files and not other_files and not read_only_files:\n            self.io.tool_output(\"\\nNo files in chat, git repo, or read-only list.\")\n            return\n\n        if other_files:\n            self.io.tool_output(\"Repo files not in the chat:\\n\")\n        for file in other_files:\n            self.io.tool_output(f\"  {file}\")\n\n        if read_only_files:\n            self.io.tool_output(\"\\nRead-only files:\\n\")\n        for file in read_only_files:\n            self.io.tool_output(f\"  {file}\")\n\n        if chat_files:\n            self.io.tool_output(\"\\nFiles in chat:\\n\")\n        for file in chat_files:\n            self.io.tool_output(f\"  {file}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::21",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 157,
      "span_ids": [
        "Commands.basic_help"
      ],
      "start_line": 996,
      "end_line": 1010,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def basic_help(self):\n        commands = sorted(self.get_commands())\n        pad = max(len(cmd) for cmd in commands)\n        pad = \"{cmd:\" + str(pad) + \"}\"\n        for cmd in commands:\n            cmd_method_name = f\"cmd_{cmd[1:]}\".replace(\"-\", \"_\")\n            cmd_method = getattr(self, cmd_method_name, None)\n            cmd = pad.format(cmd=cmd)\n            if cmd_method:\n                description = cmd_method.__doc__\n                self.io.tool_output(f\"{cmd} {description}\")\n            else:\n                self.io.tool_output(f\"{cmd} No description available.\")\n        self.io.tool_output()\n        self.io.tool_output(\"Use `/help <question>` to ask questions about how to use aider.\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::22",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 309,
      "span_ids": [
        "Commands.cmd_help"
      ],
      "start_line": 1012,
      "end_line": 1061,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_help(self, args):\n        \"Ask questions about aider\"\n\n        if not args.strip():\n            self.basic_help()\n            return\n\n        self.coder.event(\"interactive help\")\n        from aider.coders.base_coder import Coder\n\n        if not self.help:\n            res = install_help_extra(self.io)\n            if not res:\n                self.io.tool_error(\"Unable to initialize interactive help.\")\n                return\n\n            self.help = Help()\n\n        coder = Coder.create(\n            io=self.io,\n            from_coder=self.coder,\n            edit_format=\"help\",\n            summarize_from_coder=False,\n            map_tokens=512,\n            map_mul_no_files=1,\n        )\n        user_msg = self.help.ask(args)\n        user_msg += \"\"\"\n# Announcement lines from when this session of aider was launched:\n\n\"\"\"\n        user_msg += \"\\n\".join(self.coder.get_announcements()) + \"\\n\"\n\n        coder.run(user_msg, preproc=False)\n\n        if self.coder.repo_map:\n            map_tokens = self.coder.repo_map.max_map_tokens\n            map_mul_no_files = self.coder.repo_map.map_mul_no_files\n        else:\n            map_tokens = 0\n            map_mul_no_files = 1\n\n        raise SwitchCoder(\n            edit_format=self.coder.edit_format,\n            summarize_from_coder=False,\n            from_coder=coder,\n            map_tokens=map_tokens,\n            map_mul_no_files=map_mul_no_files,\n            show_announcements=False,\n        )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::23",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 147,
      "span_ids": [
        "Commands.cmd_ask",
        "Commands.cmd_architect",
        "Commands.cmd_code"
      ],
      "start_line": 1063,
      "end_line": 1073,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_ask(self, args):\n        \"\"\"Ask questions about the code base without editing any files. If no prompt provided, switches to ask mode.\"\"\"  # noqa\n        return self._generic_chat_command(args, \"ask\")\n\n    def cmd_code(self, args):\n        \"\"\"Ask for changes to your code. If no prompt provided, switches to code mode.\"\"\"  # noqa\n        return self._generic_chat_command(args, self.coder.main_model.edit_format)\n\n    def cmd_architect(self, args):\n        \"\"\"Enter architect/editor mode using 2 different models. If no prompt provided, switches to architect/editor mode.\"\"\"  # noqa\n        return self._generic_chat_command(args, \"architect\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::24",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 140,
      "span_ids": [
        "Commands._generic_chat_command"
      ],
      "start_line": 1075,
      "end_line": 1097,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def _generic_chat_command(self, args, edit_format):\n        if not args.strip():\n            # Switch to the corresponding chat mode if no args provided\n            return self.cmd_chat_mode(edit_format)\n\n        from aider.coders.base_coder import Coder\n\n        coder = Coder.create(\n            io=self.io,\n            from_coder=self.coder,\n            edit_format=edit_format,\n            summarize_from_coder=False,\n        )\n\n        user_msg = args\n        coder.run(user_msg)\n\n        raise SwitchCoder(\n            edit_format=self.coder.edit_format,\n            summarize_from_coder=False,\n            from_coder=coder,\n            show_announcements=False,\n        )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::25",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 139,
      "span_ids": [
        "Commands.get_help_md"
      ],
      "start_line": 1099,
      "end_line": 1117,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def get_help_md(self):\n        \"Show help about all commands in markdown\"\n\n        res = \"\"\"\n|Command|Description|\n|:------|:----------|\n\"\"\"\n        commands = sorted(self.get_commands())\n        for cmd in commands:\n            cmd_method_name = f\"cmd_{cmd[1:]}\".replace(\"-\", \"_\")\n            cmd_method = getattr(self, cmd_method_name, None)\n            if cmd_method:\n                description = cmd_method.__doc__\n                res += f\"| **{cmd}** | {description} |\\n\"\n            else:\n                res += f\"| **{cmd}** | |\\n\"\n\n        res += \"\\n\"\n        return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::26",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 191,
      "span_ids": [
        "Commands.cmd_voice"
      ],
      "start_line": 1119,
      "end_line": 1143,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_voice(self, args):\n        \"Record and transcribe voice input\"\n\n        if not self.voice:\n            if \"OPENAI_API_KEY\" not in os.environ:\n                self.io.tool_error(\"To use /voice you must provide an OpenAI API key.\")\n                return\n            try:\n                self.voice = voice.Voice(\n                    audio_format=self.voice_format or \"wav\", device_name=self.voice_input_device\n                )\n            except voice.SoundDeviceError:\n                self.io.tool_error(\n                    \"Unable to import `sounddevice` and/or `soundfile`, is portaudio installed?\"\n                )\n                return\n\n        try:\n            text = self.voice.record_and_transcribe(None, language=self.voice_language)\n        except litellm.OpenAIError as err:\n            self.io.tool_error(f\"Unable to use OpenAI whisper model: {err}\")\n            return\n\n        if text:\n            self.io.placeholder = text",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::27",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 392,
      "span_ids": [
        "Commands.cmd_paste"
      ],
      "start_line": 1145,
      "end_line": 1193,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_paste(self, args):\n        \"\"\"Paste image/text from the clipboard into the chat.\\\n        Optionally provide a name for the image.\"\"\"\n        try:\n            # Check for image first\n            image = ImageGrab.grabclipboard()\n            if isinstance(image, Image.Image):\n                if args.strip():\n                    filename = args.strip()\n                    ext = os.path.splitext(filename)[1].lower()\n                    if ext in (\".jpg\", \".jpeg\", \".png\"):\n                        basename = filename\n                    else:\n                        basename = f\"{filename}.png\"\n                else:\n                    basename = \"clipboard_image.png\"\n\n                temp_dir = tempfile.mkdtemp()\n                temp_file_path = os.path.join(temp_dir, basename)\n                image_format = \"PNG\" if basename.lower().endswith(\".png\") else \"JPEG\"\n                image.save(temp_file_path, image_format)\n\n                abs_file_path = Path(temp_file_path).resolve()\n\n                # Check if a file with the same name already exists in the chat\n                existing_file = next(\n                    (f for f in self.coder.abs_fnames if Path(f).name == abs_file_path.name), None\n                )\n                if existing_file:\n                    self.coder.abs_fnames.remove(existing_file)\n                    self.io.tool_output(f\"Replaced existing image in the chat: {existing_file}\")\n\n                self.coder.abs_fnames.add(str(abs_file_path))\n                self.io.tool_output(f\"Added clipboard image to the chat: {abs_file_path}\")\n                self.coder.check_added_files()\n\n                return\n\n            # If not an image, try to get text\n            text = pyperclip.paste()\n            if text:\n                self.io.tool_output(text)\n                return text\n\n            self.io.tool_error(\"No image or text content found in clipboard.\")\n            return\n\n        except Exception as e:\n            self.io.tool_error(f\"Error processing clipboard content: {e}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::28",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 331,
      "span_ids": [
        "Commands.cmd_read_only"
      ],
      "start_line": 1195,
      "end_line": 1232,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_read_only(self, args):\n        \"Add files to the chat that are for reference only, or turn added files to read-only\"\n        if not args.strip():\n            # Convert all files in chat to read-only\n            for fname in list(self.coder.abs_fnames):\n                self.coder.abs_fnames.remove(fname)\n                self.coder.abs_read_only_fnames.add(fname)\n                rel_fname = self.coder.get_rel_fname(fname)\n                self.io.tool_output(f\"Converted {rel_fname} to read-only\")\n            return\n\n        filenames = parse_quoted_filenames(args)\n        all_paths = []\n\n        # First collect all expanded paths\n        for pattern in filenames:\n            expanded_pattern = expanduser(pattern)\n            if os.path.isabs(expanded_pattern):\n                # For absolute paths, glob it\n                matches = list(glob.glob(expanded_pattern))\n            else:\n                # For relative paths and globs, use glob from the root directory\n                matches = list(Path(self.coder.root).glob(expanded_pattern))\n\n            if not matches:\n                self.io.tool_error(f\"No matches found for: {pattern}\")\n            else:\n                all_paths.extend(matches)\n\n        # Then process them in sorted order\n        for path in sorted(all_paths):\n            abs_path = self.coder.abs_root_path(path)\n            if os.path.isfile(abs_path):\n                self._add_read_only_file(abs_path, path)\n            elif os.path.isdir(abs_path):\n                self._add_read_only_directory(abs_path, path)\n            else:\n                self.io.tool_error(f\"Not a file or directory: {abs_path}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::29",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 214,
      "span_ids": [
        "Commands._add_read_only_file"
      ],
      "start_line": 1234,
      "end_line": 1253,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def _add_read_only_file(self, abs_path, original_name):\n        if is_image_file(original_name) and not self.coder.main_model.info.get(\"supports_vision\"):\n            self.io.tool_error(\n                f\"Cannot add image file {original_name} as the\"\n                f\" {self.coder.main_model.name} does not support images.\"\n            )\n            return\n\n        if abs_path in self.coder.abs_read_only_fnames:\n            self.io.tool_error(f\"{original_name} is already in the chat as a read-only file\")\n            return\n        elif abs_path in self.coder.abs_fnames:\n            self.coder.abs_fnames.remove(abs_path)\n            self.coder.abs_read_only_fnames.add(abs_path)\n            self.io.tool_output(\n                f\"Moved {original_name} from editable to read-only files in the chat\"\n            )\n        else:\n            self.coder.abs_read_only_fnames.add(abs_path)\n            self.io.tool_output(f\"Added {original_name} to read-only files.\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::30",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 162,
      "span_ids": [
        "Commands._add_read_only_directory"
      ],
      "start_line": 1255,
      "end_line": 1272,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def _add_read_only_directory(self, abs_path, original_name):\n        added_files = 0\n        for root, _, files in os.walk(abs_path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                if (\n                    file_path not in self.coder.abs_fnames\n                    and file_path not in self.coder.abs_read_only_fnames\n                ):\n                    self.coder.abs_read_only_fnames.add(file_path)\n                    added_files += 1\n\n        if added_files > 0:\n            self.io.tool_output(\n                f\"Added {added_files} files from directory {original_name} to read-only files.\"\n            )\n        else:\n            self.io.tool_output(f\"No new files added from directory {original_name}.\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::31",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 203,
      "span_ids": [
        "Commands.cmd_settings",
        "Commands.cmd_map",
        "Commands.cmd_map_refresh",
        "Commands.completions_raw_load"
      ],
      "start_line": 1274,
      "end_line": 1296,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_map(self, args):\n        \"Print out the current repository map\"\n        repo_map = self.coder.get_repo_map()\n        if repo_map:\n            self.io.tool_output(repo_map)\n        else:\n            self.io.tool_output(\"No repository map available.\")\n\n    def cmd_map_refresh(self, args):\n        \"Force a refresh of the repository map\"\n        repo_map = self.coder.get_repo_map(force_refresh=True)\n        if repo_map:\n            self.io.tool_output(\"The repo map has been refreshed, use /map to view it.\")\n\n    def cmd_settings(self, args):\n        \"Print out the current settings\"\n        settings = format_settings(self.parser, self.args)\n        announcements = \"\\n\".join(self.coder.get_announcements())\n        output = f\"{announcements}\\n{settings}\"\n        self.io.tool_output(output)\n\n    def completions_raw_load(self, document, complete_event):\n        return self.completions_raw_read_only(document, complete_event)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::32",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 220,
      "span_ids": [
        "Commands.cmd_load",
        "Commands.completions_raw_save"
      ],
      "start_line": 1298,
      "end_line": 1328,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_load(self, args):\n        \"Load and execute commands from a file\"\n        if not args.strip():\n            self.io.tool_error(\"Please provide a filename containing commands to load.\")\n            return\n\n        try:\n            with open(args.strip(), \"r\", encoding=self.io.encoding, errors=\"replace\") as f:\n                commands = f.readlines()\n        except FileNotFoundError:\n            self.io.tool_error(f\"File not found: {args}\")\n            return\n        except Exception as e:\n            self.io.tool_error(f\"Error reading file: {e}\")\n            return\n\n        for cmd in commands:\n            cmd = cmd.strip()\n            if not cmd or cmd.startswith(\"#\"):\n                continue\n\n            self.io.tool_output(f\"\\nExecuting: {cmd}\")\n            try:\n                self.run(cmd)\n            except SwitchCoder:\n                self.io.tool_error(\n                    f\"Command '{cmd}' is only supported in interactive mode, skipping.\"\n                )\n\n    def completions_raw_save(self, document, complete_event):\n        return self.completions_raw_read_only(document, complete_event)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::33",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 257,
      "span_ids": [
        "Commands.cmd_save"
      ],
      "start_line": 1330,
      "end_line": 1355,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_save(self, args):\n        \"Save commands to a file that can reconstruct the current chat session's files\"\n        if not args.strip():\n            self.io.tool_error(\"Please provide a filename to save the commands to.\")\n            return\n\n        try:\n            with open(args.strip(), \"w\", encoding=self.io.encoding) as f:\n                f.write(\"/drop\\n\")\n                # Write commands to add editable files\n                for fname in sorted(self.coder.abs_fnames):\n                    rel_fname = self.coder.get_rel_fname(fname)\n                    f.write(f\"/add       {rel_fname}\\n\")\n\n                # Write commands to add read-only files\n                for fname in sorted(self.coder.abs_read_only_fnames):\n                    # Use absolute path for files outside repo root, relative path for files inside\n                    if Path(fname).is_relative_to(self.coder.root):\n                        rel_fname = self.coder.get_rel_fname(fname)\n                        f.write(f\"/read-only {rel_fname}\\n\")\n                    else:\n                        f.write(f\"/read-only {fname}\\n\")\n\n            self.io.tool_output(f\"Saved commands to {args.strip()}\")\n        except Exception as e:\n            self.io.tool_error(f\"Error saving commands to file: {e}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::34",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 284,
      "span_ids": [
        "Commands.cmd_multiline_mode",
        "Commands.cmd_copy"
      ],
      "start_line": 1357,
      "end_line": 1386,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_multiline_mode(self, args):\n        \"Toggle multiline mode (swaps behavior of Enter and Meta+Enter)\"\n        self.io.toggle_multiline_mode()\n\n    def cmd_copy(self, args):\n        \"Copy the last assistant message to the clipboard\"\n        all_messages = self.coder.done_messages + self.coder.cur_messages\n        assistant_messages = [msg for msg in reversed(all_messages) if msg[\"role\"] == \"assistant\"]\n\n        if not assistant_messages:\n            self.io.tool_error(\"No assistant messages found to copy.\")\n            return\n\n        last_assistant_message = assistant_messages[0][\"content\"]\n\n        try:\n            pyperclip.copy(last_assistant_message)\n            preview = (\n                last_assistant_message[:50] + \"...\"\n                if len(last_assistant_message) > 50\n                else last_assistant_message\n            )\n            self.io.tool_output(f\"Copied last assistant message to clipboard. Preview: {preview}\")\n        except pyperclip.PyperclipException as e:\n            self.io.tool_error(f\"Failed to copy to clipboard: {str(e)}\")\n            self.io.tool_output(\n                \"You may need to install xclip or xsel on Linux, or pbcopy on macOS.\"\n            )\n        except Exception as e:\n            self.io.tool_error(f\"An unexpected error occurred while copying to clipboard: {str(e)}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::35",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 137,
      "span_ids": [
        "Commands.cmd_editor",
        "Commands.cmd_report"
      ],
      "start_line": 1388,
      "end_line": 1407,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_report(self, args):\n        \"Report a problem by opening a GitHub Issue\"\n        from aider.report import report_github_issue\n\n        announcements = \"\\n\".join(self.coder.get_announcements())\n        issue_text = announcements\n\n        if args.strip():\n            title = args.strip()\n        else:\n            title = None\n\n        report_github_issue(issue_text, title=title, confirm=False)\n\n    def cmd_editor(self, initial_content=\"\"):\n        \"Open an editor to write a prompt\"\n\n        user_input = pipe_editor(initial_content, suffix=\"md\", editor=self.editor)\n        if user_input.strip():\n            self.io.set_placeholder(user_input.rstrip())",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::36",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 316,
      "span_ids": [
        "Commands.cmd_copy_context"
      ],
      "start_line": 1409,
      "end_line": 1451,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Commands:\n\n    def cmd_copy_context(self, args=None):\n        \"\"\"Copy the current chat context as markdown, suitable to paste into a web UI\"\"\"\n\n        chunks = self.coder.format_chat_chunks()\n\n        markdown = \"\"\n\n        # Only include specified chunks in order\n        for messages in [chunks.repo, chunks.readonly_files, chunks.chat_files]:\n            for msg in messages:\n                # Only include user messages\n                if msg[\"role\"] != \"user\":\n                    continue\n\n                content = msg[\"content\"]\n\n                # Handle image/multipart content\n                if isinstance(content, list):\n                    for part in content:\n                        if part.get(\"type\") == \"text\":\n                            markdown += part[\"text\"] + \"\\n\\n\"\n                else:\n                    markdown += content + \"\\n\\n\"\n\n        args = args or \"\"\n        markdown += f\"\"\"\nJust tell me how to edit the files to make the changes.\nDon't give me back entire files.\nJust show me the edits I need to make.\n\n{args}\n\"\"\"\n\n        try:\n            pyperclip.copy(markdown)\n            self.io.tool_output(\"Copied code context to clipboard.\")\n        except pyperclip.PyperclipException as e:\n            self.io.tool_error(f\"Failed to copy to clipboard: {str(e)}\")\n            self.io.tool_output(\n                \"You may need to install xclip or xsel on Linux, or pbcopy on macOS.\"\n            )\n        except Exception as e:\n            self.io.tool_error(f\"An unexpected error occurred while copying to clipboard: {str(e)}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/commands.py::37",
    "metadata": {
      "file_path": "aider/commands.py",
      "file_name": "commands.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 148,
      "span_ids": [
        "expand_subdir",
        "get_help_md",
        "parse_quoted_filenames",
        "main",
        "impl"
      ],
      "start_line": 1454,
      "end_line": 1484,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def expand_subdir(file_path):\n    if file_path.is_file():\n        yield file_path\n        return\n\n    if file_path.is_dir():\n        for file in file_path.rglob(\"*\"):\n            if file.is_file():\n                yield file\n\n\ndef parse_quoted_filenames(args):\n    filenames = re.findall(r\"\\\"(.+?)\\\"|(\\S+)\", args)\n    filenames = [name for sublist in filenames for name in sublist if name]\n    return filenames\n\n\ndef get_help_md():\n    md = Commands(None, None).get_help_md()\n    return md\n\n\ndef main():\n    md = get_help_md()\n    print(md)\n\n\nif __name__ == \"__main__\":\n    status = main()\n    sys.exit(status)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/copypaste.py::1",
    "metadata": {
      "file_path": "aider/copypaste.py",
      "file_name": "copypaste.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 400,
      "span_ids": [
        "ClipboardWatcher.__init__",
        "ClipboardWatcher.stop",
        "ClipboardWatcher",
        "main",
        "imports",
        "ClipboardWatcher.start",
        "impl"
      ],
      "start_line": 1,
      "end_line": 73,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import threading\nimport time\n\nimport pyperclip\n\n\nclass ClipboardWatcher:\n    \"\"\"Watches clipboard for changes and updates IO placeholder\"\"\"\n\n    def __init__(self, io, verbose=False):\n        self.io = io\n        self.verbose = verbose\n        self.stop_event = None\n        self.watcher_thread = None\n        self.last_clipboard = None\n        self.io.clipboard_watcher = self\n\n    def start(self):\n        \"\"\"Start watching clipboard for changes\"\"\"\n        self.stop_event = threading.Event()\n        self.last_clipboard = pyperclip.paste()\n\n        def watch_clipboard():\n            while not self.stop_event.is_set():\n                try:\n                    current = pyperclip.paste()\n                    if current != self.last_clipboard:\n                        self.last_clipboard = current\n                        self.io.interrupt_input()\n                        self.io.placeholder = current\n                        if len(current.splitlines()) > 1:\n                            self.io.placeholder = \"\\n\" + self.io.placeholder + \"\\n\"\n\n                    time.sleep(0.5)\n                except Exception as e:\n                    if self.verbose:\n                        from aider.dump import dump\n\n                        dump(f\"Clipboard watcher error: {e}\")\n                    continue\n\n        self.watcher_thread = threading.Thread(target=watch_clipboard, daemon=True)\n        self.watcher_thread.start()\n\n    def stop(self):\n        \"\"\"Stop watching clipboard for changes\"\"\"\n        if self.stop_event:\n            self.stop_event.set()\n        if self.watcher_thread:\n            self.watcher_thread.join()\n            self.watcher_thread = None\n            self.stop_event = None\n\n\ndef main():\n    \"\"\"Example usage of the clipboard watcher\"\"\"\n    from aider.io import InputOutput\n\n    io = InputOutput()\n    watcher = ClipboardWatcher(io, verbose=True)\n\n    try:\n        watcher.start()\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        print(\"\\nStopped watching clipboard\")\n        watcher.stop()\n\n\nif __name__ == \"__main__\":\n    main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/diffs.py::1",
    "metadata": {
      "file_path": "aider/diffs.py",
      "file_name": "diffs.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 250,
      "span_ids": [
        "assert_newlines",
        "imports",
        "main",
        "create_progress_bar"
      ],
      "start_line": 1,
      "end_line": 40,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import difflib\nimport sys\n\nfrom .dump import dump  # noqa: F401\n\n\ndef main():\n    if len(sys.argv) != 3:\n        print(\"Usage: python diffs.py file1 file\")\n        sys.exit(1)\n\n    file_orig, file_updated = sys.argv[1], sys.argv[2]\n\n    with open(file_orig, \"r\", encoding=\"utf-8\") as f:\n        lines_orig = f.readlines()\n\n    with open(file_updated, \"r\", encoding=\"utf-8\") as f:\n        lines_updated = f.readlines()\n\n    for i in range(len(file_updated)):\n        res = diff_partial_update(lines_orig, lines_updated[:i])\n        print(res)\n        input()\n\n\ndef create_progress_bar(percentage):\n    block = \"\u2588\"\n    empty = \"\u2591\"\n    total_blocks = 30\n    filled_blocks = int(total_blocks * percentage // 100)\n    empty_blocks = total_blocks - filled_blocks\n    bar = block * filled_blocks + empty * empty_blocks\n    return bar\n\n\ndef assert_newlines(lines):\n    if not lines:\n        return\n    for line in lines[:-1]:\n        assert line and line[-1] == \"\\n\", line",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/diffs.py::2",
    "metadata": {
      "file_path": "aider/diffs.py",
      "file_name": "diffs.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 384,
      "span_ids": [
        "diff_partial_update"
      ],
      "start_line": 43,
      "end_line": 102,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def diff_partial_update(lines_orig, lines_updated, final=False, fname=None):\n    \"\"\"\n    Given only the first part of an updated file, show the diff while\n    ignoring the block of \"deleted\" lines that are past the end of the\n    partially complete update.\n    \"\"\"\n\n    # dump(lines_orig)\n    # dump(lines_updated)\n\n    assert_newlines(lines_orig)\n\n    num_orig_lines = len(lines_orig)\n\n    if final:\n        last_non_deleted = num_orig_lines\n    else:\n        last_non_deleted = find_last_non_deleted(lines_orig, lines_updated)\n\n    # dump(last_non_deleted)\n    if last_non_deleted is None:\n        return \"\"\n\n    if num_orig_lines:\n        pct = last_non_deleted * 100 / num_orig_lines\n    else:\n        pct = 50\n    bar = create_progress_bar(pct)\n    bar = f\" {last_non_deleted:3d} / {num_orig_lines:3d} lines [{bar}] {pct:3.0f}%\\n\"\n\n    lines_orig = lines_orig[:last_non_deleted]\n\n    if not final:\n        lines_updated = lines_updated[:-1] + [bar]\n\n    diff = difflib.unified_diff(lines_orig, lines_updated, n=5)\n\n    diff = list(diff)[2:]\n\n    diff = \"\".join(diff)\n    if not diff.endswith(\"\\n\"):\n        diff += \"\\n\"\n\n    for i in range(3, 10):\n        backticks = \"`\" * i\n        if backticks not in diff:\n            break\n\n    show = f\"{backticks}diff\\n\"\n    if fname:\n        show += f\"--- {fname} original\\n\"\n        show += f\"+++ {fname} updated\\n\"\n\n    show += diff\n\n    show += f\"{backticks}\\n\\n\"\n\n    # print(diff)\n\n    return show",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/diffs.py::3",
    "metadata": {
      "file_path": "aider/diffs.py",
      "file_name": "diffs.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 156,
      "span_ids": [
        "impl",
        "find_last_non_deleted"
      ],
      "start_line": 105,
      "end_line": 129,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def find_last_non_deleted(lines_orig, lines_updated):\n    diff = list(difflib.ndiff(lines_orig, lines_updated))\n\n    num_orig = 0\n    last_non_deleted_orig = None\n\n    for line in diff:\n        # print(f\"{num_orig:2d} {num_updated:2d} {line}\", end=\"\")\n        code = line[0]\n        if code == \" \":\n            num_orig += 1\n            last_non_deleted_orig = num_orig\n        elif code == \"-\":\n            # line only in orig\n            num_orig += 1\n        elif code == \"+\":\n            # line only in updated\n            pass\n\n    return last_non_deleted_orig\n\n\nif __name__ == \"__main__\":\n    main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/dump.py::1",
    "metadata": {
      "file_path": "aider/dump.py",
      "file_name": "dump.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 181,
      "span_ids": [
        "imports",
        "cvt",
        "dump"
      ],
      "start_line": 1,
      "end_line": 30,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import json\nimport traceback\n\n\ndef cvt(s):\n    if isinstance(s, str):\n        return s\n    try:\n        return json.dumps(s, indent=4)\n    except TypeError:\n        return str(s)\n\n\ndef dump(*vals):\n    # http://docs.python.org/library/traceback.html\n    stack = traceback.extract_stack()\n    vars = stack[-2][3]\n\n    # strip away the call to dump()\n    vars = \"(\".join(vars.split(\"(\")[1:])\n    vars = \")\".join(vars.split(\")\")[:-1])\n\n    vals = [cvt(v) for v in vals]\n    has_newline = sum(1 for v in vals if \"\\n\" in v)\n    if has_newline:\n        print(\"%s:\" % vars)\n        print(\", \".join(vals))\n    else:\n        print(\"%s:\" % vars, \", \".join(vals))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/editor.py::1",
    "metadata": {
      "file_path": "aider/editor.py",
      "file_name": "editor.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 200,
      "span_ids": [
        "docstring",
        "print_status_message"
      ],
      "start_line": 1,
      "end_line": 37,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "\"\"\"\nEditor module for handling system text editor interactions.\n\nThis module provides functionality to:\n- Discover and launch the system's configured text editor\n- Create and manage temporary files for editing\n- Handle editor preferences from environment variables\n- Support cross-platform editor operations\n\"\"\"\n\nimport os\nimport platform\nimport shlex\nimport subprocess\nimport tempfile\n\nfrom rich.console import Console\n\nDEFAULT_EDITOR_NIX = \"vi\"\nDEFAULT_EDITOR_OS_X = \"vim\"\nDEFAULT_EDITOR_WINDOWS = \"notepad\"\n\nconsole = Console()\n\n\ndef print_status_message(success, message, style=None):\n    \"\"\"\n    Print a status message with appropriate styling.\n\n    :param success: Whether the operation was successful\n    :param message: The message to display\n    :param style: Optional style override. If None, uses green for success and red for failure\n    \"\"\"\n    if style is None:\n        style = \"bold green\" if success else \"bold red\"\n    console.print(message, style=style)\n    print(\"\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/editor.py::2",
    "metadata": {
      "file_path": "aider/editor.py",
      "file_name": "editor.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 190,
      "span_ids": [
        "write_temp_file"
      ],
      "start_line": 40,
      "end_line": 66,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def write_temp_file(\n    input_data=\"\",\n    suffix=None,\n    prefix=None,\n    dir=None,\n):\n    \"\"\"\n    Create a temporary file with the given input data.\n\n    :param input_data: Content to write to the temporary file\n    :param suffix: Optional file extension (without the dot)\n    :param prefix: Optional prefix for the temporary filename\n    :param dir: Optional directory to create the file in\n    :return: Path to the created temporary file\n    :raises: OSError if file creation or writing fails\n    \"\"\"\n    kwargs = {\"prefix\": prefix, \"dir\": dir}\n    if suffix:\n        kwargs[\"suffix\"] = f\".{suffix}\"\n    fd, filepath = tempfile.mkstemp(**kwargs)\n    try:\n        with os.fdopen(fd, \"w\") as f:\n            f.write(input_data)\n    except Exception:\n        os.close(fd)\n        raise\n    return filepath",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/editor.py::3",
    "metadata": {
      "file_path": "aider/editor.py",
      "file_name": "editor.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 124,
      "span_ids": [
        "get_environment_editor"
      ],
      "start_line": 69,
      "end_line": 85,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_environment_editor(default=None):\n    \"\"\"\n    Fetches the preferred editor from the environment variables.\n\n    This function checks the following environment variables in order to\n    determine the user's preferred editor:\n\n     - VISUAL\n     - EDITOR\n\n    :param default: The default editor to return if no environment variable is set.\n    :type default: str or None\n    :return: The preferred editor as specified by environment variables or the default value.\n    :rtype: str or None\n    \"\"\"\n    editor = os.environ.get(\"VISUAL\", os.environ.get(\"EDITOR\", default))\n    return editor",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/editor.py::4",
    "metadata": {
      "file_path": "aider/editor.py",
      "file_name": "editor.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 185,
      "span_ids": [
        "discover_editor"
      ],
      "start_line": 88,
      "end_line": 112,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def discover_editor(editor_override=None):\n    \"\"\"\n    Discovers and returns the appropriate editor command as a list of arguments.\n\n    Handles cases where the editor command includes arguments, including quoted arguments\n    with spaces (e.g. 'vim -c \"set noswapfile\"').\n\n    :return: A list of command parts ready for subprocess execution\n    :rtype: list[str]\n    \"\"\"\n    system = platform.system()\n    if system == \"Windows\":\n        default_editor = DEFAULT_EDITOR_WINDOWS\n    elif system == \"Darwin\":\n        default_editor = DEFAULT_EDITOR_OS_X\n    else:\n        default_editor = DEFAULT_EDITOR_NIX\n    if editor_override:\n        editor = editor_override\n    else:\n        editor = get_environment_editor(default_editor)\n    try:\n        return shlex.split(editor)\n    except ValueError as e:\n        raise RuntimeError(f\"Invalid editor command format '{editor}': {e}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/editor.py::5",
    "metadata": {
      "file_path": "aider/editor.py",
      "file_name": "editor.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 260,
      "span_ids": [
        "pipe_editor"
      ],
      "start_line": 115,
      "end_line": 147,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def pipe_editor(input_data=\"\", suffix=None, editor=None):\n    \"\"\"\n    Opens the system editor with optional input data and returns the edited content.\n\n    This function creates a temporary file with the provided input data, opens it in\n    the system editor, waits for the user to make changes and close the editor, then\n    reads and returns the modified content. The temporary file is deleted afterwards.\n\n    :param input_data: Initial content to populate the editor with\n    :type input_data: str\n    :param suffix: Optional file extension for the temporary file (e.g. '.txt', '.md')\n    :type suffix: str or None\n    :return: The edited content after the editor is closed\n    :rtype: str\n    \"\"\"\n    filepath = write_temp_file(input_data, suffix)\n    command_parts = discover_editor(editor)\n    command_parts.append(filepath)\n    subprocess.call(command_parts)\n    with open(filepath, \"r\") as f:\n        output_data = f.read()\n    try:\n        os.remove(filepath)\n    except PermissionError:\n        print_status_message(\n            False,\n            (\n                f\"WARNING: Unable to delete temporary file {filepath!r}. You may need to delete it\"\n                \" manually.\"\n            ),\n        )\n    return output_data",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/exceptions.py::1",
    "metadata": {
      "file_path": "aider/exceptions.py",
      "file_name": "exceptions.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 388,
      "span_ids": [
        "imports",
        "ExInfo",
        "impl"
      ],
      "start_line": 1,
      "end_line": 48,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from dataclasses import dataclass\n\n\n@dataclass\nclass ExInfo:\n    name: str\n    retry: bool\n    description: str\n\n\nEXCEPTIONS = [\n    ExInfo(\"APIConnectionError\", True, None),\n    ExInfo(\"APIError\", True, None),\n    ExInfo(\"APIResponseValidationError\", True, None),\n    ExInfo(\n        \"AuthenticationError\",\n        False,\n        \"The API provider is not able to authenticate you. Check your API key.\",\n    ),\n    ExInfo(\"AzureOpenAIError\", True, None),\n    ExInfo(\"BadRequestError\", False, None),\n    ExInfo(\"BudgetExceededError\", True, None),\n    ExInfo(\n        \"ContentPolicyViolationError\",\n        True,\n        \"The API provider has refused the request due to a safety policy about the content.\",\n    ),\n    ExInfo(\"ContextWindowExceededError\", False, None),  # special case handled in base_coder\n    ExInfo(\"InternalServerError\", True, \"The API provider's servers are down or overloaded.\"),\n    ExInfo(\"InvalidRequestError\", True, None),\n    ExInfo(\"JSONSchemaValidationError\", True, None),\n    ExInfo(\"NotFoundError\", False, None),\n    ExInfo(\"OpenAIError\", True, None),\n    ExInfo(\n        \"RateLimitError\",\n        True,\n        \"The API provider has rate limited you. Try again later or check your quotas.\",\n    ),\n    ExInfo(\"RouterRateLimitError\", True, None),\n    ExInfo(\"ServiceUnavailableError\", True, \"The API provider's servers are down or overloaded.\"),\n    ExInfo(\"UnprocessableEntityError\", True, None),\n    ExInfo(\"UnsupportedParamsError\", True, None),\n    ExInfo(\n        \"Timeout\",\n        True,\n        \"The API provider timed out without returning a response. They may be down or overloaded.\",\n    ),\n]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/exceptions.py::2",
    "metadata": {
      "file_path": "aider/exceptions.py",
      "file_name": "exceptions.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 152,
      "span_ids": [
        "LiteLLMExceptions._load",
        "LiteLLMExceptions",
        "LiteLLMExceptions.__init__",
        "LiteLLMExceptions.exceptions_tuple"
      ],
      "start_line": 51,
      "end_line": 77,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class LiteLLMExceptions:\n    exceptions = dict()\n\n    def __init__(self):\n        self._load()\n\n    def _load(self, strict=False):\n        import litellm\n\n        for var in dir(litellm):\n            if not var.endswith(\"Error\"):\n                continue\n\n            ex_info = None\n            for exi in EXCEPTIONS:\n                if var == exi.name:\n                    ex_info = exi\n                    break\n\n            if strict and not ex_info:\n                raise ValueError(f\"{var} is in litellm but not in aider's exceptions list\")\n\n            ex = getattr(litellm, var)\n            self.exceptions[ex] = ex_info\n\n    def exceptions_tuple(self):\n        return tuple(self.exceptions)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/exceptions.py::3",
    "metadata": {
      "file_path": "aider/exceptions.py",
      "file_name": "exceptions.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 133,
      "span_ids": [
        "LiteLLMExceptions.get_ex_info"
      ],
      "start_line": 79,
      "end_line": 91,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class LiteLLMExceptions:\n\n    def get_ex_info(self, ex):\n        \"\"\"Return the ExInfo for a given exception instance\"\"\"\n        import litellm\n\n        if ex.__class__ is litellm.APIConnectionError:\n            if \"google.auth\" in str(ex):\n                return ExInfo(\n                    \"APIConnectionError\", False, \"You need to: pip install google-generativeai\"\n                )\n            if \"boto3\" in str(ex):\n                return ExInfo(\"APIConnectionError\", False, \"You need to: pip install boto3\")\n        return self.exceptions.get(ex.__class__, ExInfo(None, None, None))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/format_settings.py::1",
    "metadata": {
      "file_path": "aider/format_settings.py",
      "file_name": "format_settings.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 261,
      "span_ids": [
        "scrub_sensitive_info",
        "format_settings"
      ],
      "start_line": 1,
      "end_line": 27,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def scrub_sensitive_info(args, text):\n    # Replace sensitive information with last 4 characters\n    if text and args.openai_api_key:\n        last_4 = args.openai_api_key[-4:]\n        text = text.replace(args.openai_api_key, f\"...{last_4}\")\n    if text and args.anthropic_api_key:\n        last_4 = args.anthropic_api_key[-4:]\n        text = text.replace(args.anthropic_api_key, f\"...{last_4}\")\n    return text\n\n\ndef format_settings(parser, args):\n    show = scrub_sensitive_info(args, parser.format_values())\n    # clean up the headings for consistency w/ new lines\n    heading_env = \"Environment Variables:\"\n    heading_defaults = \"Defaults:\"\n    if heading_env in show:\n        show = show.replace(heading_env, \"\\n\" + heading_env)\n        show = show.replace(heading_defaults, \"\\n\" + heading_defaults)\n    show += \"\\n\"\n    show += \"Option settings:\\n\"\n    for arg, val in sorted(vars(args).items()):\n        if val:\n            val = scrub_sensitive_info(args, str(val))\n        show += f\"  - {arg}: {val}\\n\"  # noqa: E221\n    return show",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/gui.py::1",
    "metadata": {
      "file_path": "aider/gui.py",
      "file_name": "gui.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 321,
      "span_ids": [
        "CaptureIO",
        "CaptureIO.get_captured_lines",
        "State",
        "CaptureIO.tool_error",
        "search",
        "State.init",
        "get_state",
        "docstring",
        "CaptureIO.tool_warning",
        "CaptureIO.tool_output"
      ],
      "start_line": 1,
      "end_line": 66,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "#!/usr/bin/env python\n\nimport os\nimport random\nimport sys\n\nimport streamlit as st\n\nfrom aider import urls\nfrom aider.coders import Coder\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import InputOutput\nfrom aider.main import main as cli_main\nfrom aider.scrape import Scraper\n\n\nclass CaptureIO(InputOutput):\n    lines = []\n\n    def tool_output(self, msg, log_only=False):\n        if not log_only:\n            self.lines.append(msg)\n        super().tool_output(msg, log_only=log_only)\n\n    def tool_error(self, msg):\n        self.lines.append(msg)\n        super().tool_error(msg)\n\n    def tool_warning(self, msg):\n        self.lines.append(msg)\n        super().tool_warning(msg)\n\n    def get_captured_lines(self):\n        lines = self.lines\n        self.lines = []\n        return lines\n\n\ndef search(text=None):\n    results = []\n    for root, _, files in os.walk(\"aider\"):\n        for file in files:\n            path = os.path.join(root, file)\n            if not text or text in path:\n                results.append(path)\n    # dump(results)\n\n    return results\n\n\n# Keep state as a resource, which survives browser reloads (since Coder does too)\nclass State:\n    keys = set()\n\n    def init(self, key, val=None):\n        if key in self.keys:\n            return\n\n        self.keys.add(key)\n        setattr(self, key, val)\n        return True\n\n\n@st.cache_resource\ndef get_state():\n    return State()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/gui.py::2",
    "metadata": {
      "file_path": "aider/gui.py",
      "file_name": "gui.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 127,
      "span_ids": [
        "get_coder"
      ],
      "start_line": 69,
      "end_line": 89,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "@st.cache_resource\ndef get_coder():\n    coder = cli_main(return_coder=True)\n    if not isinstance(coder, Coder):\n        raise ValueError(coder)\n    if not coder.repo:\n        raise ValueError(\"GUI can currently only be used inside a git repo\")\n\n    io = CaptureIO(\n        pretty=False,\n        yes=True,\n        dry_run=coder.io.dry_run,\n        encoding=coder.io.encoding,\n    )\n    # coder.io = io # this breaks the input_history\n    coder.commands.io = io\n\n    for line in coder.get_announcements():\n        coder.io.tool_output(line)\n\n    return coder",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/gui.py::3",
    "metadata": {
      "file_path": "aider/gui.py",
      "file_name": "gui.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 297,
      "span_ids": [
        "GUI",
        "GUI.show_edit_info",
        "GUI.announce"
      ],
      "start_line": 92,
      "end_line": 136,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GUI:\n    prompt = None\n    prompt_as = \"user\"\n    last_undo_empty = None\n    recent_msgs_empty = None\n    web_content_empty = None\n\n    def announce(self):\n        lines = self.coder.get_announcements()\n        lines = \"  \\n\".join(lines)\n        return lines\n\n    def show_edit_info(self, edit):\n        commit_hash = edit.get(\"commit_hash\")\n        commit_message = edit.get(\"commit_message\")\n        diff = edit.get(\"diff\")\n        fnames = edit.get(\"fnames\")\n        if fnames:\n            fnames = sorted(fnames)\n\n        if not commit_hash and not fnames:\n            return\n\n        show_undo = False\n        res = \"\"\n        if commit_hash:\n            res += f\"Commit `{commit_hash}`: {commit_message}  \\n\"\n            if commit_hash == self.coder.last_aider_commit_hash:\n                show_undo = True\n\n        if fnames:\n            fnames = [f\"`{fname}`\" for fname in fnames]\n            fnames = \", \".join(fnames)\n            res += f\"Applied edits to {fnames}.\"\n\n        if diff:\n            with st.expander(res):\n                st.code(diff, language=\"diff\")\n                if show_undo:\n                    self.add_undo(commit_hash)\n        else:\n            with st.container(border=True):\n                st.write(res)\n                if show_undo:\n                    self.add_undo(commit_hash)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/gui.py::4",
    "metadata": {
      "file_path": "aider/gui.py",
      "file_name": "gui.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 217,
      "span_ids": [
        "GUI.add_undo",
        "GUI.do_sidebar"
      ],
      "start_line": 138,
      "end_line": 164,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GUI:\n\n    def add_undo(self, commit_hash):\n        if self.last_undo_empty:\n            self.last_undo_empty.empty()\n\n        self.last_undo_empty = st.empty()\n        undone = self.state.last_undone_commit_hash == commit_hash\n        if not undone:\n            with self.last_undo_empty:\n                if self.button(f\"Undo commit `{commit_hash}`\", key=f\"undo_{commit_hash}\"):\n                    self.do_undo(commit_hash)\n\n    def do_sidebar(self):\n        with st.sidebar:\n            st.title(\"Aider\")\n            # self.cmds_tab, self.settings_tab = st.tabs([\"Commands\", \"Settings\"])\n\n            # self.do_recommended_actions()\n            self.do_add_to_chat()\n            self.do_recent_msgs()\n            self.do_clear_chat_history()\n            # st.container(height=150, border=False)\n            # st.write(\"### Experimental\")\n\n            st.warning(\n                \"This browser version of aider is experimental. Please share feedback in [GitHub\"\n                \" issues](https://github.com/Aider-AI/aider/issues).\"\n            )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/gui.py::5",
    "metadata": {
      "file_path": "aider/gui.py",
      "file_name": "gui.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 198,
      "span_ids": [
        "GUI.do_recommended_actions",
        "GUI.do_add_to_chat",
        "GUI.do_settings_tab"
      ],
      "start_line": 166,
      "end_line": 185,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GUI:\n\n    def do_settings_tab(self):\n        pass\n\n    def do_recommended_actions(self):\n        text = \"Aider works best when your code is stored in a git repo.  \\n\"\n        text += f\"[See the FAQ for more info]({urls.git})\"\n\n        with st.expander(\"Recommended actions\", expanded=True):\n            with st.popover(\"Create a git repo to track changes\"):\n                st.write(text)\n                self.button(\"Create git repo\", key=random.random(), help=\"?\")\n\n            with st.popover(\"Update your `.gitignore` file\"):\n                st.write(\"It's best to keep aider's internal files out of your git repo.\")\n                self.button(\"Add `.aider*` to `.gitignore`\", key=random.random(), help=\"?\")\n\n    def do_add_to_chat(self):\n        # with st.expander(\"Add to the chat\", expanded=True):\n        self.do_add_files()\n        self.do_add_web_page()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/gui.py::6",
    "metadata": {
      "file_path": "aider/gui.py",
      "file_name": "gui.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 191,
      "span_ids": [
        "GUI.do_add_files"
      ],
      "start_line": 187,
      "end_line": 208,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GUI:\n\n    def do_add_files(self):\n        fnames = st.multiselect(\n            \"Add files to the chat\",\n            self.coder.get_all_relative_files(),\n            default=self.state.initial_inchat_files,\n            placeholder=\"Files to edit\",\n            disabled=self.prompt_pending(),\n            help=(\n                \"Only add the files that need to be *edited* for the task you are working\"\n                \" on. Aider will pull in other relevant code to provide context to the LLM.\"\n            ),\n        )\n\n        for fname in fnames:\n            if fname not in self.coder.get_inchat_relative_files():\n                self.coder.add_rel_fname(fname)\n                self.info(f\"Added {fname} to the chat\")\n\n        for fname in self.coder.get_inchat_relative_files():\n            if fname not in fnames:\n                self.coder.drop_rel_fname(fname)\n                self.info(f\"Removed {fname} from the chat\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/gui.py::7",
    "metadata": {
      "file_path": "aider/gui.py",
      "file_name": "gui.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 236,
      "span_ids": [
        "GUI.do_run_shell",
        "GUI.do_add_web_page",
        "GUI.do_add_image"
      ],
      "start_line": 210,
      "end_line": 240,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GUI:\n\n    def do_add_web_page(self):\n        with st.popover(\"Add a web page to the chat\"):\n            self.do_web()\n\n    def do_add_image(self):\n        with st.popover(\"Add image\"):\n            st.markdown(\"Hello World \ud83d\udc4b\")\n            st.file_uploader(\"Image file\", disabled=self.prompt_pending())\n\n    def do_run_shell(self):\n        with st.popover(\"Run shell commands, tests, etc\"):\n            st.markdown(\n                \"Run a shell command and optionally share the output with the LLM. This is\"\n                \" a great way to run your program or run tests and have the LLM fix bugs.\"\n            )\n            st.text_input(\"Command:\")\n            st.radio(\n                \"Share the command output with the LLM?\",\n                [\n                    \"Review the output and decide whether to share\",\n                    \"Automatically share the output on non-zero exit code (ie, if any tests fail)\",\n                ],\n            )\n            st.selectbox(\n                \"Recent commands\",\n                [\n                    \"my_app.py --doit\",\n                    \"my_app.py --cleanup\",\n                ],\n                disabled=self.prompt_pending(),\n            )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/gui.py::8",
    "metadata": {
      "file_path": "aider/gui.py",
      "file_name": "gui.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 182,
      "span_ids": [
        "GUI.do_tokens_and_cost",
        "GUI.do_show_metrics",
        "GUI.do_clear_chat_history",
        "GUI.do_show_token_usage"
      ],
      "start_line": 242,
      "end_line": 260,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GUI:\n\n    def do_tokens_and_cost(self):\n        with st.expander(\"Tokens and costs\", expanded=True):\n            pass\n\n    def do_show_token_usage(self):\n        with st.popover(\"Show token usage\"):\n            st.write(\"hi\")\n\n    def do_clear_chat_history(self):\n        text = \"Saves tokens, reduces confusion\"\n        if self.button(\"Clear chat history\", help=text):\n            self.coder.done_messages = []\n            self.coder.cur_messages = []\n            self.info(\"Cleared chat history. Now the LLM can't see anything before this line.\")\n\n    def do_show_metrics(self):\n        st.metric(\"Cost of last message send & reply\", \"$0.0019\", help=\"foo\")\n        st.metric(\"Cost to send next message\", \"$0.0013\", help=\"foo\")\n        st.metric(\"Total cost this session\", \"$0.22\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/gui.py::9",
    "metadata": {
      "file_path": "aider/gui.py",
      "file_name": "gui.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 120,
      "span_ids": [
        "GUI.do_git"
      ],
      "start_line": 262,
      "end_line": 278,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GUI:\n\n    def do_git(self):\n        with st.expander(\"Git\", expanded=False):\n            # st.button(\"Show last diff\")\n            # st.button(\"Undo last commit\")\n            self.button(\"Commit any pending changes\")\n            with st.popover(\"Run git command\"):\n                st.markdown(\"## Run git command\")\n                st.text_input(\"git\", value=\"git \")\n                self.button(\"Run\")\n                st.selectbox(\n                    \"Recent git commands\",\n                    [\n                        \"git checkout -b experiment\",\n                        \"git stash\",\n                    ],\n                    disabled=self.prompt_pending(),\n                )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/gui.py::10",
    "metadata": {
      "file_path": "aider/gui.py",
      "file_name": "gui.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 142,
      "span_ids": [
        "GUI.do_recent_msgs"
      ],
      "start_line": 280,
      "end_line": 299,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GUI:\n\n    def do_recent_msgs(self):\n        if not self.recent_msgs_empty:\n            self.recent_msgs_empty = st.empty()\n\n        if self.prompt_pending():\n            self.recent_msgs_empty.empty()\n            self.state.recent_msgs_num += 1\n\n        with self.recent_msgs_empty:\n            self.old_prompt = st.selectbox(\n                \"Resend a recent chat message\",\n                self.state.input_history,\n                placeholder=\"Choose a recent chat message\",\n                # label_visibility=\"collapsed\",\n                index=None,\n                key=f\"recent_msgs_{self.state.recent_msgs_num}\",\n                disabled=self.prompt_pending(),\n            )\n            if self.old_prompt:\n                self.prompt = self.old_prompt",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/gui.py::11",
    "metadata": {
      "file_path": "aider/gui.py",
      "file_name": "gui.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 175,
      "span_ids": [
        "GUI.do_messages_container"
      ],
      "start_line": 301,
      "end_line": 326,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GUI:\n\n    def do_messages_container(self):\n        self.messages = st.container()\n\n        # stuff a bunch of vertical whitespace at the top\n        # to get all the chat text to the bottom\n        # self.messages.container(height=300, border=False)\n\n        with self.messages:\n            for msg in self.state.messages:\n                role = msg[\"role\"]\n\n                if role == \"edit\":\n                    self.show_edit_info(msg)\n                elif role == \"info\":\n                    st.info(msg[\"content\"])\n                elif role == \"text\":\n                    text = msg[\"content\"]\n                    line = text.splitlines()[0]\n                    with self.messages.expander(line):\n                        st.text(text)\n                elif role in (\"user\", \"assistant\"):\n                    with st.chat_message(role):\n                        st.write(msg[\"content\"])\n                        # self.cost()\n                else:\n                    st.dict(msg)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/gui.py::12",
    "metadata": {
      "file_path": "aider/gui.py",
      "file_name": "gui.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 211,
      "span_ids": [
        "GUI.initialize_state"
      ],
      "start_line": 328,
      "end_line": 349,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GUI:\n\n    def initialize_state(self):\n        messages = [\n            dict(role=\"info\", content=self.announce()),\n            dict(role=\"assistant\", content=\"How can I help you?\"),\n        ]\n\n        self.state.init(\"messages\", messages)\n        self.state.init(\"last_aider_commit_hash\", self.coder.last_aider_commit_hash)\n        self.state.init(\"last_undone_commit_hash\")\n        self.state.init(\"recent_msgs_num\", 0)\n        self.state.init(\"web_content_num\", 0)\n        self.state.init(\"prompt\")\n        self.state.init(\"scraper\")\n\n        self.state.init(\"initial_inchat_files\", self.coder.get_inchat_relative_files())\n\n        if \"input_history\" not in self.state.keys:\n            input_history = list(self.coder.io.get_input_history())\n            seen = set()\n            input_history = [x for x in input_history if not (x in seen or seen.add(x))]\n            self.state.input_history = input_history\n            self.state.keys.add(\"input_history\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/gui.py::13",
    "metadata": {
      "file_path": "aider/gui.py",
      "file_name": "gui.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 375,
      "span_ids": [
        "GUI.button",
        "GUI.cost",
        "GUI.prompt_pending",
        "GUI.__init__"
      ],
      "start_line": 351,
      "end_line": 410,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GUI:\n\n    def button(self, args, **kwargs):\n        \"Create a button, disabled if prompt pending\"\n\n        # Force everything to be disabled if there is a prompt pending\n        if self.prompt_pending():\n            kwargs[\"disabled\"] = True\n\n        return st.button(args, **kwargs)\n\n    def __init__(self):\n        self.coder = get_coder()\n        self.state = get_state()\n\n        # Force the coder to cooperate, regardless of cmd line args\n        self.coder.yield_stream = True\n        self.coder.stream = True\n        self.coder.pretty = False\n\n        self.initialize_state()\n\n        self.do_messages_container()\n        self.do_sidebar()\n\n        user_inp = st.chat_input(\"Say something\")\n        if user_inp:\n            self.prompt = user_inp\n\n        if self.prompt_pending():\n            self.process_chat()\n\n        if not self.prompt:\n            return\n\n        self.state.prompt = self.prompt\n\n        if self.prompt_as == \"user\":\n            self.coder.io.add_to_input_history(self.prompt)\n\n        self.state.input_history.append(self.prompt)\n\n        if self.prompt_as:\n            self.state.messages.append({\"role\": self.prompt_as, \"content\": self.prompt})\n        if self.prompt_as == \"user\":\n            with self.messages.chat_message(\"user\"):\n                st.write(self.prompt)\n        elif self.prompt_as == \"text\":\n            line = self.prompt.splitlines()[0]\n            line += \"??\"\n            with self.messages.expander(line):\n                st.text(self.prompt)\n\n        # re-render the UI for the prompt_pending state\n        st.rerun()\n\n    def prompt_pending(self):\n        return self.state.prompt is not None\n\n    def cost(self):\n        cost = random.random() * 0.003 + 0.001\n        st.caption(f\"${cost:0.4f}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/gui.py::14",
    "metadata": {
      "file_path": "aider/gui.py",
      "file_name": "gui.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 343,
      "span_ids": [
        "GUI.process_chat"
      ],
      "start_line": 412,
      "end_line": 454,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GUI:\n\n    def process_chat(self):\n        prompt = self.state.prompt\n        self.state.prompt = None\n\n        # This duplicates logic from within Coder\n        self.num_reflections = 0\n        self.max_reflections = 3\n\n        while prompt:\n            with self.messages.chat_message(\"assistant\"):\n                res = st.write_stream(self.coder.run_stream(prompt))\n                self.state.messages.append({\"role\": \"assistant\", \"content\": res})\n                # self.cost()\n\n            prompt = None\n            if self.coder.reflected_message:\n                if self.num_reflections < self.max_reflections:\n                    self.num_reflections += 1\n                    self.info(self.coder.reflected_message)\n                    prompt = self.coder.reflected_message\n\n        with self.messages:\n            edit = dict(\n                role=\"edit\",\n                fnames=self.coder.aider_edited_files,\n            )\n            if self.state.last_aider_commit_hash != self.coder.last_aider_commit_hash:\n                edit[\"commit_hash\"] = self.coder.last_aider_commit_hash\n                edit[\"commit_message\"] = self.coder.last_aider_commit_message\n                commits = f\"{self.coder.last_aider_commit_hash}~1\"\n                diff = self.coder.repo.diff_commits(\n                    self.coder.pretty,\n                    commits,\n                    self.coder.last_aider_commit_hash,\n                )\n                edit[\"diff\"] = diff\n                self.state.last_aider_commit_hash = self.coder.last_aider_commit_hash\n\n            self.state.messages.append(edit)\n            self.show_edit_info(edit)\n\n        # re-render the UI for the non-prompt_pending state\n        st.rerun()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/gui.py::15",
    "metadata": {
      "file_path": "aider/gui.py",
      "file_name": "gui.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 265,
      "span_ids": [
        "GUI.info",
        "GUI.do_web"
      ],
      "start_line": 456,
      "end_line": 496,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GUI:\n\n    def info(self, message, echo=True):\n        info = dict(role=\"info\", content=message)\n        self.state.messages.append(info)\n\n        # We will render the tail of the messages array after this call\n        if echo:\n            self.messages.info(message)\n\n    def do_web(self):\n        st.markdown(\"Add the text content of a web page to the chat\")\n\n        if not self.web_content_empty:\n            self.web_content_empty = st.empty()\n\n        if self.prompt_pending():\n            self.web_content_empty.empty()\n            self.state.web_content_num += 1\n\n        with self.web_content_empty:\n            self.web_content = st.text_input(\n                \"URL\",\n                placeholder=\"https://...\",\n                key=f\"web_content_{self.state.web_content_num}\",\n            )\n\n        if not self.web_content:\n            return\n\n        url = self.web_content\n\n        if not self.state.scraper:\n            self.scraper = Scraper(print_error=self.info)\n\n        content = self.scraper.scrape(url) or \"\"\n        if content.strip():\n            content = f\"{url}\\n\\n\" + content\n            self.prompt = content\n            self.prompt_as = \"text\"\n        else:\n            self.info(f\"No web content found for `{url}`.\")\n            self.web_content = None",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/gui.py::16",
    "metadata": {
      "file_path": "aider/gui.py",
      "file_name": "gui.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 168,
      "span_ids": [
        "GUI.do_undo"
      ],
      "start_line": 498,
      "end_line": 521,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GUI:\n\n    def do_undo(self, commit_hash):\n        self.last_undo_empty.empty()\n\n        if (\n            self.state.last_aider_commit_hash != commit_hash\n            or self.coder.last_aider_commit_hash != commit_hash\n        ):\n            self.info(f\"Commit `{commit_hash}` is not the latest commit.\")\n            return\n\n        self.coder.commands.io.get_captured_lines()\n        reply = self.coder.commands.cmd_undo(None)\n        lines = self.coder.commands.io.get_captured_lines()\n\n        lines = \"\\n\".join(lines)\n        lines = lines.splitlines()\n        lines = \"  \\n\".join(lines)\n        self.info(lines, echo=False)\n\n        self.state.last_undone_commit_hash = commit_hash\n\n        if reply:\n            self.prompt_as = None\n            self.prompt = reply",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/gui.py::17",
    "metadata": {
      "file_path": "aider/gui.py",
      "file_name": "gui.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 138,
      "span_ids": [
        "impl",
        "gui_main"
      ],
      "start_line": 524,
      "end_line": 546,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def gui_main():\n    st.set_page_config(\n        layout=\"wide\",\n        page_title=\"Aider\",\n        page_icon=urls.favicon,\n        menu_items={\n            \"Get Help\": urls.website,\n            \"Report a bug\": \"https://github.com/Aider-AI/aider/issues\",\n            \"About\": \"# Aider\\nAI pair programming in your browser.\",\n        },\n    )\n\n    # config_options = st.config._config_options\n    # for key, value in config_options.items():\n    #    print(f\"{key}: {value.value}\")\n\n    GUI()\n\n\nif __name__ == \"__main__\":\n    status = gui_main()\n    sys.exit(status)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/help.py::1",
    "metadata": {
      "file_path": "aider/help.py",
      "file_name": "help.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 215,
      "span_ids": [
        "install_help_extra",
        "docstring",
        "get_package_files"
      ],
      "start_line": 1,
      "end_line": 39,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "#!/usr/bin/env python\n\nimport json\nimport os\nimport shutil\nimport warnings\nfrom pathlib import Path\n\nimport importlib_resources\n\nfrom aider import __version__, utils\nfrom aider.dump import dump  # noqa: F401\nfrom aider.help_pats import exclude_website_pats\n\nwarnings.simplefilter(\"ignore\", category=FutureWarning)\n\n\ndef install_help_extra(io):\n    pip_install_cmd = [\n        \"aider-chat[help]\",\n        \"--extra-index-url\",\n        \"https://download.pytorch.org/whl/cpu\",\n    ]\n    res = utils.check_pip_install_extra(\n        io,\n        \"llama_index.embeddings.huggingface\",\n        \"To use interactive /help you need to install the help extras\",\n        pip_install_cmd,\n    )\n    return res\n\n\ndef get_package_files():\n    for path in importlib_resources.files(\"aider.website\").iterdir():\n        if path.is_file():\n            yield path\n        elif path.is_dir():\n            for subpath in path.rglob(\"*.md\"):\n                yield subpath",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/help.py::2",
    "metadata": {
      "file_path": "aider/help.py",
      "file_name": "help.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 287,
      "span_ids": [
        "fname_to_url"
      ],
      "start_line": 42,
      "end_line": 81,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def fname_to_url(filepath):\n    website = \"website\"\n    index = \"index.md\"\n    md = \".md\"\n\n    # Convert backslashes to forward slashes for consistency\n    filepath = filepath.replace(\"\\\\\", \"/\")\n\n    # Convert to Path object for easier manipulation\n    path = Path(filepath)\n\n    # Split the path into parts\n    parts = path.parts\n\n    # Find the 'website' part in the path\n    try:\n        website_index = [p.lower() for p in parts].index(website.lower())\n    except ValueError:\n        return \"\"  # 'website' not found in the path\n\n    # Extract the part of the path starting from 'website'\n    relevant_parts = parts[website_index + 1 :]\n\n    # Handle _includes directory\n    if relevant_parts and relevant_parts[0].lower() == \"_includes\":\n        return \"\"\n\n    # Join the remaining parts\n    url_path = \"/\".join(relevant_parts)\n\n    # Handle index.md and other .md files\n    if url_path.lower().endswith(index.lower()):\n        url_path = url_path[: -len(index)]\n    elif url_path.lower().endswith(md.lower()):\n        url_path = url_path[: -len(md)] + \".html\"\n\n    # Ensure the URL starts and ends with '/'\n    url_path = url_path.strip(\"/\")\n\n    return f\"https://aider.chat/{url_path}\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/help.py::3",
    "metadata": {
      "file_path": "aider/help.py",
      "file_name": "help.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 285,
      "span_ids": [
        "get_index"
      ],
      "start_line": 84,
      "end_line": 130,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_index():\n    from llama_index.core import (\n        Document,\n        StorageContext,\n        VectorStoreIndex,\n        load_index_from_storage,\n    )\n    from llama_index.core.node_parser import MarkdownNodeParser\n\n    dname = Path.home() / \".aider\" / \"caches\" / (\"help.\" + __version__)\n\n    index = None\n    try:\n        if dname.exists():\n            storage_context = StorageContext.from_defaults(\n                persist_dir=dname,\n            )\n            index = load_index_from_storage(storage_context)\n    except (OSError, json.JSONDecodeError):\n        shutil.rmtree(dname)\n\n    if index is None:\n        parser = MarkdownNodeParser()\n\n        nodes = []\n        for fname in get_package_files():\n            fname = Path(fname)\n            if any(fname.match(pat) for pat in exclude_website_pats):\n                continue\n\n            doc = Document(\n                text=importlib_resources.files(\"aider.website\")\n                .joinpath(fname)\n                .read_text(encoding=\"utf-8\"),\n                metadata=dict(\n                    filename=fname.name,\n                    extension=fname.suffix,\n                    url=fname_to_url(str(fname)),\n                ),\n            )\n            nodes += parser.get_nodes_from_documents([doc])\n\n        index = VectorStoreIndex(nodes, show_progress=True)\n        dname.parent.mkdir(parents=True, exist_ok=True)\n        index.storage_context.persist(dname)\n\n    return index",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/help.py::4",
    "metadata": {
      "file_path": "aider/help.py",
      "file_name": "help.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 201,
      "span_ids": [
        "Help.ask",
        "Help.__init__",
        "Help"
      ],
      "start_line": 133,
      "end_line": 164,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Help:\n    def __init__(self):\n        from llama_index.core import Settings\n        from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n        Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n\n        index = get_index()\n\n        self.retriever = index.as_retriever(similarity_top_k=20)\n\n    def ask(self, question):\n        nodes = self.retriever.retrieve(question)\n\n        context = f\"\"\"# Question: {question}\n\n# Relevant docs:\n\n\"\"\"  # noqa: E231\n\n        for node in nodes:\n            url = node.metadata.get(\"url\", \"\")\n            if url:\n                url = f' from_url=\"{url}\"'\n\n            context += f\"<doc{url}>\\n\"\n            context += node.text\n            context += \"\\n</doc>\\n\\n\"\n\n        return context",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/help_pats.py::1",
    "metadata": {
      "file_path": "aider/help_pats.py",
      "file_name": "help_pats.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 79,
      "span_ids": [
        "docstring"
      ],
      "start_line": 1,
      "end_line": 14,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# This needs to sync with MANIFEST.in\n\nexclude_website_pats = [\n    \"**/.DS_Store\",\n    \"examples/**\",\n    \"_posts/**\",\n    \"HISTORY.md\",\n    \"docs/benchmarks*md\",\n    \"docs/ctags.md\",\n    \"docs/unified-diffs.md\",\n    \"docs/leaderboards/index.md\",\n    \"assets/**\",\n]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/history.py::1",
    "metadata": {
      "file_path": "aider/history.py",
      "file_name": "history.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 178,
      "span_ids": [
        "ChatSummary",
        "ChatSummary.__init__",
        "ChatSummary.too_big",
        "ChatSummary.tokenize",
        "imports"
      ],
      "start_line": 1,
      "end_line": 26,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import argparse\n\nfrom aider import models, prompts\nfrom aider.dump import dump  # noqa: F401\nfrom aider.sendchat import simple_send_with_retries\n\n\nclass ChatSummary:\n    def __init__(self, models=None, max_tokens=1024):\n        if not models:\n            raise ValueError(\"At least one model must be provided\")\n        self.models = models if isinstance(models, list) else [models]\n        self.max_tokens = max_tokens\n        self.token_count = self.models[0].token_count\n\n    def too_big(self, messages):\n        sized = self.tokenize(messages)\n        total = sum(tokens for tokens, _msg in sized)\n        return total > self.max_tokens\n\n    def tokenize(self, messages):\n        sized = []\n        for msg in messages:\n            tokens = self.token_count(msg)\n            sized.append((tokens, msg))\n        return sized",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/history.py::2",
    "metadata": {
      "file_path": "aider/history.py",
      "file_name": "history.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 445,
      "span_ids": [
        "ChatSummary.summarize"
      ],
      "start_line": 28,
      "end_line": 91,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class ChatSummary:\n\n    def summarize(self, messages, depth=0):\n        if not self.models:\n            raise ValueError(\"No models available for summarization\")\n\n        sized = self.tokenize(messages)\n        total = sum(tokens for tokens, _msg in sized)\n        if total <= self.max_tokens and depth == 0:\n            return messages\n\n        min_split = 4\n        if len(messages) <= min_split or depth > 3:\n            return self.summarize_all(messages)\n\n        tail_tokens = 0\n        split_index = len(messages)\n        half_max_tokens = self.max_tokens // 2\n\n        # Iterate over the messages in reverse order\n        for i in range(len(sized) - 1, -1, -1):\n            tokens, _msg = sized[i]\n            if tail_tokens + tokens < half_max_tokens:\n                tail_tokens += tokens\n                split_index = i\n            else:\n                break\n\n        # Ensure the head ends with an assistant message\n        while messages[split_index - 1][\"role\"] != \"assistant\" and split_index > 1:\n            split_index -= 1\n\n        if split_index <= min_split:\n            return self.summarize_all(messages)\n\n        head = messages[:split_index]\n        tail = messages[split_index:]\n\n        sized = sized[:split_index]\n        head.reverse()\n        sized.reverse()\n        keep = []\n        total = 0\n\n        # These sometimes come set with value = None\n        model_max_input_tokens = self.models[0].info.get(\"max_input_tokens\") or 4096\n        model_max_input_tokens -= 512\n\n        for i in range(split_index):\n            total += sized[i][0]\n            if total > model_max_input_tokens:\n                break\n            keep.append(head[i])\n\n        keep.reverse()\n\n        summary = self.summarize_all(keep)\n\n        tail_tokens = sum(tokens for tokens, msg in sized[split_index:])\n        summary_tokens = self.token_count(summary)\n\n        result = summary + tail\n        if summary_tokens + tail_tokens < self.max_tokens:\n            return result\n\n        return self.summarize(result, depth + 1)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/history.py::3",
    "metadata": {
      "file_path": "aider/history.py",
      "file_name": "history.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 198,
      "span_ids": [
        "ChatSummary.summarize_all"
      ],
      "start_line": 93,
      "end_line": 118,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class ChatSummary:\n\n    def summarize_all(self, messages):\n        content = \"\"\n        for msg in messages:\n            role = msg[\"role\"].upper()\n            if role not in (\"USER\", \"ASSISTANT\"):\n                continue\n            content += f\"# {role}\\n\"\n            content += msg[\"content\"]\n            if not content.endswith(\"\\n\"):\n                content += \"\\n\"\n\n        summarize_messages = [\n            dict(role=\"system\", content=prompts.summarize),\n            dict(role=\"user\", content=content),\n        ]\n\n        for model in self.models:\n            try:\n                summary = simple_send_with_retries(model, summarize_messages)\n                if summary is not None:\n                    summary = prompts.summary_prefix + summary\n                    return [dict(role=\"user\", content=summary)]\n            except Exception as e:\n                print(f\"Summarization failed for model {model.name}: {str(e)}\")\n\n        raise ValueError(\"summarizer unexpectedly failed for all models\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/history.py::4",
    "metadata": {
      "file_path": "aider/history.py",
      "file_name": "history.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 131,
      "span_ids": [
        "impl",
        "main"
      ],
      "start_line": 121,
      "end_line": 139,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"filename\", help=\"Markdown file to parse\")\n    args = parser.parse_args()\n\n    model_names = [\"gpt-3.5-turbo\", \"gpt-4\"]  # Add more model names as needed\n    model_list = [models.Model(name) for name in model_names]\n    summarizer = ChatSummary(model_list)\n\n    with open(args.filename, \"r\") as f:\n        text = f.read()\n\n    summary = summarizer.summarize_chat_history_markdown(text)\n    dump(summary)\n\n\nif __name__ == \"__main__\":\n    main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::1",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 301,
      "span_ids": [
        "imports",
        "ConfirmGroup.__init__",
        "ConfirmGroup"
      ],
      "start_line": 1,
      "end_line": 44,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import base64\nimport os\nimport signal\nimport time\nimport webbrowser\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom io import StringIO\nfrom pathlib import Path\n\nfrom prompt_toolkit.completion import Completer, Completion, ThreadedCompleter\nfrom prompt_toolkit.cursor_shapes import ModalCursorShapeConfig\nfrom prompt_toolkit.enums import EditingMode\nfrom prompt_toolkit.filters import Condition, is_searching\nfrom prompt_toolkit.history import FileHistory\nfrom prompt_toolkit.key_binding import KeyBindings\nfrom prompt_toolkit.keys import Keys\nfrom prompt_toolkit.lexers import PygmentsLexer\nfrom prompt_toolkit.output.vt100 import is_dumb_terminal\nfrom prompt_toolkit.shortcuts import CompleteStyle, PromptSession\nfrom prompt_toolkit.styles import Style\nfrom pygments.lexers import MarkdownLexer, guess_lexer_for_filename\nfrom pygments.token import Token\nfrom rich.columns import Columns\nfrom rich.console import Console\nfrom rich.markdown import Markdown\nfrom rich.style import Style as RichStyle\nfrom rich.text import Text\n\nfrom aider.mdstream import MarkdownStream\n\nfrom .dump import dump  # noqa: F401\nfrom .utils import is_image_file\n\n\n@dataclass\nclass ConfirmGroup:\n    preference: str = None\n    show_group: bool = True\n\n    def __init__(self, items=None):\n        if items is not None:\n            self.show_group = len(items) > 1",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::2",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 275,
      "span_ids": [
        "AutoCompleter.__init__",
        "AutoCompleter"
      ],
      "start_line": 47,
      "end_line": 81,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class AutoCompleter(Completer):\n    def __init__(\n        self, root, rel_fnames, addable_rel_fnames, commands, encoding, abs_read_only_fnames=None\n    ):\n        self.addable_rel_fnames = addable_rel_fnames\n        self.rel_fnames = rel_fnames\n        self.encoding = encoding\n        self.abs_read_only_fnames = abs_read_only_fnames or []\n\n        fname_to_rel_fnames = defaultdict(list)\n        for rel_fname in addable_rel_fnames:\n            fname = os.path.basename(rel_fname)\n            if fname != rel_fname:\n                fname_to_rel_fnames[fname].append(rel_fname)\n        self.fname_to_rel_fnames = fname_to_rel_fnames\n\n        self.words = set()\n\n        self.commands = commands\n        self.command_completions = dict()\n        if commands:\n            self.command_names = self.commands.get_commands()\n\n        for rel_fname in addable_rel_fnames:\n            self.words.add(rel_fname)\n\n        for rel_fname in rel_fnames:\n            self.words.add(rel_fname)\n\n        all_fnames = [Path(root) / rel_fname for rel_fname in rel_fnames]\n        if abs_read_only_fnames:\n            all_fnames.extend(abs_read_only_fnames)\n\n        self.all_fnames = all_fnames\n        self.tokenized = False",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::3",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 161,
      "span_ids": [
        "AutoCompleter.tokenize"
      ],
      "start_line": 83,
      "end_line": 102,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class AutoCompleter(Completer):\n\n    def tokenize(self):\n        if self.tokenized:\n            return\n        self.tokenized = True\n\n        for fname in self.all_fnames:\n            try:\n                with open(fname, \"r\", encoding=self.encoding) as f:\n                    content = f.read()\n            except (FileNotFoundError, UnicodeDecodeError, IsADirectoryError):\n                continue\n            try:\n                lexer = guess_lexer_for_filename(fname, content)\n            except Exception:  # On Windows, bad ref to time.clock which is deprecated\n                continue\n\n            tokens = list(lexer.get_tokens(content))\n            self.words.update(\n                (token[1], f\"`{token[1]}`\") for token in tokens if token[0] in Token.Name\n            )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::4",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 295,
      "span_ids": [
        "AutoCompleter.get_command_completions"
      ],
      "start_line": 104,
      "end_line": 140,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class AutoCompleter(Completer):\n\n    def get_command_completions(self, document, complete_event, text, words):\n        if len(words) == 1 and not text[-1].isspace():\n            partial = words[0].lower()\n            candidates = [cmd for cmd in self.command_names if cmd.startswith(partial)]\n            for candidate in sorted(candidates):\n                yield Completion(candidate, start_position=-len(words[-1]))\n            return\n\n        if len(words) <= 1 or text[-1].isspace():\n            return\n\n        cmd = words[0]\n        partial = words[-1].lower()\n\n        matches, _, _ = self.commands.matching_commands(cmd)\n        if len(matches) == 1:\n            cmd = matches[0]\n        elif cmd not in matches:\n            return\n\n        raw_completer = self.commands.get_raw_completions(cmd)\n        if raw_completer:\n            yield from raw_completer(document, complete_event)\n            return\n\n        if cmd not in self.command_completions:\n            candidates = self.commands.get_completions(cmd)\n            self.command_completions[cmd] = candidates\n        else:\n            candidates = self.command_completions[cmd]\n\n        if candidates is None:\n            return\n\n        candidates = [word for word in candidates if partial in word.lower()]\n        for candidate in sorted(candidates):\n            yield Completion(candidate, start_position=-len(words[-1]))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::5",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 262,
      "span_ids": [
        "AutoCompleter.get_completions"
      ],
      "start_line": 142,
      "end_line": 174,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class AutoCompleter(Completer):\n\n    def get_completions(self, document, complete_event):\n        self.tokenize()\n\n        text = document.text_before_cursor\n        words = text.split()\n        if not words:\n            return\n\n        if text and text[-1].isspace():\n            # don't keep completing after a space\n            return\n\n        if text[0] == \"/\":\n            yield from self.get_command_completions(document, complete_event, text, words)\n            return\n\n        candidates = self.words\n        candidates.update(set(self.fname_to_rel_fnames))\n        candidates = [word if type(word) is tuple else (word, word) for word in candidates]\n\n        last_word = words[-1]\n        completions = []\n        for word_match, word_insert in candidates:\n            if word_match.lower().startswith(last_word.lower()):\n                completions.append((word_insert, -len(last_word), word_match))\n\n                rel_fnames = self.fname_to_rel_fnames.get(word_match, [])\n                if rel_fnames:\n                    for rel_fname in rel_fnames:\n                        completions.append((rel_fname, -len(last_word), rel_fname))\n\n        for ins, pos, match in sorted(completions):\n            yield Completion(ins, start_position=pos, display=match)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::6",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 927,
      "span_ids": [
        "InputOutput.__init__",
        "InputOutput"
      ],
      "start_line": 177,
      "end_line": 293,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class InputOutput:\n    num_error_outputs = 0\n    num_user_asks = 0\n    clipboard_watcher = None\n\n    def __init__(\n        self,\n        pretty=True,\n        yes=None,\n        input_history_file=None,\n        chat_history_file=None,\n        input=None,\n        output=None,\n        user_input_color=\"blue\",\n        tool_output_color=None,\n        tool_error_color=\"red\",\n        tool_warning_color=\"#FFA500\",\n        assistant_output_color=\"blue\",\n        completion_menu_color=None,\n        completion_menu_bg_color=None,\n        completion_menu_current_color=None,\n        completion_menu_current_bg_color=None,\n        code_theme=\"default\",\n        encoding=\"utf-8\",\n        line_endings=\"platform\",\n        dry_run=False,\n        llm_history_file=None,\n        editingmode=EditingMode.EMACS,\n        fancy_input=True,\n        file_watcher=None,\n        multiline_mode=False,\n        root=\".\",\n    ):\n        self.placeholder = None\n        self.interrupted = False\n        self.never_prompts = set()\n        self.editingmode = editingmode\n        self.multiline_mode = multiline_mode\n        no_color = os.environ.get(\"NO_COLOR\")\n        if no_color is not None and no_color != \"\":\n            pretty = False\n\n        self.user_input_color = user_input_color if pretty else None\n        self.tool_output_color = tool_output_color if pretty else None\n        self.tool_error_color = tool_error_color if pretty else None\n        self.tool_warning_color = tool_warning_color if pretty else None\n        self.assistant_output_color = assistant_output_color\n        self.completion_menu_color = completion_menu_color if pretty else None\n        self.completion_menu_bg_color = completion_menu_bg_color if pretty else None\n        self.completion_menu_current_color = completion_menu_current_color if pretty else None\n        self.completion_menu_current_bg_color = completion_menu_current_bg_color if pretty else None\n\n        self.code_theme = code_theme\n\n        self.input = input\n        self.output = output\n\n        self.pretty = pretty\n        if self.output:\n            self.pretty = False\n\n        self.yes = yes\n\n        self.input_history_file = input_history_file\n        self.llm_history_file = llm_history_file\n        if chat_history_file is not None:\n            self.chat_history_file = Path(chat_history_file)\n        else:\n            self.chat_history_file = None\n\n        self.encoding = encoding\n        valid_line_endings = {\"platform\", \"lf\", \"crlf\"}\n        if line_endings not in valid_line_endings:\n            raise ValueError(\n                f\"Invalid line_endings value: {line_endings}. \"\n                f\"Must be one of: {', '.join(valid_line_endings)}\"\n            )\n        self.newline = (\n            None if line_endings == \"platform\" else \"\\n\" if line_endings == \"lf\" else \"\\r\\n\"\n        )\n        self.dry_run = dry_run\n\n        current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.append_chat_history(f\"\\n# aider chat started at {current_time}\\n\\n\")\n\n        self.prompt_session = None\n        self.is_dumb_terminal = is_dumb_terminal()\n\n        if self.is_dumb_terminal:\n            self.pretty = False\n            fancy_input = False\n\n        if fancy_input:\n            # Initialize PromptSession only if we have a capable terminal\n            session_kwargs = {\n                \"input\": self.input,\n                \"output\": self.output,\n                \"lexer\": PygmentsLexer(MarkdownLexer),\n                \"editing_mode\": self.editingmode,\n            }\n            if self.editingmode == EditingMode.VI:\n                session_kwargs[\"cursor\"] = ModalCursorShapeConfig()\n            if self.input_history_file is not None:\n                session_kwargs[\"history\"] = FileHistory(self.input_history_file)\n            try:\n                self.prompt_session = PromptSession(**session_kwargs)\n                self.console = Console()  # pretty console\n            except Exception as err:\n                self.console = Console(force_terminal=False, no_color=True)\n                self.tool_error(f\"Can't initialize prompt toolkit: {err}\")  # non-pretty\n        else:\n            self.console = Console(force_terminal=False, no_color=True)  # non-pretty\n            if self.is_dumb_terminal:\n                self.tool_output(\"Detected dumb terminal, disabling fancy input and pretty output.\")\n\n        self.file_watcher = file_watcher\n        self.root = root",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::7",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 267,
      "span_ids": [
        "InputOutput._get_style"
      ],
      "start_line": 295,
      "end_line": 328,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class InputOutput:\n\n    def _get_style(self):\n        style_dict = {}\n        if not self.pretty:\n            return Style.from_dict(style_dict)\n\n        if self.user_input_color:\n            style_dict.setdefault(\"\", self.user_input_color)\n            style_dict.update(\n                {\n                    \"pygments.literal.string\": f\"bold italic {self.user_input_color}\",\n                }\n            )\n\n        # Conditionally add 'completion-menu' style\n        completion_menu_style = []\n        if self.completion_menu_bg_color:\n            completion_menu_style.append(f\"bg:{self.completion_menu_bg_color}\")\n        if self.completion_menu_color:\n            completion_menu_style.append(self.completion_menu_color)\n        if completion_menu_style:\n            style_dict[\"completion-menu\"] = \" \".join(completion_menu_style)\n\n        # Conditionally add 'completion-menu.completion.current' style\n        completion_menu_current_style = []\n        if self.completion_menu_current_bg_color:\n            completion_menu_current_style.append(f\"bg:{self.completion_menu_current_bg_color}\")\n        if self.completion_menu_current_color:\n            completion_menu_current_style.append(self.completion_menu_current_color)\n        if completion_menu_current_style:\n            style_dict[\"completion-menu.completion.current\"] = \" \".join(\n                completion_menu_current_style\n            )\n\n        return Style.from_dict(style_dict)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::8",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 136,
      "span_ids": [
        "InputOutput.read_image"
      ],
      "start_line": 330,
      "end_line": 346,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class InputOutput:\n\n    def read_image(self, filename):\n        try:\n            with open(str(filename), \"rb\") as image_file:\n                encoded_string = base64.b64encode(image_file.read())\n                return encoded_string.decode(\"utf-8\")\n        except OSError as err:\n            self.tool_error(f\"{filename}: unable to read: {err}\")\n            return\n        except FileNotFoundError:\n            self.tool_error(f\"{filename}: file not found error\")\n            return\n        except IsADirectoryError:\n            self.tool_error(f\"{filename}: is a directory\")\n            return\n        except Exception as e:\n            self.tool_error(f\"{filename}: {e}\")\n            return",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::9",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 173,
      "span_ids": [
        "InputOutput.read_text"
      ],
      "start_line": 348,
      "end_line": 371,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class InputOutput:\n\n    def read_text(self, filename, silent=False):\n        if is_image_file(filename):\n            return self.read_image(filename)\n\n        try:\n            with open(str(filename), \"r\", encoding=self.encoding) as f:\n                return f.read()\n        except FileNotFoundError:\n            if not silent:\n                self.tool_error(f\"{filename}: file not found error\")\n            return\n        except IsADirectoryError:\n            if not silent:\n                self.tool_error(f\"{filename}: is a directory\")\n            return\n        except OSError as err:\n            if not silent:\n                self.tool_error(f\"{filename}: unable to read: {err}\")\n            return\n        except UnicodeError as e:\n            if not silent:\n                self.tool_error(f\"{filename}: {e}\")\n                self.tool_error(\"Use --encoding to set the unicode encoding.\")\n            return",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::10",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 361,
      "span_ids": [
        "InputOutput.write_text",
        "InputOutput.interrupt_input",
        "InputOutput.rule"
      ],
      "start_line": 373,
      "end_line": 416,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class InputOutput:\n\n    def write_text(self, filename, content, max_retries=5, initial_delay=0.1):\n        \"\"\"\n        Writes content to a file, retrying with progressive backoff if the file is locked.\n\n        :param filename: Path to the file to write.\n        :param content: Content to write to the file.\n        :param max_retries: Maximum number of retries if a file lock is encountered.\n        :param initial_delay: Initial delay (in seconds) before the first retry.\n        \"\"\"\n        if self.dry_run:\n            return\n\n        delay = initial_delay\n        for attempt in range(max_retries):\n            try:\n                with open(str(filename), \"w\", encoding=self.encoding, newline=self.newline) as f:\n                    f.write(content)\n                return  # Successfully wrote the file\n            except PermissionError as err:\n                if attempt < max_retries - 1:\n                    time.sleep(delay)\n                    delay *= 2  # Exponential backoff\n                else:\n                    self.tool_error(\n                        f\"Unable to write file {filename} after {max_retries} attempts: {err}\"\n                    )\n                    raise\n            except OSError as err:\n                self.tool_error(f\"Unable to write file {filename}: {err}\")\n                raise\n\n    def rule(self):\n        if self.pretty:\n            style = dict(style=self.user_input_color) if self.user_input_color else dict()\n            self.console.rule(**style)\n        else:\n            print()\n\n    def interrupt_input(self):\n        if self.prompt_session and self.prompt_session.app:\n            # Store any partial input before interrupting\n            self.placeholder = self.prompt_session.app.current_buffer.text\n            self.interrupted = True\n            self.prompt_session.app.exit()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::11",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 555,
      "span_ids": [
        "InputOutput.get_input"
      ],
      "start_line": 418,
      "end_line": 502,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class InputOutput:\n\n    def get_input(\n        self,\n        root,\n        rel_fnames,\n        addable_rel_fnames,\n        commands,\n        abs_read_only_fnames=None,\n        edit_format=None,\n    ):\n        self.rule()\n\n        rel_fnames = list(rel_fnames)\n        show = \"\"\n        if rel_fnames:\n            rel_read_only_fnames = [\n                get_rel_fname(fname, root) for fname in (abs_read_only_fnames or [])\n            ]\n            show = self.format_files_for_input(rel_fnames, rel_read_only_fnames)\n        if edit_format:\n            show += edit_format\n        if self.multiline_mode:\n            show += (\" \" if edit_format else \"\") + \"multi\"\n        show += \"> \"\n\n        inp = \"\"\n        multiline_input = False\n\n        style = self._get_style()\n\n        completer_instance = ThreadedCompleter(\n            AutoCompleter(\n                root,\n                rel_fnames,\n                addable_rel_fnames,\n                commands,\n                self.encoding,\n                abs_read_only_fnames=abs_read_only_fnames,\n            )\n        )\n\n        def suspend_to_bg(event):\n            \"\"\"Suspend currently running application.\"\"\"\n            event.app.suspend_to_background()\n\n        kb = KeyBindings()\n\n        @kb.add(Keys.ControlZ, filter=Condition(lambda: hasattr(signal, \"SIGTSTP\")))\n        def _(event):\n            \"Suspend to background with ctrl-z\"\n            suspend_to_bg(event)\n\n        @kb.add(\"c-space\")\n        def _(event):\n            \"Ignore Ctrl when pressing space bar\"\n            event.current_buffer.insert_text(\" \")\n\n        @kb.add(\"c-up\")\n        def _(event):\n            \"Navigate backward through history\"\n            event.current_buffer.history_backward()\n\n        @kb.add(\"c-down\")\n        def _(event):\n            \"Navigate forward through history\"\n            event.current_buffer.history_forward()\n\n        @kb.add(\"enter\", eager=True, filter=~is_searching)\n        def _(event):\n            \"Handle Enter key press\"\n            if self.multiline_mode:\n                # In multiline mode, Enter adds a newline\n                event.current_buffer.insert_text(\"\\n\")\n            else:\n                # In normal mode, Enter submits\n                event.current_buffer.validate_and_handle()\n\n        @kb.add(\"escape\", \"enter\", eager=True, filter=~is_searching)  # This is Alt+Enter\n        def _(event):\n            \"Handle Alt+Enter key press\"\n            if self.multiline_mode:\n                # In multiline mode, Alt+Enter submits\n                event.current_buffer.validate_and_handle()\n            else:\n                # In normal mode, Alt+Enter adds a newline\n                event.current_buffer.insert_text(\"\\n\")\n        # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::12",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 599,
      "span_ids": [
        "InputOutput.get_input"
      ],
      "start_line": 504,
      "end_line": 598,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class InputOutput:\n\n    def get_input(\n        self,\n        root,\n        rel_fnames,\n        addable_rel_fnames,\n        commands,\n        abs_read_only_fnames=None,\n        edit_format=None,\n    ):\n        # ... other code\n\n        while True:\n            if multiline_input:\n                show = \". \"\n\n            try:\n                if self.prompt_session:\n                    # Use placeholder if set, then clear it\n                    default = self.placeholder or \"\"\n                    self.placeholder = None\n\n                    self.interrupted = False\n                    if not multiline_input:\n                        if self.file_watcher:\n                            self.file_watcher.start()\n                        if self.clipboard_watcher:\n                            self.clipboard_watcher.start()\n\n                    line = self.prompt_session.prompt(\n                        show,\n                        default=default,\n                        completer=completer_instance,\n                        reserve_space_for_menu=4,\n                        complete_style=CompleteStyle.MULTI_COLUMN,\n                        style=style,\n                        key_bindings=kb,\n                        complete_while_typing=True,\n                    )\n                else:\n                    line = input(show)\n\n                # Check if we were interrupted by a file change\n                if self.interrupted:\n                    line = line or \"\"\n                    if self.file_watcher:\n                        cmd = self.file_watcher.process_changes()\n                        return cmd\n\n            except EOFError:\n                raise\n            except Exception as err:\n                import traceback\n\n                self.tool_error(str(err))\n                self.tool_error(traceback.format_exc())\n                return \"\"\n            except UnicodeEncodeError as err:\n                self.tool_error(str(err))\n                return \"\"\n            finally:\n                if self.file_watcher:\n                    self.file_watcher.stop()\n                if self.clipboard_watcher:\n                    self.clipboard_watcher.stop()\n\n            if line.strip(\"\\r\\n\") and not multiline_input:\n                stripped = line.strip(\"\\r\\n\")\n                if stripped == \"{\":\n                    multiline_input = True\n                    multiline_tag = None\n                    inp += \"\"\n                elif stripped[0] == \"{\":\n                    # Extract tag if it exists (only alphanumeric chars)\n                    tag = \"\".join(c for c in stripped[1:] if c.isalnum())\n                    if stripped == \"{\" + tag:\n                        multiline_input = True\n                        multiline_tag = tag\n                        inp += \"\"\n                    else:\n                        inp = line\n                        break\n                else:\n                    inp = line\n                    break\n                continue\n            elif multiline_input and line.strip():\n                if multiline_tag:\n                    # Check if line is exactly \"tag}\"\n                    if line.strip(\"\\r\\n\") == f\"{multiline_tag}}}\":\n                        break\n                    else:\n                        inp += line + \"\\n\"\n                # Check if line is exactly \"}\"\n                elif line.strip(\"\\r\\n\") == \"}\":\n                    break\n                else:\n                    inp += line + \"\\n\"\n            elif multiline_input:\n                inp += line + \"\\n\"\n            else:\n                inp = line\n                break\n\n        print()\n        self.user_input(inp)\n        return inp",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::13",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 465,
      "span_ids": [
        "InputOutput.user_input",
        "InputOutput.ai_output",
        "InputOutput.get_input_history",
        "InputOutput.add_to_input_history",
        "InputOutput.log_llm_history",
        "InputOutput.display_user_input",
        "InputOutput.offer_url"
      ],
      "start_line": 600,
      "end_line": 663,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class InputOutput:\n\n    def add_to_input_history(self, inp):\n        if not self.input_history_file:\n            return\n        try:\n            FileHistory(self.input_history_file).append_string(inp)\n            # Also add to the in-memory history if it exists\n            if self.prompt_session and self.prompt_session.history:\n                self.prompt_session.history.append_string(inp)\n        except OSError as err:\n            self.tool_warning(f\"Unable to write to input history file: {err}\")\n\n    def get_input_history(self):\n        if not self.input_history_file:\n            return []\n\n        fh = FileHistory(self.input_history_file)\n        return fh.load_history_strings()\n\n    def log_llm_history(self, role, content):\n        if not self.llm_history_file:\n            return\n        timestamp = datetime.now().isoformat(timespec=\"seconds\")\n        with open(self.llm_history_file, \"a\", encoding=self.encoding) as log_file:\n            log_file.write(f\"{role.upper()} {timestamp}\\n\")\n            log_file.write(content + \"\\n\")\n\n    def display_user_input(self, inp):\n        if self.pretty and self.user_input_color:\n            style = dict(style=self.user_input_color)\n        else:\n            style = dict()\n\n        self.console.print(Text(inp), **style)\n\n    def user_input(self, inp, log_only=True):\n        if not log_only:\n            self.display_user_input(inp)\n\n        prefix = \"####\"\n        if inp:\n            hist = inp.splitlines()\n        else:\n            hist = [\"<blank>\"]\n\n        hist = f\"  \\n{prefix} \".join(hist)\n\n        hist = f\"\"\"\n{prefix} {hist}\"\"\"\n        self.append_chat_history(hist, linebreak=True)\n\n    # OUTPUT\n\n    def ai_output(self, content):\n        hist = \"\\n\" + content.strip() + \"\\n\\n\"\n        self.append_chat_history(hist)\n\n    def offer_url(self, url, prompt=\"Open URL for more info?\", allow_never=True):\n        \"\"\"Offer to open a URL in the browser, returns True if opened.\"\"\"\n        if url in self.never_prompts:\n            return False\n        if self.confirm_ask(prompt, subject=url, allow_never=allow_never):\n            webbrowser.open(url)\n            return True\n        return False",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::14",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 747,
      "span_ids": [
        "InputOutput.confirm_ask"
      ],
      "start_line": 665,
      "end_line": 778,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class InputOutput:\n\n    def confirm_ask(\n        self,\n        question,\n        default=\"y\",\n        subject=None,\n        explicit_yes_required=False,\n        group=None,\n        allow_never=False,\n    ):\n        # Temporarily disable multiline mode for yes/no prompts\n        orig_multiline = self.multiline_mode\n        self.multiline_mode = False\n        self.num_user_asks += 1\n\n        question_id = (question, subject)\n\n        if question_id in self.never_prompts:\n            return False\n\n        if group and not group.show_group:\n            group = None\n        if group:\n            allow_never = True\n\n        valid_responses = [\"yes\", \"no\"]\n        options = \" (Y)es/(N)o\"\n        if group:\n            if not explicit_yes_required:\n                options += \"/(A)ll\"\n                valid_responses.append(\"all\")\n            options += \"/(S)kip all\"\n            valid_responses.append(\"skip\")\n        if allow_never:\n            options += \"/(D)on't ask again\"\n            valid_responses.append(\"don't\")\n\n        question += options + \" [Yes]: \"\n\n        if subject:\n            self.tool_output()\n            if \"\\n\" in subject:\n                lines = subject.splitlines()\n                max_length = max(len(line) for line in lines)\n                padded_lines = [line.ljust(max_length) for line in lines]\n                padded_subject = \"\\n\".join(padded_lines)\n                self.tool_output(padded_subject, bold=True)\n            else:\n                self.tool_output(subject, bold=True)\n\n        style = self._get_style()\n\n        def is_valid_response(text):\n            if not text:\n                return True\n            return text.lower() in valid_responses\n\n        if self.yes is True:\n            res = \"n\" if explicit_yes_required else \"y\"\n        elif self.yes is False:\n            res = \"n\"\n        elif group and group.preference:\n            res = group.preference\n            self.user_input(f\"{question}{res}\", log_only=False)\n        else:\n            while True:\n                if self.prompt_session:\n                    res = self.prompt_session.prompt(\n                        question,\n                        style=style,\n                        complete_while_typing=False,\n                    )\n                else:\n                    res = input(question)\n\n                if not res:\n                    res = \"y\"  # Default to Yes if no input\n                    break\n                res = res.lower()\n                good = any(valid_response.startswith(res) for valid_response in valid_responses)\n                if good:\n                    break\n\n                error_message = f\"Please answer with one of: {', '.join(valid_responses)}\"\n                self.tool_error(error_message)\n\n        res = res.lower()[0]\n\n        if res == \"d\" and allow_never:\n            self.never_prompts.add(question_id)\n            hist = f\"{question.strip()} {res}\"\n            self.append_chat_history(hist, linebreak=True, blockquote=True)\n            return False\n\n        if explicit_yes_required:\n            is_yes = res == \"y\"\n        else:\n            is_yes = res in (\"y\", \"a\")\n\n        is_all = res == \"a\" and group is not None and not explicit_yes_required\n        is_skip = res == \"s\" and group is not None\n\n        if group:\n            if is_all and not explicit_yes_required:\n                group.preference = \"all\"\n            elif is_skip:\n                group.preference = \"skip\"\n\n        hist = f\"{question.strip()} {res}\"\n        self.append_chat_history(hist, linebreak=True, blockquote=True)\n\n        # Restore original multiline mode\n        self.multiline_mode = orig_multiline\n\n        return is_yes",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::15",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 225,
      "span_ids": [
        "InputOutput.prompt_ask"
      ],
      "start_line": 780,
      "end_line": 815,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class InputOutput:\n\n    def prompt_ask(self, question, default=\"\", subject=None):\n        # Temporarily disable multiline mode for prompts\n        orig_multiline = self.multiline_mode\n        self.multiline_mode = False\n        self.num_user_asks += 1\n\n        if subject:\n            self.tool_output()\n            self.tool_output(subject, bold=True)\n\n        style = self._get_style()\n\n        if self.yes is True:\n            res = \"yes\"\n        elif self.yes is False:\n            res = \"no\"\n        else:\n            if self.prompt_session:\n                res = self.prompt_session.prompt(\n                    question + \" \",\n                    default=default,\n                    style=style,\n                    complete_while_typing=True,\n                )\n            else:\n                res = input(question + \" \")\n\n        hist = f\"{question.strip()} {res.strip()}\"\n        self.append_chat_history(hist, linebreak=True, blockquote=True)\n        if self.yes in (True, False):\n            self.tool_output(hist)\n\n        # Restore original multiline mode\n        self.multiline_mode = orig_multiline\n\n        return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::16",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 185,
      "span_ids": [
        "InputOutput._tool_message"
      ],
      "start_line": 817,
      "end_line": 836,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class InputOutput:\n\n    def _tool_message(self, message=\"\", strip=True, color=None):\n        if message.strip():\n            if \"\\n\" in message:\n                for line in message.splitlines():\n                    self.append_chat_history(line, linebreak=True, blockquote=True, strip=strip)\n            else:\n                hist = message.strip() if strip else message\n                self.append_chat_history(hist, linebreak=True, blockquote=True)\n\n        if not isinstance(message, Text):\n            message = Text(message)\n        style = dict(style=color) if self.pretty and color else dict()\n        try:\n            self.console.print(message, **style)\n        except UnicodeEncodeError:\n            # Fallback to ASCII-safe output\n            if isinstance(message, Text):\n                message = message.plain\n            message = str(message).encode(\"ascii\", errors=\"replace\").decode(\"ascii\")\n            self.console.print(message, **style)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::17",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 187,
      "span_ids": [
        "InputOutput.tool_output",
        "InputOutput.tool_error",
        "InputOutput.tool_warning"
      ],
      "start_line": 838,
      "end_line": 862,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class InputOutput:\n\n    def tool_error(self, message=\"\", strip=True):\n        self.num_error_outputs += 1\n        self._tool_message(message, strip, self.tool_error_color)\n\n    def tool_warning(self, message=\"\", strip=True):\n        self._tool_message(message, strip, self.tool_warning_color)\n\n    def tool_output(self, *messages, log_only=False, bold=False):\n        if messages:\n            hist = \" \".join(messages)\n            hist = f\"{hist.strip()}\"\n            self.append_chat_history(hist, linebreak=True, blockquote=True)\n\n        if log_only:\n            return\n\n        messages = list(map(Text, messages))\n        style = dict()\n        if self.pretty:\n            if self.tool_output_color:\n                style[\"color\"] = self.tool_output_color\n            style[\"reverse\"] = bold\n\n        style = RichStyle(**style)\n        self.console.print(*messages, style=style)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::18",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 273,
      "span_ids": [
        "InputOutput.print",
        "InputOutput.toggle_multiline_mode",
        "InputOutput.assistant_output",
        "InputOutput.set_placeholder",
        "InputOutput.get_assistant_mdstream"
      ],
      "start_line": 864,
      "end_line": 902,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class InputOutput:\n\n    def get_assistant_mdstream(self):\n        mdargs = dict(style=self.assistant_output_color, code_theme=self.code_theme)\n        mdStream = MarkdownStream(mdargs=mdargs)\n        return mdStream\n\n    def assistant_output(self, message, pretty=None):\n        show_resp = message\n\n        # Coder will force pretty off if fence is not triple-backticks\n        if pretty is None:\n            pretty = self.pretty\n\n        if pretty:\n            show_resp = Markdown(\n                message, style=self.assistant_output_color, code_theme=self.code_theme\n            )\n        else:\n            show_resp = Text(message or \"<no response>\")\n\n        self.console.print(show_resp)\n\n    def set_placeholder(self, placeholder):\n        \"\"\"Set a one-time placeholder text for the next input prompt.\"\"\"\n        self.placeholder = placeholder\n\n    def print(self, message=\"\"):\n        print(message)\n\n    def toggle_multiline_mode(self):\n        \"\"\"Toggle between normal and multiline input modes\"\"\"\n        self.multiline_mode = not self.multiline_mode\n        if self.multiline_mode:\n            self.tool_output(\n                \"Multiline mode: Enabled. Enter inserts newline, Alt-Enter submits text\"\n            )\n        else:\n            self.tool_output(\n                \"Multiline mode: Disabled. Alt-Enter inserts newline, Enter submits text\"\n            )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::19",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 175,
      "span_ids": [
        "InputOutput.append_chat_history"
      ],
      "start_line": 904,
      "end_line": 922,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class InputOutput:\n\n    def append_chat_history(self, text, linebreak=False, blockquote=False, strip=True):\n        if blockquote:\n            if strip:\n                text = text.strip()\n            text = \"> \" + text\n        if linebreak:\n            if strip:\n                text = text.rstrip()\n            text = text + \"  \\n\"\n        if not text.endswith(\"\\n\"):\n            text += \"\\n\"\n        if self.chat_history_file is not None:\n            try:\n                with self.chat_history_file.open(\"a\", encoding=self.encoding, errors=\"ignore\") as f:\n                    f.write(text)\n            except (PermissionError, OSError) as err:\n                print(f\"Warning: Unable to write to chat history file {self.chat_history_file}.\")\n                print(err)\n                self.chat_history_file = None  # Disable further attempts to write\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/io.py::20",
    "metadata": {
      "file_path": "aider/io.py",
      "file_name": "io.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 433,
      "span_ids": [
        "get_rel_fname",
        "InputOutput.format_files_for_input"
      ],
      "start_line": 924,
      "end_line": 977,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class InputOutput:\n\n    def format_files_for_input(self, rel_fnames, rel_read_only_fnames):\n        if not self.pretty:\n            read_only_files = []\n            for full_path in sorted(rel_read_only_fnames or []):\n                read_only_files.append(f\"{full_path} (read only)\")\n\n            editable_files = []\n            for full_path in sorted(rel_fnames):\n                if full_path in rel_read_only_fnames:\n                    continue\n                editable_files.append(f\"{full_path}\")\n\n            return \"\\n\".join(read_only_files + editable_files) + \"\\n\"\n\n        output = StringIO()\n        console = Console(file=output, force_terminal=False)\n\n        read_only_files = sorted(rel_read_only_fnames or [])\n        editable_files = [f for f in sorted(rel_fnames) if f not in rel_read_only_fnames]\n\n        if read_only_files:\n            # Use shorter of abs/rel paths for readonly files\n            ro_paths = []\n            for rel_path in read_only_files:\n                abs_path = os.path.abspath(os.path.join(self.root, rel_path))\n                ro_paths.append(abs_path if len(abs_path) < len(rel_path) else rel_path)\n\n            files_with_label = [\"Readonly:\"] + ro_paths\n            read_only_output = StringIO()\n            Console(file=read_only_output, force_terminal=False).print(Columns(files_with_label))\n            read_only_lines = read_only_output.getvalue().splitlines()\n            console.print(Columns(files_with_label))\n\n        if editable_files:\n            files_with_label = editable_files\n            if read_only_files:\n                files_with_label = [\"Editable:\"] + editable_files\n                editable_output = StringIO()\n                Console(file=editable_output, force_terminal=False).print(Columns(files_with_label))\n                editable_lines = editable_output.getvalue().splitlines()\n\n                if len(read_only_lines) > 1 or len(editable_lines) > 1:\n                    console.print()\n            console.print(Columns(files_with_label))\n\n        return output.getvalue()\n\n\ndef get_rel_fname(fname, root):\n    try:\n        return os.path.relpath(fname, root)\n    except ValueError:\n        return fname",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/linter.py::1",
    "metadata": {
      "file_path": "aider/linter.py",
      "file_name": "linter.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 239,
      "span_ids": [
        "Linter.__init__",
        "Linter.set_linter",
        "Linter",
        "imports",
        "Linter.get_rel_fname"
      ],
      "start_line": 1,
      "end_line": 44,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import os\nimport re\nimport subprocess\nimport sys\nimport traceback\nimport warnings\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\nfrom grep_ast import TreeContext, filename_to_lang\nfrom tree_sitter_languages import get_parser  # noqa: E402\n\nfrom aider.dump import dump  # noqa: F401\nfrom aider.run_cmd import run_cmd_subprocess  # noqa: F401\n\n# tree_sitter is throwing a FutureWarning\nwarnings.simplefilter(\"ignore\", category=FutureWarning)\n\n\nclass Linter:\n    def __init__(self, encoding=\"utf-8\", root=None):\n        self.encoding = encoding\n        self.root = root\n\n        self.languages = dict(\n            python=self.py_lint,\n        )\n        self.all_lint_cmd = None\n\n    def set_linter(self, lang, cmd):\n        if lang:\n            self.languages[lang] = cmd\n            return\n\n        self.all_lint_cmd = cmd\n\n    def get_rel_fname(self, fname):\n        if self.root:\n            try:\n                return os.path.relpath(fname, self.root)\n            except ValueError:\n                return fname\n        else:\n            return fname",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/linter.py::2",
    "metadata": {
      "file_path": "aider/linter.py",
      "file_name": "linter.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 237,
      "span_ids": [
        "Linter.errors_to_lint_result",
        "Linter.run_cmd"
      ],
      "start_line": 46,
      "end_line": 79,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Linter:\n\n    def run_cmd(self, cmd, rel_fname, code):\n        cmd += \" \" + rel_fname\n\n        returncode = 0\n        stdout = \"\"\n        try:\n            returncode, stdout = run_cmd_subprocess(\n                cmd,\n                cwd=self.root,\n                encoding=self.encoding,\n            )\n        except OSError as err:\n            print(f\"Unable to execute lint command: {err}\")\n            return\n        errors = stdout\n        if returncode == 0:\n            return  # zero exit status\n\n        res = f\"## Running: {cmd}\\n\\n\"\n        res += errors\n\n        return self.errors_to_lint_result(rel_fname, res)\n\n    def errors_to_lint_result(self, rel_fname, errors):\n        if not errors:\n            return\n\n        linenums = []\n        filenames_linenums = find_filenames_and_linenums(errors, [rel_fname])\n        if filenames_linenums:\n            filename, linenums = next(iter(filenames_linenums.items()))\n            linenums = [num - 1 for num in linenums]\n\n        return LintResult(text=errors, lines=linenums)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/linter.py::3",
    "metadata": {
      "file_path": "aider/linter.py",
      "file_name": "linter.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 228,
      "span_ids": [
        "Linter.lint"
      ],
      "start_line": 81,
      "end_line": 115,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Linter:\n\n    def lint(self, fname, cmd=None):\n        rel_fname = self.get_rel_fname(fname)\n        try:\n            code = Path(fname).read_text(encoding=self.encoding, errors=\"replace\")\n        except OSError as err:\n            print(f\"Unable to read {fname}: {err}\")\n            return\n\n        if cmd:\n            cmd = cmd.strip()\n        if not cmd:\n            lang = filename_to_lang(fname)\n            if not lang:\n                return\n            if self.all_lint_cmd:\n                cmd = self.all_lint_cmd\n            else:\n                cmd = self.languages.get(lang)\n\n        if callable(cmd):\n            lintres = cmd(fname, rel_fname, code)\n        elif cmd:\n            lintres = self.run_cmd(cmd, rel_fname, code)\n        else:\n            lintres = basic_lint(rel_fname, code)\n\n        if not lintres:\n            return\n\n        res = \"# Fix any errors below, if possible.\\n\\n\"\n        res += lintres.text\n        res += \"\\n\"\n        res += tree_context(rel_fname, code, lintres.lines)\n\n        return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/linter.py::4",
    "metadata": {
      "file_path": "aider/linter.py",
      "file_name": "linter.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 124,
      "span_ids": [
        "Linter.py_lint"
      ],
      "start_line": 117,
      "end_line": 133,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Linter:\n\n    def py_lint(self, fname, rel_fname, code):\n        basic_res = basic_lint(rel_fname, code)\n        compile_res = lint_python_compile(fname, code)\n        flake_res = self.flake8_lint(rel_fname)\n\n        text = \"\"\n        lines = set()\n        for res in [basic_res, compile_res, flake_res]:\n            if not res:\n                continue\n            if text:\n                text += \"\\n\"\n            text += res.text\n            lines.update(res.lines)\n\n        if text or lines:\n            return LintResult(text, lines)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/linter.py::5",
    "metadata": {
      "file_path": "aider/linter.py",
      "file_name": "linter.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 208,
      "span_ids": [
        "Linter.flake8_lint"
      ],
      "start_line": 135,
      "end_line": 167,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Linter:\n\n    def flake8_lint(self, rel_fname):\n        fatal = \"E9,F821,F823,F831,F406,F407,F701,F702,F704,F706\"\n        flake8_cmd = [\n            sys.executable,\n            \"-m\",\n            \"flake8\",\n            f\"--select={fatal}\",\n            \"--show-source\",\n            \"--isolated\",\n            rel_fname,\n        ]\n\n        text = f\"## Running: {' '.join(flake8_cmd)}\\n\\n\"\n\n        try:\n            result = subprocess.run(\n                flake8_cmd,\n                capture_output=True,\n                text=True,\n                check=False,\n                encoding=self.encoding,\n                errors=\"replace\",\n                cwd=self.root,\n            )\n            errors = result.stdout + result.stderr\n        except Exception as e:\n            errors = f\"Error running flake8: {str(e)}\"\n\n        if not errors:\n            return\n\n        text += errors\n        return self.errors_to_lint_result(rel_fname, text)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/linter.py::6",
    "metadata": {
      "file_path": "aider/linter.py",
      "file_name": "linter.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 194,
      "span_ids": [
        "LintResult",
        "lint_python_compile"
      ],
      "start_line": 170,
      "end_line": 197,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "@dataclass\nclass LintResult:\n    text: str\n    lines: list\n\n\ndef lint_python_compile(fname, code):\n    try:\n        compile(code, fname, \"exec\")  # USE TRACEBACK BELOW HERE\n        return\n    except Exception as err:\n        end_lineno = getattr(err, \"end_lineno\", err.lineno)\n        line_numbers = list(range(err.lineno - 1, end_lineno))\n\n        tb_lines = traceback.format_exception(type(err), err, err.__traceback__)\n        last_file_i = 0\n\n        target = \"# USE TRACEBACK\"\n        target += \" BELOW HERE\"\n        for i in range(len(tb_lines)):\n            if target in tb_lines[i]:\n                last_file_i = i\n                break\n\n        tb_lines = tb_lines[:1] + tb_lines[last_file_i + 1 :]\n\n    res = \"\".join(tb_lines)\n    return LintResult(text=res, lines=line_numbers)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/linter.py::7",
    "metadata": {
      "file_path": "aider/linter.py",
      "file_name": "linter.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 174,
      "span_ids": [
        "basic_lint"
      ],
      "start_line": 200,
      "end_line": 230,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def basic_lint(fname, code):\n    \"\"\"\n    Use tree-sitter to look for syntax errors, display them with tree context.\n    \"\"\"\n\n    lang = filename_to_lang(fname)\n    if not lang:\n        return\n\n    # Tree-sitter linter is not capable of working with typescript #1132\n    if lang == \"typescript\":\n        return\n\n    try:\n        parser = get_parser(lang)\n    except Exception as err:\n        print(f\"Unable to load parser: {err}\")\n        return\n\n    tree = parser.parse(bytes(code, \"utf-8\"))\n\n    try:\n        errors = traverse_tree(tree.root_node)\n    except RecursionError:\n        print(f\"Unable to lint {fname} due to RecursionError\")\n        return\n\n    if not errors:\n        return\n\n    return LintResult(text=\"\", lines=errors)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/linter.py::8",
    "metadata": {
      "file_path": "aider/linter.py",
      "file_name": "linter.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 215,
      "span_ids": [
        "traverse_tree",
        "tree_context"
      ],
      "start_line": 233,
      "end_line": 268,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def tree_context(fname, code, line_nums):\n    context = TreeContext(\n        fname,\n        code,\n        color=False,\n        line_number=True,\n        child_context=False,\n        last_line=False,\n        margin=0,\n        mark_lois=True,\n        loi_pad=3,\n        # header_max=30,\n        show_top_of_file_parent_scope=False,\n    )\n    line_nums = set(line_nums)\n    context.add_lines_of_interest(line_nums)\n    context.add_context()\n    s = \"s\" if len(line_nums) > 1 else \"\"\n    output = f\"## See relevant line{s} below marked with \u2588.\\n\\n\"\n    output += fname + \":\\n\"\n    output += context.format()\n\n    return output\n\n\n# Traverse the tree to find errors\ndef traverse_tree(node):\n    errors = []\n    if node.type == \"ERROR\" or node.is_missing:\n        line_no = node.start_point[0]\n        errors.append(line_no)\n\n    for child in node.children:\n        errors += traverse_tree(child)\n\n    return errors",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/linter.py::9",
    "metadata": {
      "file_path": "aider/linter.py",
      "file_name": "linter.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 251,
      "span_ids": [
        "impl:2",
        "find_filenames_and_linenums",
        "main"
      ],
      "start_line": 271,
      "end_line": 304,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def find_filenames_and_linenums(text, fnames):\n    \"\"\"\n    Search text for all occurrences of <filename>:\\\\d+ and make a list of them\n    where <filename> is one of the filenames in the list `fnames`.\n    \"\"\"\n    pattern = re.compile(r\"(\\b(?:\" + \"|\".join(re.escape(fname) for fname in fnames) + r\"):\\d+\\b)\")\n    matches = pattern.findall(text)\n    result = {}\n    for match in matches:\n        fname, linenum = match.rsplit(\":\", 1)\n        if fname not in result:\n            result[fname] = set()\n        result[fname].add(int(linenum))\n    return result\n\n\ndef main():\n    \"\"\"\n    Main function to parse files provided as command line arguments.\n    \"\"\"\n    if len(sys.argv) < 2:\n        print(\"Usage: python linter.py <file1> <file2> ...\")\n        sys.exit(1)\n\n    linter = Linter(root=os.getcwd())\n    for file_path in sys.argv[1:]:\n        errors = linter.lint(file_path)\n        if errors:\n            print(errors)\n\n\nif __name__ == \"__main__\":\n    main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/llm.py::1",
    "metadata": {
      "file_path": "aider/llm.py",
      "file_name": "llm.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 108,
      "span_ids": [
        "imports"
      ],
      "start_line": 1,
      "end_line": 16,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import importlib\nimport os\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")\n\nAIDER_SITE_URL = \"https://aider.chat\"\nAIDER_APP_NAME = \"Aider\"\n\nos.environ[\"OR_SITE_URL\"] = AIDER_SITE_URL\nos.environ[\"OR_APP_NAME\"] = AIDER_APP_NAME\nos.environ[\"LITELLM_MODE\"] = \"PRODUCTION\"\n\n# `import litellm` takes 1.5 seconds, defer it!\n\nVERBOSE = False",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/llm.py::2",
    "metadata": {
      "file_path": "aider/llm.py",
      "file_name": "llm.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 164,
      "span_ids": [
        "impl:11",
        "LazyLiteLLM",
        "LazyLiteLLM._load_litellm",
        "LazyLiteLLM.__getattr__"
      ],
      "start_line": 19,
      "end_line": 46,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class LazyLiteLLM:\n    _lazy_module = None\n\n    def __getattr__(self, name):\n        if name == \"_lazy_module\":\n            return super()\n        self._load_litellm()\n        return getattr(self._lazy_module, name)\n\n    def _load_litellm(self):\n        if self._lazy_module is not None:\n            return\n\n        if VERBOSE:\n            print(\"Loading litellm...\")\n\n        self._lazy_module = importlib.import_module(\"litellm\")\n\n        self._lazy_module.suppress_debug_info = True\n        self._lazy_module.set_verbose = False\n        self._lazy_module.drop_params = True\n        self._lazy_module._logging._disable_debugging()\n\n\nlitellm = LazyLiteLLM()\n\n__all__ = [litellm]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::1",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 240,
      "span_ids": [
        "imports"
      ],
      "start_line": 1,
      "end_line": 38,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import configparser\nimport json\nimport os\nimport re\nimport sys\nimport threading\nimport traceback\nimport webbrowser\nfrom dataclasses import fields\nfrom pathlib import Path\n\ntry:\n    import git\nexcept ImportError:\n    git = None\n\nimport importlib_resources\nfrom dotenv import load_dotenv\nfrom prompt_toolkit.enums import EditingMode\n\nfrom aider import __version__, models, urls, utils\nfrom aider.analytics import Analytics\nfrom aider.args import get_parser\nfrom aider.coders import Coder\nfrom aider.coders.base_coder import UnknownEditFormat\nfrom aider.commands import Commands, SwitchCoder\nfrom aider.copypaste import ClipboardWatcher\nfrom aider.format_settings import format_settings, scrub_sensitive_info\nfrom aider.history import ChatSummary\nfrom aider.io import InputOutput\nfrom aider.llm import litellm  # noqa: F401; properly init litellm on launch\nfrom aider.models import ModelSettings\nfrom aider.repo import ANY_GIT_ERROR, GitRepo\nfrom aider.report import report_uncaught_exceptions\nfrom aider.versioncheck import check_version, install_from_main_branch, install_upgrade\nfrom aider.watch import FileWatcher\n\nfrom .dump import dump",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::2",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 190,
      "span_ids": [
        "imports",
        "check_config_files_for_yes",
        "get_git_root"
      ],
      "start_line": 38,
      "end_line": 64,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "  # noqa: F401\n\n\ndef check_config_files_for_yes(config_files):\n    found = False\n    for config_file in config_files:\n        if Path(config_file).exists():\n            try:\n                with open(config_file, \"r\") as f:\n                    for line in f:\n                        if line.strip().startswith(\"yes:\"):\n                            print(\"Configuration error detected.\")\n                            print(f\"The file {config_file} contains a line starting with 'yes:'\")\n                            print(\"Please replace 'yes:' with 'yes-always:' in this file.\")\n                            found = True\n            except Exception:\n                pass\n    return found\n\n\ndef get_git_root():\n    \"\"\"Try and guess the git repo, since the conf.yml can be at the repo root\"\"\"\n    try:\n        repo = git.Repo(search_parent_directories=True)\n        return repo.working_tree_dir\n    except (git.InvalidGitRepositoryError, FileNotFoundError):\n        return None",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::3",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 227,
      "span_ids": [
        "make_new_repo",
        "guessed_wrong_repo"
      ],
      "start_line": 67,
      "end_line": 96,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def guessed_wrong_repo(io, git_root, fnames, git_dname):\n    \"\"\"After we parse the args, we can determine the real repo. Did we guess wrong?\"\"\"\n\n    try:\n        check_repo = Path(GitRepo(io, fnames, git_dname).root).resolve()\n    except (OSError,) + ANY_GIT_ERROR:\n        return\n\n    # we had no guess, rely on the \"true\" repo result\n    if not git_root:\n        return str(check_repo)\n\n    git_root = Path(git_root).resolve()\n    if check_repo == git_root:\n        return\n\n    return str(check_repo)\n\n\ndef make_new_repo(git_root, io):\n    try:\n        repo = git.Repo.init(git_root)\n        check_gitignore(git_root, io, False)\n    except ANY_GIT_ERROR as err:  # issue #1233\n        io.tool_error(f\"Unable to create git repo in {git_root}\")\n        io.tool_output(str(err))\n        return\n\n    io.tool_output(f\"Git repository created in {git_root}\")\n    return repo",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::4",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 367,
      "span_ids": [
        "setup_git"
      ],
      "start_line": 99,
      "end_line": 152,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def setup_git(git_root, io):\n    if git is None:\n        return\n\n    try:\n        cwd = Path.cwd()\n    except OSError:\n        cwd = None\n\n    repo = None\n\n    if git_root:\n        try:\n            repo = git.Repo(git_root)\n        except ANY_GIT_ERROR:\n            pass\n    elif cwd == Path.home():\n        io.tool_warning(\n            \"You should probably run aider in your project's directory, not your home dir.\"\n        )\n        return\n    elif cwd and io.confirm_ask(\n        \"No git repo found, create one to track aider's changes (recommended)?\"\n    ):\n        git_root = str(cwd.resolve())\n        repo = make_new_repo(git_root, io)\n\n    if not repo:\n        return\n\n    user_name = None\n    user_email = None\n    with repo.config_reader() as config:\n        try:\n            user_name = config.get_value(\"user\", \"name\", None)\n        except (configparser.NoSectionError, configparser.NoOptionError):\n            pass\n        try:\n            user_email = config.get_value(\"user\", \"email\", None)\n        except (configparser.NoSectionError, configparser.NoOptionError):\n            pass\n\n    if user_name and user_email:\n        return repo.working_tree_dir\n\n    with repo.config_writer() as git_config:\n        if not user_name:\n            git_config.set_value(\"user\", \"name\", \"Your Name\")\n            io.tool_warning('Update git name with: git config user.name \"Your Name\"')\n        if not user_email:\n            git_config.set_value(\"user\", \"email\", \"you@example.com\")\n            io.tool_warning('Update git email with: git config user.email \"you@example.com\"')\n\n    return repo.working_tree_dir",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::5",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 391,
      "span_ids": [
        "check_gitignore"
      ],
      "start_line": 155,
      "end_line": 206,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def check_gitignore(git_root, io, ask=True):\n    if not git_root:\n        return\n\n    try:\n        repo = git.Repo(git_root)\n        if repo.ignored(\".aider\") and repo.ignored(\".env\"):\n            return\n    except ANY_GIT_ERROR:\n        pass\n\n    patterns = [\".aider*\", \".env\"]\n    patterns_to_add = []\n\n    gitignore_file = Path(git_root) / \".gitignore\"\n    if gitignore_file.exists():\n        try:\n            content = io.read_text(gitignore_file)\n            if content is None:\n                return\n            existing_lines = content.splitlines()\n            for pat in patterns:\n                if pat not in existing_lines:\n                    if \"*\" in pat or (Path(git_root) / pat).exists():\n                        patterns_to_add.append(pat)\n        except OSError as e:\n            io.tool_error(f\"Error when trying to read {gitignore_file}: {e}\")\n            return\n    else:\n        content = \"\"\n        patterns_to_add = patterns\n\n    if not patterns_to_add:\n        return\n\n    if ask and not io.confirm_ask(f\"Add {', '.join(patterns_to_add)} to .gitignore (recommended)?\"):\n        return\n\n    if content and not content.endswith(\"\\n\"):\n        content += \"\\n\"\n    content += \"\\n\".join(patterns_to_add) + \"\\n\"\n\n    try:\n        io.write_text(gitignore_file, content)\n        io.tool_output(f\"Added {', '.join(patterns_to_add)} to .gitignore\")\n    except OSError as e:\n        io.tool_error(f\"Error when trying to write to {gitignore_file}: {e}\")\n        io.tool_output(\n            \"Try running with appropriate permissions or manually add these patterns to .gitignore:\"\n        )\n        for pattern in patterns_to_add:\n            io.tool_output(f\"  {pattern}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::6",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 170,
      "span_ids": [
        "check_streamlit_install",
        "write_streamlit_credentials"
      ],
      "start_line": 209,
      "end_line": 231,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def check_streamlit_install(io):\n    return utils.check_pip_install_extra(\n        io,\n        \"streamlit\",\n        \"You need to install the aider browser feature\",\n        [\"aider-chat[browser]\"],\n    )\n\n\ndef write_streamlit_credentials():\n    from streamlit.file_util import get_streamlit_file_path\n\n    # See https://github.com/Aider-AI/aider/issues/772\n\n    credential_path = Path(get_streamlit_file_path()) / \"credentials.toml\"\n    if not os.path.exists(credential_path):\n        empty_creds = '[general]\\nemail = \"\"\\n'\n\n        os.makedirs(os.path.dirname(credential_path), exist_ok=True)\n        with open(credential_path, \"w\") as f:\n            f.write(empty_creds)\n    else:\n        print(\"Streamlit credentials already exist.\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::7",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 258,
      "span_ids": [
        "launch_gui"
      ],
      "start_line": 234,
      "end_line": 276,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def launch_gui(args):\n    from streamlit.web import cli\n\n    from aider import gui\n\n    print()\n    print(\"CONTROL-C to exit...\")\n\n    # Necessary so streamlit does not prompt the user for an email address.\n    write_streamlit_credentials()\n\n    target = gui.__file__\n\n    st_args = [\"run\", target]\n\n    st_args += [\n        \"--browser.gatherUsageStats=false\",\n        \"--runner.magicEnabled=false\",\n        \"--server.runOnSave=false\",\n    ]\n\n    # https://github.com/Aider-AI/aider/issues/2193\n    is_dev = \"-dev\" in str(__version__)\n\n    if is_dev:\n        print(\"Watching for file changes.\")\n    else:\n        st_args += [\n            \"--global.developmentMode=false\",\n            \"--server.fileWatcherType=none\",\n            \"--client.toolbarMode=viewer\",  # minimal?\n        ]\n\n    st_args += [\"--\"] + args\n\n    cli.main(st_args)\n\n    # from click.testing import CliRunner\n    # runner = CliRunner()\n    # from streamlit.web import bootstrap\n    # bootstrap.load_config_options(flag_options={})\n    # cli.main_run(target, args)\n    # sys.argv = ['streamlit', 'run', '--'] + args\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::8",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 187,
      "span_ids": [
        "parse_lint_cmds"
      ],
      "start_line": 279,
      "end_line": 303,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def parse_lint_cmds(lint_cmds, io):\n    err = False\n    res = dict()\n    for lint_cmd in lint_cmds:\n        if re.match(r\"^[a-z]+:.*\", lint_cmd):\n            pieces = lint_cmd.split(\":\")\n            lang = pieces[0]\n            cmd = lint_cmd[len(lang) + 1 :]\n            lang = lang.strip()\n        else:\n            lang = None\n            cmd = lint_cmd\n\n        cmd = cmd.strip()\n\n        if cmd:\n            res[lang] = cmd\n        else:\n            io.tool_error(f'Unable to parse --lint-cmd \"{lint_cmd}\"')\n            io.tool_output('The arg should be \"language: cmd --args ...\"')\n            io.tool_output('For example: --lint-cmd \"python: flake8 --select=E9\"')\n            err = True\n    if err:\n        return\n    return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::9",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 168,
      "span_ids": [
        "generate_search_path_list"
      ],
      "start_line": 306,
      "end_line": 333,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def generate_search_path_list(default_file, git_root, command_line_file):\n    files = []\n    files.append(Path.home() / default_file)  # homedir\n    if git_root:\n        files.append(Path(git_root) / default_file)  # git root\n    files.append(default_file)\n    if command_line_file:\n        files.append(command_line_file)\n\n    resolved_files = []\n    for fn in files:\n        try:\n            resolved_files.append(Path(fn).resolve())\n        except OSError:\n            pass\n\n    files = resolved_files\n    files.reverse()\n    uniq = []\n    for fn in files:\n        if fn not in uniq:\n            uniq.append(fn)\n    uniq.reverse()\n    files = uniq\n    files = list(map(str, files))\n    files = list(dict.fromkeys(files))\n\n    return files",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::10",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 188,
      "span_ids": [
        "register_models"
      ],
      "start_line": 336,
      "end_line": 359,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def register_models(git_root, model_settings_fname, io, verbose=False):\n    model_settings_files = generate_search_path_list(\n        \".aider.model.settings.yml\", git_root, model_settings_fname\n    )\n\n    try:\n        files_loaded = models.register_models(model_settings_files)\n        if len(files_loaded) > 0:\n            if verbose:\n                io.tool_output(\"Loaded model settings from:\")\n                for file_loaded in files_loaded:\n                    io.tool_output(f\"  - {file_loaded}\")  # noqa: E221\n        elif verbose:\n            io.tool_output(\"No model settings files loaded\")\n    except Exception as e:\n        io.tool_error(f\"Error loading aider model settings: {e}\")\n        return 1\n\n    if verbose:\n        io.tool_output(\"Searched for model settings files:\")\n        for file in model_settings_files:\n            io.tool_output(f\"  - {file}\")\n\n    return None",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::11",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 120,
      "span_ids": [
        "load_dotenv_files"
      ],
      "start_line": 362,
      "end_line": 378,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def load_dotenv_files(git_root, dotenv_fname, encoding=\"utf-8\"):\n    dotenv_files = generate_search_path_list(\n        \".env\",\n        git_root,\n        dotenv_fname,\n    )\n    loaded = []\n    for fname in dotenv_files:\n        try:\n            if Path(fname).exists():\n                load_dotenv(fname, override=True, encoding=encoding)\n                loaded.append(fname)\n        except OSError as e:\n            print(f\"OSError loading {fname}: {e}\")\n        except Exception as e:\n            print(f\"Error loading {fname}: {e}\")\n    return loaded",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::12",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 188,
      "span_ids": [
        "register_litellm_models"
      ],
      "start_line": 381,
      "end_line": 400,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def register_litellm_models(git_root, model_metadata_fname, io, verbose=False):\n    model_metadata_files = []\n\n    # Add the resource file path\n    resource_metadata = importlib_resources.files(\"aider.resources\").joinpath(\"model-metadata.json\")\n    model_metadata_files.append(str(resource_metadata))\n\n    model_metadata_files += generate_search_path_list(\n        \".aider.model.metadata.json\", git_root, model_metadata_fname\n    )\n\n    try:\n        model_metadata_files_loaded = models.register_litellm_models(model_metadata_files)\n        if len(model_metadata_files_loaded) > 0 and verbose:\n            io.tool_output(\"Loaded model metadata from:\")\n            for model_metadata_file in model_metadata_files_loaded:\n                io.tool_output(f\"  - {model_metadata_file}\")  # noqa: E221\n    except Exception as e:\n        io.tool_error(f\"Error loading model metadata models: {e}\")\n        return 1",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::13",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 316,
      "span_ids": [
        "sanity_check_repo"
      ],
      "start_line": 403,
      "end_line": 439,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def sanity_check_repo(repo, io):\n    if not repo:\n        return True\n\n    if not repo.repo.working_tree_dir:\n        io.tool_error(\"The git repo does not seem to have a working tree?\")\n        return False\n\n    bad_ver = False\n    try:\n        repo.get_tracked_files()\n        if not repo.git_repo_error:\n            return True\n        error_msg = str(repo.git_repo_error)\n    except UnicodeDecodeError as exc:\n        error_msg = (\n            \"Failed to read the Git repository. This issue is likely caused by a path encoded \"\n            f'in a format different from the expected encoding \"{sys.getfilesystemencoding()}\".\\n'\n            f\"Internal error: {str(exc)}\"\n        )\n    except ANY_GIT_ERROR as exc:\n        error_msg = str(exc)\n        bad_ver = \"version in (1, 2)\" in error_msg\n    except AssertionError as exc:\n        error_msg = str(exc)\n        bad_ver = True\n\n    if bad_ver:\n        io.tool_error(\"Aider only works with git repos with version number 1 or 2.\")\n        io.tool_output(\"You may be able to convert your repo: git update-index --index-version=2\")\n        io.tool_output(\"Or run aider --no-git to proceed without using git.\")\n        io.offer_url(urls.git_index_version, \"Open documentation url for more info?\")\n        return False\n\n    io.tool_error(\"Unable to read git repository, it may be corrupt?\")\n    io.tool_output(error_msg)\n    return False",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::14",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 704,
      "span_ids": [
        "main"
      ],
      "start_line": 442,
      "end_line": 533,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main(argv=None, input=None, output=None, force_git_root=None, return_coder=False):\n    report_uncaught_exceptions()\n\n    if argv is None:\n        argv = sys.argv[1:]\n\n    if git is None:\n        git_root = None\n    elif force_git_root:\n        git_root = force_git_root\n    else:\n        git_root = get_git_root()\n\n    conf_fname = Path(\".aider.conf.yml\")\n\n    default_config_files = []\n    try:\n        default_config_files += [conf_fname.resolve()]  # CWD\n    except OSError:\n        pass\n\n    if git_root:\n        git_conf = Path(git_root) / conf_fname  # git root\n        if git_conf not in default_config_files:\n            default_config_files.append(git_conf)\n    default_config_files.append(Path.home() / conf_fname)  # homedir\n    default_config_files = list(map(str, default_config_files))\n\n    parser = get_parser(default_config_files, git_root)\n    try:\n        args, unknown = parser.parse_known_args(argv)\n    except AttributeError as e:\n        if all(word in str(e) for word in [\"bool\", \"object\", \"has\", \"no\", \"attribute\", \"strip\"]):\n            if check_config_files_for_yes(default_config_files):\n                return 1\n        raise e\n\n    if args.verbose:\n        print(\"Config files search order, if no --config:\")\n        for file in default_config_files:\n            exists = \"(exists)\" if Path(file).exists() else \"\"\n            print(f\"  - {file} {exists}\")\n\n    default_config_files.reverse()\n\n    parser = get_parser(default_config_files, git_root)\n\n    args, unknown = parser.parse_known_args(argv)\n\n    # Load the .env file specified in the arguments\n    loaded_dotenvs = load_dotenv_files(git_root, args.env_file, args.encoding)\n\n    # Parse again to include any arguments that might have been defined in .env\n    args = parser.parse_args(argv)\n\n    if git is None:\n        args.git = False\n\n    if args.analytics_disable:\n        analytics = Analytics(permanently_disable=True)\n        print(\"Analytics have been permanently disabled.\")\n\n    if not args.verify_ssl:\n        import httpx\n\n        os.environ[\"SSL_VERIFY\"] = \"\"\n        litellm._load_litellm()\n        litellm._lazy_module.client_session = httpx.Client(verify=False)\n        litellm._lazy_module.aclient_session = httpx.AsyncClient(verify=False)\n\n    if args.timeout:\n        litellm._load_litellm()\n        litellm._lazy_module.request_timeout = args.timeout\n\n    if args.dark_mode:\n        args.user_input_color = \"#32FF32\"\n        args.tool_error_color = \"#FF3333\"\n        args.tool_warning_color = \"#FFFF00\"\n        args.assistant_output_color = \"#00FFFF\"\n        args.code_theme = \"monokai\"\n\n    if args.light_mode:\n        args.user_input_color = \"green\"\n        args.tool_error_color = \"red\"\n        args.tool_warning_color = \"#FFA500\"\n        args.assistant_output_color = \"blue\"\n        args.code_theme = \"default\"\n\n    if return_coder and args.yes_always is None:\n        args.yes_always = True\n\n    editing_mode = EditingMode.VI if args.vim else EditingMode.EMACS\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::15",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 232,
      "span_ids": [
        "main"
      ],
      "start_line": 535,
      "end_line": 560,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main(argv=None, input=None, output=None, force_git_root=None, return_coder=False):\n    # ... other code\n\n    def get_io(pretty):\n        return InputOutput(\n            pretty,\n            args.yes_always,\n            args.input_history_file,\n            args.chat_history_file,\n            input=input,\n            output=output,\n            user_input_color=args.user_input_color,\n            tool_output_color=args.tool_output_color,\n            tool_warning_color=args.tool_warning_color,\n            tool_error_color=args.tool_error_color,\n            completion_menu_color=args.completion_menu_color,\n            completion_menu_bg_color=args.completion_menu_bg_color,\n            completion_menu_current_color=args.completion_menu_current_color,\n            completion_menu_current_bg_color=args.completion_menu_current_bg_color,\n            assistant_output_color=args.assistant_output_color,\n            code_theme=args.code_theme,\n            dry_run=args.dry_run,\n            encoding=args.encoding,\n            line_endings=args.line_endings,\n            llm_history_file=args.llm_history_file,\n            editingmode=editing_mode,\n            fancy_input=args.fancy_input,\n            multiline_mode=args.multiline,\n        )\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::16",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 804,
      "span_ids": [
        "main"
      ],
      "start_line": 562,
      "end_line": 655,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main(argv=None, input=None, output=None, force_git_root=None, return_coder=False):\n    # ... other code\n\n    io = get_io(args.pretty)\n    try:\n        io.rule()\n    except UnicodeEncodeError as err:\n        if not io.pretty:\n            raise err\n        io = get_io(False)\n        io.tool_warning(\"Terminal does not support pretty output (UnicodeDecodeError)\")\n\n    # Process any environment variables set via --set-env\n    if args.set_env:\n        for env_setting in args.set_env:\n            try:\n                name, value = env_setting.split(\"=\", 1)\n                os.environ[name.strip()] = value.strip()\n            except ValueError:\n                io.tool_error(f\"Invalid --set-env format: {env_setting}\")\n                io.tool_output(\"Format should be: ENV_VAR_NAME=value\")\n                return 1\n\n    # Process any API keys set via --api-key\n    if args.api_key:\n        for api_setting in args.api_key:\n            try:\n                provider, key = api_setting.split(\"=\", 1)\n                env_var = f\"{provider.strip().upper()}_API_KEY\"\n                os.environ[env_var] = key.strip()\n            except ValueError:\n                io.tool_error(f\"Invalid --api-key format: {api_setting}\")\n                io.tool_output(\"Format should be: provider=key\")\n                return 1\n\n    if args.anthropic_api_key:\n        os.environ[\"ANTHROPIC_API_KEY\"] = args.anthropic_api_key\n\n    if args.openai_api_key:\n        os.environ[\"OPENAI_API_KEY\"] = args.openai_api_key\n    if args.openai_api_base:\n        os.environ[\"OPENAI_API_BASE\"] = args.openai_api_base\n    if args.openai_api_version:\n        io.tool_warning(\n            \"--openai-api-version is deprecated, use --set-env OPENAI_API_VERSION=<value>\"\n        )\n        os.environ[\"OPENAI_API_VERSION\"] = args.openai_api_version\n    if args.openai_api_type:\n        io.tool_warning(\"--openai-api-type is deprecated, use --set-env OPENAI_API_TYPE=<value>\")\n        os.environ[\"OPENAI_API_TYPE\"] = args.openai_api_type\n    if args.openai_organization_id:\n        io.tool_warning(\n            \"--openai-organization-id is deprecated, use --set-env OPENAI_ORGANIZATION=<value>\"\n        )\n        os.environ[\"OPENAI_ORGANIZATION\"] = args.openai_organization_id\n\n    analytics = Analytics(logfile=args.analytics_log, permanently_disable=args.analytics_disable)\n    if args.analytics is not False:\n        if analytics.need_to_ask(args.analytics):\n            io.tool_output(\n                \"Aider respects your privacy and never collects your code, chat messages, keys or\"\n                \" personal info.\"\n            )\n            io.tool_output(f\"For more info: {urls.analytics}\")\n            disable = not io.confirm_ask(\n                \"Allow collection of anonymous analytics to help improve aider?\"\n            )\n\n            analytics.asked_opt_in = True\n            if disable:\n                analytics.disable(permanently=True)\n                io.tool_output(\"Analytics have been permanently disabled.\")\n\n            analytics.save_data()\n            io.tool_output()\n\n        # This is a no-op if the user has opted out\n        analytics.enable()\n\n    analytics.event(\"launched\")\n\n    if args.gui and not return_coder:\n        if not check_streamlit_install(io):\n            analytics.event(\"exit\", reason=\"Streamlit not installed\")\n            return\n        analytics.event(\"gui session\")\n        launch_gui(argv)\n        analytics.event(\"exit\", reason=\"GUI session ended\")\n        return\n\n    if args.verbose:\n        for fname in loaded_dotenvs:\n            io.tool_output(f\"Loaded {fname}\")\n\n    all_files = args.files + (args.file or [])\n    fnames = [str(Path(fn).resolve()) for fn in all_files]\n    read_only_fnames = []\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::17",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 850,
      "span_ids": [
        "main"
      ],
      "start_line": 656,
      "end_line": 749,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main(argv=None, input=None, output=None, force_git_root=None, return_coder=False):\n    # ... other code\n    for fn in args.read or []:\n        path = Path(fn).expanduser().resolve()\n        if path.is_dir():\n            read_only_fnames.extend(str(f) for f in path.rglob(\"*\") if f.is_file())\n        else:\n            read_only_fnames.append(str(path))\n\n    if len(all_files) > 1:\n        good = True\n        for fname in all_files:\n            if Path(fname).is_dir():\n                io.tool_error(f\"{fname} is a directory, not provided alone.\")\n                good = False\n        if not good:\n            io.tool_output(\n                \"Provide either a single directory of a git repo, or a list of one or more files.\"\n            )\n            analytics.event(\"exit\", reason=\"Invalid directory input\")\n            return 1\n\n    git_dname = None\n    if len(all_files) == 1:\n        if Path(all_files[0]).is_dir():\n            if args.git:\n                git_dname = str(Path(all_files[0]).resolve())\n                fnames = []\n            else:\n                io.tool_error(f\"{all_files[0]} is a directory, but --no-git selected.\")\n                analytics.event(\"exit\", reason=\"Directory with --no-git\")\n                return 1\n\n    # We can't know the git repo for sure until after parsing the args.\n    # If we guessed wrong, reparse because that changes things like\n    # the location of the config.yml and history files.\n    if args.git and not force_git_root and git is not None:\n        right_repo_root = guessed_wrong_repo(io, git_root, fnames, git_dname)\n        if right_repo_root:\n            analytics.event(\"exit\", reason=\"Recursing with correct repo\")\n            return main(argv, input, output, right_repo_root, return_coder=return_coder)\n\n    if args.just_check_update:\n        update_available = check_version(io, just_check=True, verbose=args.verbose)\n        analytics.event(\"exit\", reason=\"Just checking update\")\n        return 0 if not update_available else 1\n\n    if args.install_main_branch:\n        success = install_from_main_branch(io)\n        analytics.event(\"exit\", reason=\"Installed main branch\")\n        return 0 if success else 1\n\n    if args.upgrade:\n        success = install_upgrade(io)\n        analytics.event(\"exit\", reason=\"Upgrade completed\")\n        return 0 if success else 1\n\n    if args.check_update:\n        check_version(io, verbose=args.verbose)\n\n    if args.list_models:\n        models.print_matching_models(io, args.list_models)\n        analytics.event(\"exit\", reason=\"Listed models\")\n        return 0\n\n    if args.git:\n        git_root = setup_git(git_root, io)\n        if args.gitignore:\n            check_gitignore(git_root, io)\n\n    if args.verbose:\n        show = format_settings(parser, args)\n        io.tool_output(show)\n\n    cmd_line = \" \".join(sys.argv)\n    cmd_line = scrub_sensitive_info(args, cmd_line)\n    io.tool_output(cmd_line, log_only=True)\n\n    is_first_run = is_first_run_of_new_version(io, verbose=args.verbose)\n    check_and_load_imports(io, is_first_run, verbose=args.verbose)\n\n    register_models(git_root, args.model_settings_file, io, verbose=args.verbose)\n    register_litellm_models(git_root, args.model_metadata_file, io, verbose=args.verbose)\n\n    # Process any command line aliases\n    if args.alias:\n        for alias_def in args.alias:\n            # Split on first colon only\n            parts = alias_def.split(\":\", 1)\n            if len(parts) != 2:\n                io.tool_error(f\"Invalid alias format: {alias_def}\")\n                io.tool_output(\"Format should be: alias:model-name\")\n                analytics.event(\"exit\", reason=\"Invalid alias format error\")\n                return 1\n            alias, model = parts\n            models.MODEL_ALIASES[alias.strip()] = model.strip()\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::18",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 762,
      "span_ids": [
        "main"
      ],
      "start_line": 751,
      "end_line": 855,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main(argv=None, input=None, output=None, force_git_root=None, return_coder=False):\n    # ... other code\n\n    if not args.model:\n        args.model = \"gpt-4o-2024-08-06\"\n        if os.environ.get(\"ANTHROPIC_API_KEY\"):\n            args.model = \"claude-3-5-sonnet-20241022\"\n\n    main_model = models.Model(\n        args.model,\n        weak_model=args.weak_model,\n        editor_model=args.editor_model,\n        editor_edit_format=args.editor_edit_format,\n    )\n\n    if args.copy_paste and args.edit_format is None:\n        if main_model.edit_format in (\"diff\", \"whole\"):\n            main_model.edit_format = \"editor-\" + main_model.edit_format\n\n    if args.verbose:\n        io.tool_output(\"Model metadata:\")\n        io.tool_output(json.dumps(main_model.info, indent=4))\n\n        io.tool_output(\"Model settings:\")\n        for attr in sorted(fields(ModelSettings), key=lambda x: x.name):\n            val = getattr(main_model, attr.name)\n            val = json.dumps(val, indent=4)\n            io.tool_output(f\"{attr.name}: {val}\")\n\n    lint_cmds = parse_lint_cmds(args.lint_cmd, io)\n    if lint_cmds is None:\n        analytics.event(\"exit\", reason=\"Invalid lint command format\")\n        return 1\n\n    if args.show_model_warnings:\n        problem = models.sanity_check_models(io, main_model)\n        if problem:\n            analytics.event(\"model warning\", main_model=main_model)\n            io.tool_output(\"You can skip this check with --no-show-model-warnings\")\n\n            try:\n                io.offer_url(urls.model_warnings, \"Open documentation url for more info?\")\n                io.tool_output()\n            except KeyboardInterrupt:\n                analytics.event(\"exit\", reason=\"Keyboard interrupt during model warnings\")\n                return 1\n\n    repo = None\n    if args.git:\n        try:\n            repo = GitRepo(\n                io,\n                fnames,\n                git_dname,\n                args.aiderignore,\n                models=main_model.commit_message_models(),\n                attribute_author=args.attribute_author,\n                attribute_committer=args.attribute_committer,\n                attribute_commit_message_author=args.attribute_commit_message_author,\n                attribute_commit_message_committer=args.attribute_commit_message_committer,\n                commit_prompt=args.commit_prompt,\n                subtree_only=args.subtree_only,\n            )\n        except FileNotFoundError:\n            pass\n\n    if not args.skip_sanity_check_repo:\n        if not sanity_check_repo(repo, io):\n            analytics.event(\"exit\", reason=\"Repository sanity check failed\")\n            return 1\n\n    if repo:\n        analytics.event(\"repo\", num_files=len(repo.get_tracked_files()))\n    else:\n        analytics.event(\"no-repo\")\n\n    commands = Commands(\n        io,\n        None,\n        voice_language=args.voice_language,\n        voice_input_device=args.voice_input_device,\n        voice_format=args.voice_format,\n        verify_ssl=args.verify_ssl,\n        args=args,\n        parser=parser,\n        verbose=args.verbose,\n        editor=args.editor,\n    )\n\n    summarizer = ChatSummary(\n        [main_model.weak_model, main_model],\n        args.max_chat_history_tokens or main_model.max_chat_history_tokens,\n    )\n\n    if args.cache_prompts and args.map_refresh == \"auto\":\n        args.map_refresh = \"files\"\n\n    if not main_model.streaming:\n        if args.stream:\n            io.tool_warning(\n                f\"Warning: Streaming is not supported by {main_model.name}. Disabling streaming.\"\n            )\n        args.stream = False\n\n    if args.map_tokens is None:\n        map_tokens = main_model.get_repo_map_tokens()\n    else:\n        map_tokens = args.map_tokens\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::19",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 839,
      "span_ids": [
        "main"
      ],
      "start_line": 857,
      "end_line": 976,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main(argv=None, input=None, output=None, force_git_root=None, return_coder=False):\n    # ... other code\n\n    try:\n        coder = Coder.create(\n            main_model=main_model,\n            edit_format=args.edit_format,\n            io=io,\n            repo=repo,\n            fnames=fnames,\n            read_only_fnames=read_only_fnames,\n            show_diffs=args.show_diffs,\n            auto_commits=args.auto_commits,\n            dirty_commits=args.dirty_commits,\n            dry_run=args.dry_run,\n            map_tokens=map_tokens,\n            verbose=args.verbose,\n            stream=args.stream,\n            use_git=args.git,\n            restore_chat_history=args.restore_chat_history,\n            auto_lint=args.auto_lint,\n            auto_test=args.auto_test,\n            lint_cmds=lint_cmds,\n            test_cmd=args.test_cmd,\n            commands=commands,\n            summarizer=summarizer,\n            analytics=analytics,\n            map_refresh=args.map_refresh,\n            cache_prompts=args.cache_prompts,\n            map_mul_no_files=args.map_multiplier_no_files,\n            num_cache_warming_pings=args.cache_keepalive_pings,\n            suggest_shell_commands=args.suggest_shell_commands,\n            chat_language=args.chat_language,\n            detect_urls=args.detect_urls,\n            auto_copy_context=args.copy_paste,\n        )\n    except UnknownEditFormat as err:\n        io.tool_error(str(err))\n        io.offer_url(urls.edit_formats, \"Open documentation about edit formats?\")\n        analytics.event(\"exit\", reason=\"Unknown edit format\")\n        return 1\n    except ValueError as err:\n        io.tool_error(str(err))\n        analytics.event(\"exit\", reason=\"ValueError during coder creation\")\n        return 1\n\n    if return_coder:\n        analytics.event(\"exit\", reason=\"Returning coder object\")\n        return coder\n\n    ignores = []\n    if git_root:\n        ignores.append(str(Path(git_root) / \".gitignore\"))\n    if args.aiderignore:\n        ignores.append(args.aiderignore)\n\n    if args.watch_files:\n        file_watcher = FileWatcher(\n            coder,\n            gitignores=ignores,\n            verbose=args.verbose,\n            analytics=analytics,\n            root=str(Path.cwd()) if args.subtree_only else None,\n        )\n        coder.file_watcher = file_watcher\n\n    if args.copy_paste:\n        analytics.event(\"copy-paste mode\")\n        ClipboardWatcher(coder.io, verbose=args.verbose)\n\n    coder.show_announcements()\n\n    if args.show_prompts:\n        coder.cur_messages += [\n            dict(role=\"user\", content=\"Hello!\"),\n        ]\n        messages = coder.format_messages().all_messages()\n        utils.show_messages(messages)\n        analytics.event(\"exit\", reason=\"Showed prompts\")\n        return\n\n    if args.lint:\n        coder.commands.cmd_lint(fnames=fnames)\n\n    if args.test:\n        if not args.test_cmd:\n            io.tool_error(\"No --test-cmd provided.\")\n            analytics.event(\"exit\", reason=\"No test command provided\")\n            return 1\n        coder.commands.cmd_test(args.test_cmd)\n        if io.placeholder:\n            coder.run(io.placeholder)\n\n    if args.commit:\n        if args.dry_run:\n            io.tool_output(\"Dry run enabled, skipping commit.\")\n        else:\n            coder.commands.cmd_commit()\n\n    if args.lint or args.test or args.commit:\n        analytics.event(\"exit\", reason=\"Completed lint/test/commit\")\n        return\n\n    if args.show_repo_map:\n        repo_map = coder.get_repo_map()\n        if repo_map:\n            io.tool_output(repo_map)\n        analytics.event(\"exit\", reason=\"Showed repo map\")\n        return\n\n    if args.apply:\n        content = io.read_text(args.apply)\n        if content is None:\n            analytics.event(\"exit\", reason=\"Failed to read apply content\")\n            return\n        coder.partial_response_content = content\n        coder.apply_updates()\n        analytics.event(\"exit\", reason=\"Applied updates\")\n        return\n\n    if args.apply_clipboard_edits:\n        args.edit_format = main_model.editor_edit_format\n        args.message = \"/paste\"\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::20",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 530,
      "span_ids": [
        "main"
      ],
      "start_line": 978,
      "end_line": 1049,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main(argv=None, input=None, output=None, force_git_root=None, return_coder=False):\n    # ... other code\n\n    if args.show_release_notes is True:\n        io.tool_output(f\"Opening release notes: {urls.release_notes}\")\n        io.tool_output()\n        webbrowser.open(urls.release_notes)\n    elif args.show_release_notes is None and is_first_run:\n        io.tool_output()\n        io.offer_url(\n            urls.release_notes,\n            \"Would you like to see what's new in this version?\",\n            allow_never=False,\n        )\n\n    if git_root and Path.cwd().resolve() != Path(git_root).resolve():\n        io.tool_warning(\n            \"Note: in-chat filenames are always relative to the git working dir, not the current\"\n            \" working dir.\"\n        )\n\n        io.tool_output(f\"Cur working dir: {Path.cwd()}\")\n        io.tool_output(f\"Git working dir: {git_root}\")\n\n    if args.load:\n        commands.cmd_load(args.load)\n\n    if args.message:\n        io.add_to_input_history(args.message)\n        io.tool_output()\n        try:\n            coder.run(with_message=args.message)\n        except SwitchCoder:\n            pass\n        analytics.event(\"exit\", reason=\"Completed --message\")\n        return\n\n    if args.message_file:\n        try:\n            message_from_file = io.read_text(args.message_file)\n            io.tool_output()\n            coder.run(with_message=message_from_file)\n        except FileNotFoundError:\n            io.tool_error(f\"Message file not found: {args.message_file}\")\n            analytics.event(\"exit\", reason=\"Message file not found\")\n            return 1\n        except IOError as e:\n            io.tool_error(f\"Error reading message file: {e}\")\n            analytics.event(\"exit\", reason=\"Message file IO error\")\n            return 1\n\n        analytics.event(\"exit\", reason=\"Completed --message-file\")\n        return\n\n    if args.exit:\n        analytics.event(\"exit\", reason=\"Exit flag set\")\n        return\n\n    analytics.event(\"cli session\", main_model=main_model, edit_format=main_model.edit_format)\n\n    while True:\n        try:\n            coder.run()\n            analytics.event(\"exit\", reason=\"Completed main CLI coder.run\")\n            return\n        except SwitchCoder as switch:\n            kwargs = dict(io=io, from_coder=coder)\n            kwargs.update(switch.kwargs)\n            if \"show_announcements\" in kwargs:\n                del kwargs[\"show_announcements\"]\n\n            coder = Coder.create(**kwargs)\n\n            if switch.kwargs.get(\"show_announcements\") is not False:\n                coder.show_announcements()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::21",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 329,
      "span_ids": [
        "is_first_run_of_new_version"
      ],
      "start_line": 1052,
      "end_line": 1092,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def is_first_run_of_new_version(io, verbose=False):\n    \"\"\"Check if this is the first run of a new version/executable combination\"\"\"\n    installs_file = Path.home() / \".aider\" / \"installs.json\"\n    key = (__version__, sys.executable)\n\n    # Never show notes for .dev versions\n    if \".dev\" in __version__:\n        return False\n\n    if verbose:\n        io.tool_output(\n            f\"Checking imports for version {__version__} and executable {sys.executable}\"\n        )\n        io.tool_output(f\"Installs file: {installs_file}\")\n\n    try:\n        if installs_file.exists():\n            with open(installs_file, \"r\") as f:\n                installs = json.load(f)\n            if verbose:\n                io.tool_output(\"Installs file exists and loaded\")\n        else:\n            installs = {}\n            if verbose:\n                io.tool_output(\"Installs file does not exist, creating new dictionary\")\n\n        is_first_run = str(key) not in installs\n\n        if is_first_run:\n            installs[str(key)] = True\n            installs_file.parent.mkdir(parents=True, exist_ok=True)\n            with open(installs_file, \"w\") as f:\n                json.dump(installs, f, indent=4)\n\n        return is_first_run\n\n    except Exception as e:\n        io.tool_warning(f\"Error checking version: {e}\")\n        if verbose:\n            io.tool_output(f\"Full exception details: {traceback.format_exc()}\")\n        return True  # Safer to assume it's a first run if we hit an error\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::22",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 217,
      "span_ids": [
        "check_and_load_imports"
      ],
      "start_line": 1095,
      "end_line": 1122,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def check_and_load_imports(io, is_first_run, verbose=False):\n    try:\n        if is_first_run:\n            if verbose:\n                io.tool_output(\n                    \"First run for this version and executable, loading imports synchronously\"\n                )\n            try:\n                load_slow_imports(swallow=False)\n            except Exception as err:\n                io.tool_error(str(err))\n                io.tool_output(\"Error loading required imports. Did you install aider properly?\")\n                io.offer_url(urls.install_properly, \"Open documentation url for more info?\")\n                sys.exit(1)\n\n            if verbose:\n                io.tool_output(\"Imports loaded and installs file updated\")\n        else:\n            if verbose:\n                io.tool_output(\"Not first run, loading imports in background thread\")\n            thread = threading.Thread(target=load_slow_imports)\n            thread.daemon = True\n            thread.start()\n\n    except Exception as e:\n        io.tool_warning(f\"Error in loading imports: {e}\")\n        if verbose:\n            io.tool_output(f\"Full exception details: {traceback.format_exc()}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/main.py::23",
    "metadata": {
      "file_path": "aider/main.py",
      "file_name": "main.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 137,
      "span_ids": [
        "load_slow_imports",
        "impl:5"
      ],
      "start_line": 1125,
      "end_line": 1144,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def load_slow_imports(swallow=True):\n    # These imports are deferred in various ways to\n    # improve startup time.\n    # This func is called either synchronously or in a thread\n    # depending on whether it's been run before for this version and executable.\n\n    try:\n        import httpx  # noqa: F401\n        import litellm  # noqa: F401\n        import networkx  # noqa: F401\n        import numpy  # noqa: F401\n    except Exception as e:\n        if not swallow:\n            raise e\n\n\nif __name__ == \"__main__\":\n    status = main()\n    sys.exit(status)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/mdstream.py::1",
    "metadata": {
      "file_path": "aider/mdstream.py",
      "file_name": "mdstream.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 218,
      "span_ids": [
        "docstring"
      ],
      "start_line": 1,
      "end_line": 46,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "#!/usr/bin/env python\n\nimport io\nimport time\n\nfrom rich.console import Console\nfrom rich.live import Live\nfrom rich.markdown import Markdown\nfrom rich.text import Text\n\nfrom aider.dump import dump  # noqa: F401\n\n_text_prefix = \"\"\"\n# Header\n\nLorem Ipsum is simply dummy text of the printing and typesetting industry.\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s,\nwhen an unknown printer took a galley of type and scrambled it to make a type\nspecimen book. It has survived not only five centuries, but also the leap into\nelectronic typesetting, remaining essentially unchanged. It was popularised in\nthe 1960s with the release of Letraset sheets containing Lorem Ipsum passages,\nand more recently with desktop publishing software like Aldus PageMaker\nincluding versions of Lorem Ipsum.\n\n\n\n## Sub header\n\n- List 1\n- List 2\n- List me\n- List you\n\n\n\n```python\n\"\"\"\n\n_text_suffix = \"\"\"\n```\n\n## Sub header too\n\nThe end.\n\n\"\"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/mdstream.py::2",
    "metadata": {
      "file_path": "aider/mdstream.py",
      "file_name": "mdstream.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 242,
      "span_ids": [
        "docstring",
        "MarkdownStream.__init__",
        "MarkdownStream"
      ],
      "start_line": 46,
      "end_line": 77,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "  # noqa: E501\n\n\nclass MarkdownStream:\n    \"\"\"Streaming markdown renderer that progressively displays content with a live updating window.\n\n    Uses rich.console and rich.live to render markdown content with smooth scrolling\n    and partial updates. Maintains a sliding window of visible content while streaming\n    in new markdown text.\n    \"\"\"\n\n    live = None  # Rich Live display instance\n    when = 0  # Timestamp of last update\n    min_delay = 1.0 / 20  # Minimum time between updates (20fps)\n    live_window = 6  # Number of lines to keep visible at bottom during streaming\n\n    def __init__(self, mdargs=None):\n        \"\"\"Initialize the markdown stream.\n\n        Args:\n            mdargs (dict, optional): Additional arguments to pass to rich Markdown renderer\n        \"\"\"\n        self.printed = []  # Stores lines that have already been printed\n\n        if mdargs:\n            self.mdargs = mdargs\n        else:\n            self.mdargs = dict()\n\n        # Initialize rich Live display with empty text\n        self.live = Live(Text(\"\"), refresh_per_second=1.0 / self.min_delay)\n        self.live.start()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/mdstream.py::3",
    "metadata": {
      "file_path": "aider/mdstream.py",
      "file_name": "mdstream.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 176,
      "span_ids": [
        "MarkdownStream.__del__",
        "MarkdownStream._render_markdown_to_lines"
      ],
      "start_line": 79,
      "end_line": 104,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class MarkdownStream:\n\n    def _render_markdown_to_lines(self, text):\n        \"\"\"Render markdown text to a list of lines.\n\n        Args:\n            text (str): Markdown text to render\n\n        Returns:\n            list: List of rendered lines with line endings preserved\n        \"\"\"\n        # Render the markdown to a string buffer\n        string_io = io.StringIO()\n        console = Console(file=string_io, force_terminal=True)\n        markdown = Markdown(text, **self.mdargs)\n        console.print(markdown)\n        output = string_io.getvalue()\n\n        # Split rendered output into lines\n        return output.splitlines(keepends=True)\n\n    def __del__(self):\n        \"\"\"Destructor to ensure Live display is properly cleaned up.\"\"\"\n        if self.live:\n            try:\n                self.live.stop()\n            except Exception:\n                pass  # Ignore any errors during cleanup\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/mdstream.py::4",
    "metadata": {
      "file_path": "aider/mdstream.py",
      "file_name": "mdstream.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 680,
      "span_ids": [
        "MarkdownStream.find_minimal_suffix",
        "impl:5",
        "MarkdownStream.update"
      ],
      "start_line": 106,
      "end_line": 194,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class MarkdownStream:\n\n    def update(self, text, final=False):\n        \"\"\"Update the displayed markdown content.\n\n        Args:\n            text (str): The markdown text received so far\n            final (bool): If True, this is the final update and we should clean up\n\n        Splits the output into \"stable\" older lines and the \"last few\" lines\n        which aren't considered stable. They may shift around as new chunks\n        are appended to the markdown text.\n\n        The stable lines emit to the console above the Live window.\n        The unstable lines emit into the Live window so they can be repainted.\n\n        Markdown going to the console works better in terminal scrollback buffers.\n        The live window doesn't play nice with terminal scrollback.\n        \"\"\"\n        now = time.time()\n        # Throttle updates to maintain smooth rendering\n        if not final and now - self.when < self.min_delay:\n            return\n        self.when = now\n\n        # Measure render time and adjust min_delay to maintain smooth rendering\n        start = time.time()\n        lines = self._render_markdown_to_lines(text)\n        render_time = time.time() - start\n\n        # Set min_delay to render time plus a small buffer\n        self.min_delay = min(max(render_time * 10, 1.0 / 20), 2)\n\n        num_lines = len(lines)\n\n        # How many lines have \"left\" the live window and are now considered stable?\n        # Or if final, consider all lines to be stable.\n        if not final:\n            num_lines -= self.live_window\n\n        # If we have stable content to display...\n        if final or num_lines > 0:\n            # How many stable lines do we need to newly show above the live window?\n            num_printed = len(self.printed)\n            show = num_lines - num_printed\n\n            # Skip if no new lines to show above live window\n            if show <= 0:\n                return\n\n            # Get the new lines and display them\n            show = lines[num_printed:num_lines]\n            show = \"\".join(show)\n            show = Text.from_ansi(show)\n            self.live.console.print(show)  # to the console above the live area\n\n            # Update our record of printed lines\n            self.printed = lines[:num_lines]\n\n        # Handle final update cleanup\n        if final:\n            self.live.update(Text(\"\"))\n            self.live.stop()\n            self.live = None\n            return\n\n        # Update the live window with remaining lines\n        rest = lines[num_lines:]\n        rest = \"\".join(rest)\n        rest = Text.from_ansi(rest)\n        self.live.update(rest)\n\n    def find_minimal_suffix(self, text, match_lines=50):\n        \"\"\"\n        Splits text into chunks on blank lines \"\\n\\n\".\n        \"\"\"\n\n\nif __name__ == \"__main__\":\n    with open(\"aider/io.py\", \"r\") as f:\n        code = f.read()\n    _text = _text_prefix + code + _text_suffix\n    _text = _text * 10\n\n    pm = MarkdownStream()\n    for i in range(6, len(_text), 5):\n        pm.update(_text[:i])\n        time.sleep(0.01)\n\n    pm.update(_text, final=True)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::1",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 733,
      "span_ids": [
        "imports",
        "impl:13"
      ],
      "start_line": 1,
      "end_line": 80,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import difflib\nimport json\nimport math\nimport os\nimport platform\nimport sys\nimport time\nfrom dataclasses import dataclass, fields\nfrom pathlib import Path\nfrom typing import Optional\n\nimport json5\nimport yaml\nfrom PIL import Image\n\nfrom aider.dump import dump  # noqa: F401\nfrom aider.llm import litellm\n\nDEFAULT_MODEL_NAME = \"gpt-4o\"\nANTHROPIC_BETA_HEADER = \"prompt-caching-2024-07-31,pdfs-2024-09-25\"\n\nOPENAI_MODELS = \"\"\"\ngpt-4\ngpt-4o\ngpt-4o-2024-05-13\ngpt-4-turbo-preview\ngpt-4-0314\ngpt-4-0613\ngpt-4-32k\ngpt-4-32k-0314\ngpt-4-32k-0613\ngpt-4-turbo\ngpt-4-turbo-2024-04-09\ngpt-4-1106-preview\ngpt-4-0125-preview\ngpt-4-vision-preview\ngpt-4-1106-vision-preview\ngpt-4o-mini\ngpt-4o-mini-2024-07-18\ngpt-3.5-turbo\ngpt-3.5-turbo-0301\ngpt-3.5-turbo-0613\ngpt-3.5-turbo-1106\ngpt-3.5-turbo-0125\ngpt-3.5-turbo-16k\ngpt-3.5-turbo-16k-0613\n\"\"\"\n\nOPENAI_MODELS = [ln.strip() for ln in OPENAI_MODELS.splitlines() if ln.strip()]\n\nANTHROPIC_MODELS = \"\"\"\nclaude-2\nclaude-2.1\nclaude-3-haiku-20240307\nclaude-3-5-haiku-20241022\nclaude-3-opus-20240229\nclaude-3-sonnet-20240229\nclaude-3-5-sonnet-20240620\nclaude-3-5-sonnet-20241022\n\"\"\"\n\nANTHROPIC_MODELS = [ln.strip() for ln in ANTHROPIC_MODELS.splitlines() if ln.strip()]\n\n# Mapping of model aliases to their canonical names\nMODEL_ALIASES = {\n    # Claude models\n    \"sonnet\": \"claude-3-5-sonnet-20241022\",\n    \"haiku\": \"claude-3-5-haiku-20241022\",\n    \"opus\": \"claude-3-opus-20240229\",\n    # GPT models\n    \"4\": \"gpt-4-0613\",\n    \"4o\": \"gpt-4o\",\n    \"4-turbo\": \"gpt-4-1106-preview\",\n    \"35turbo\": \"gpt-3.5-turbo\",\n    \"35-turbo\": \"gpt-3.5-turbo\",\n    \"3\": \"gpt-3.5-turbo\",\n    # Other models\n    \"deepseek\": \"deepseek/deepseek-chat\",\n    \"flash\": \"gemini/gemini-2.0-flash-exp\",\n}",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::2",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 220,
      "span_ids": [
        "ModelSettings",
        "impl:15"
      ],
      "start_line": 83,
      "end_line": 822,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "@dataclass\nclass ModelSettings:\n    # Model class needs to have each of these as well\n    name: str\n    edit_format: str = \"whole\"\n    weak_model_name: Optional[str] = None\n    use_repo_map: bool = False\n    send_undo_reply: bool = False\n    lazy: bool = False\n    reminder: str = \"user\"\n    examples_as_sys_msg: bool = False\n    extra_params: Optional[dict] = None\n    cache_control: bool = False\n    caches_by_default: bool = False\n    use_system_prompt: bool = True\n    use_temperature: bool = True\n    streaming: bool = True\n    editor_model_name: Optional[str] = None\n    editor_edit_format: Optional[str] = None\n\n\n# https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\n# https://platform.openai.com/docs/models/gpt-3-5-turbo\n# https://openai.com/pricing\n\nMODEL_SETTINGS =\n # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::3",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 506,
      "span_ids": [
        "ModelInfoManager._load_cache",
        "ModelInfoManager.__init__",
        "ModelInfoManager._update_cache",
        "ModelInfoManager.get_model_info",
        "ModelInfoManager",
        "ModelInfoManager.get_model_from_cached_json_db"
      ],
      "start_line": 825,
      "end_line": 900,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class ModelInfoManager:\n    MODEL_INFO_URL = (\n        \"https://raw.githubusercontent.com/BerriAI/litellm/main/\"\n        \"model_prices_and_context_window.json\"\n    )\n    CACHE_TTL = 60 * 60 * 24  # 24 hours\n\n    def __init__(self):\n        self.cache_dir = Path.home() / \".aider\" / \"caches\"\n        self.cache_file = self.cache_dir / \"model_prices_and_context_window.json\"\n        self.content = None\n        self._load_cache()\n\n    def _load_cache(self):\n        try:\n            self.cache_dir.mkdir(parents=True, exist_ok=True)\n            if self.cache_file.exists():\n                cache_age = time.time() - self.cache_file.stat().st_mtime\n                if cache_age < self.CACHE_TTL:\n                    self.content = json.loads(self.cache_file.read_text())\n        except OSError:\n            pass\n\n    def _update_cache(self):\n        try:\n            import requests\n\n            response = requests.get(self.MODEL_INFO_URL, timeout=5)\n            if response.status_code == 200:\n                self.content = response.json()\n                try:\n                    self.cache_file.write_text(json.dumps(self.content, indent=4))\n                except OSError:\n                    pass\n        except Exception as ex:\n            print(str(ex))\n            try:\n                # Save empty dict to cache file on failure\n                self.cache_file.write_text(\"{}\")\n            except OSError:\n                pass\n\n    def get_model_from_cached_json_db(self, model):\n        if not self.content:\n            self._update_cache()\n\n        if not self.content:\n            return dict()\n\n        info = self.content.get(model, dict())\n        if info:\n            return info\n\n        pieces = model.split(\"/\")\n        if len(pieces) == 2:\n            info = self.content.get(pieces[1])\n            if info and info.get(\"litellm_provider\") == pieces[0]:\n                return info\n\n        return dict()\n\n    def get_model_info(self, model):\n        cached_info = self.get_model_from_cached_json_db(model)\n\n        litellm_info = None\n        if litellm._lazy_module or not cached_info:\n            try:\n                litellm_info = litellm.get_model_info(model)\n            except Exception as ex:\n                if \"model_prices_and_context_window.json\" not in str(ex):\n                    print(str(ex))\n\n        if litellm_info:\n            return litellm_info\n\n        return cached_info",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::4",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 385,
      "span_ids": [
        "Model.get_model_info",
        "Model.__init__",
        "impl:17",
        "Model",
        "Model._copy_fields"
      ],
      "start_line": 903,
      "end_line": 952,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "model_info_manager = ModelInfoManager()\n\n\nclass Model(ModelSettings):\n    def __init__(self, model, weak_model=None, editor_model=None, editor_edit_format=None):\n        # Map any alias to its canonical name\n        model = MODEL_ALIASES.get(model, model)\n\n        self.name = model\n\n        self.max_chat_history_tokens = 1024\n        self.weak_model = None\n        self.editor_model = None\n\n        # Find the extra settings\n        self.extra_model_settings = next(\n            (ms for ms in MODEL_SETTINGS if ms.name == \"aider/extra_params\"), None\n        )\n\n        self.info = self.get_model_info(model)\n\n        # Are all needed keys/params available?\n        res = self.validate_environment()\n        self.missing_keys = res.get(\"missing_keys\")\n        self.keys_in_environment = res.get(\"keys_in_environment\")\n\n        max_input_tokens = self.info.get(\"max_input_tokens\") or 0\n        # Calculate max_chat_history_tokens as 1/16th of max_input_tokens,\n        # with minimum 1k and maximum 8k\n        self.max_chat_history_tokens = min(max(max_input_tokens / 16, 1024), 8192)\n\n        self.configure_model_settings(model)\n        if weak_model is False:\n            self.weak_model_name = None\n        else:\n            self.get_weak_model(weak_model)\n\n        if editor_model is False:\n            self.editor_model_name = None\n        else:\n            self.get_editor_model(editor_model, editor_edit_format)\n\n    def get_model_info(self, model):\n        return model_info_manager.get_model_info(model)\n\n    def _copy_fields(self, source):\n        \"\"\"Helper to copy fields from a ModelSettings instance to self\"\"\"\n        for field in fields(ModelSettings):\n            val = getattr(source, field.name)\n            setattr(self, field.name, val)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::5",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 243,
      "span_ids": [
        "Model.configure_model_settings"
      ],
      "start_line": 954,
      "end_line": 983,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Model(ModelSettings):\n\n    def configure_model_settings(self, model):\n        # Look for exact model match\n        exact_match = False\n        for ms in MODEL_SETTINGS:\n            # direct match, or match \"provider/<model>\"\n            if model == ms.name:\n                self._copy_fields(ms)\n                exact_match = True\n                break  # Continue to apply overrides\n\n        model = model.lower()\n\n        # If no exact match, try generic settings\n        if not exact_match:\n            self.apply_generic_model_settings(model)\n\n        # Apply override settings last if they exist\n        if self.extra_model_settings and self.extra_model_settings.extra_params:\n            # Initialize extra_params if it doesn't exist\n            if not self.extra_params:\n                self.extra_params = {}\n\n            # Deep merge the extra_params dicts\n            for key, value in self.extra_model_settings.extra_params.items():\n                if isinstance(value, dict) and isinstance(self.extra_params.get(key), dict):\n                    # For nested dicts, merge recursively\n                    self.extra_params[key] = {**self.extra_params[key], **value}\n                else:\n                    # For non-dict values, simply update\n                    self.extra_params[key] = value",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::6",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 472,
      "span_ids": [
        "Model.apply_generic_model_settings"
      ],
      "start_line": 985,
      "end_line": 1037,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Model(ModelSettings):\n\n    def apply_generic_model_settings(self, model):\n        if (\"llama3\" in model or \"llama-3\" in model) and \"70b\" in model:\n            self.edit_format = \"diff\"\n            self.use_repo_map = True\n            self.send_undo_reply = True\n            self.examples_as_sys_msg = True\n            return  # <--\n\n        if \"gpt-4-turbo\" in model or (\"gpt-4-\" in model and \"-preview\" in model):\n            self.edit_format = \"udiff\"\n            self.use_repo_map = True\n            self.send_undo_reply = True\n            return  # <--\n\n        if \"gpt-4\" in model or \"claude-3-opus\" in model:\n            self.edit_format = \"diff\"\n            self.use_repo_map = True\n            self.send_undo_reply = True\n            return  # <--\n\n        if \"gpt-3.5\" in model or \"gpt-4\" in model:\n            self.reminder = \"sys\"\n            return  # <--\n\n        if \"3.5-sonnet\" in model or \"3-5-sonnet\" in model:\n            self.edit_format = \"diff\"\n            self.use_repo_map = True\n            self.examples_as_sys_msg = True\n            self.reminder = \"user\"\n            return  # <--\n\n        if model.startswith(\"o1-\") or \"/o1-\" in model:\n            self.use_system_prompt = False\n            self.use_temperature = False\n            return  # <--\n\n        if (\n            \"qwen\" in model\n            and \"coder\" in model\n            and (\"2.5\" in model or \"2-5\" in model)\n            and \"32b\" in model\n        ):\n            self.edit_format = \"diff\"\n            self.editor_edit_format = \"editor-diff\"\n            self.use_repo_map = True\n            if model.startswith(\"ollama/\") or model.startswith(\"ollama_chat/\"):\n                self.extra_params = dict(num_ctx=8 * 1024)\n            return  # <--\n\n        # use the defaults\n        if self.edit_format == \"diff\":\n            self.use_repo_map = True\n            return  # <--\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::7",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 150,
      "span_ids": [
        "Model.commit_message_models",
        "Model.get_weak_model",
        "Model.__str__"
      ],
      "start_line": 1039,
      "end_line": 1062,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Model(ModelSettings):\n\n    def __str__(self):\n        return self.name\n\n    def get_weak_model(self, provided_weak_model_name):\n        # If weak_model_name is provided, override the model settings\n        if provided_weak_model_name:\n            self.weak_model_name = provided_weak_model_name\n\n        if not self.weak_model_name:\n            self.weak_model = self\n            return\n\n        if self.weak_model_name == self.name:\n            self.weak_model = self\n            return\n\n        self.weak_model = Model(\n            self.weak_model_name,\n            weak_model=False,\n        )\n        return self.weak_model\n\n    def commit_message_models(self):\n        return [self.weak_model, self]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::8",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 140,
      "span_ids": [
        "Model.get_editor_model"
      ],
      "start_line": 1064,
      "end_line": 1082,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Model(ModelSettings):\n\n    def get_editor_model(self, provided_editor_model_name, editor_edit_format):\n        # If editor_model_name is provided, override the model settings\n        if provided_editor_model_name:\n            self.editor_model_name = provided_editor_model_name\n        if editor_edit_format:\n            self.editor_edit_format = editor_edit_format\n\n        if not self.editor_model_name or self.editor_model_name == self.name:\n            self.editor_model = self\n        else:\n            self.editor_model = Model(\n                self.editor_model_name,\n                editor_model=False,\n            )\n\n        if not self.editor_edit_format:\n            self.editor_edit_format = self.editor_model.edit_format\n\n        return self.editor_model",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::9",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 149,
      "span_ids": [
        "Model.tokenizer",
        "Model.token_count"
      ],
      "start_line": 1084,
      "end_line": 1107,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Model(ModelSettings):\n\n    def tokenizer(self, text):\n        return litellm.encode(model=self.name, text=text)\n\n    def token_count(self, messages):\n        if type(messages) is list:\n            try:\n                return litellm.token_counter(model=self.name, messages=messages)\n            except Exception as err:\n                print(f\"Unable to count tokens: {err}\")\n                return 0\n\n        if not self.tokenizer:\n            return\n\n        if type(messages) is str:\n            msgs = messages\n        else:\n            msgs = json.dumps(messages)\n\n        try:\n            return len(self.tokenizer(msgs))\n        except Exception as err:\n            print(f\"Unable to count tokens: {err}\")\n            return 0",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::10",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 292,
      "span_ids": [
        "Model.token_count_for_image"
      ],
      "start_line": 1109,
      "end_line": 1138,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Model(ModelSettings):\n\n    def token_count_for_image(self, fname):\n        \"\"\"\n        Calculate the token cost for an image assuming high detail.\n        The token cost is determined by the size of the image.\n        :param fname: The filename of the image.\n        :return: The token cost for the image.\n        \"\"\"\n        width, height = self.get_image_size(fname)\n\n        # If the image is larger than 2048 in any dimension, scale it down to fit within 2048x2048\n        max_dimension = max(width, height)\n        if max_dimension > 2048:\n            scale_factor = 2048 / max_dimension\n            width = int(width * scale_factor)\n            height = int(height * scale_factor)\n\n        # Scale the image such that the shortest side is 768 pixels long\n        min_dimension = min(width, height)\n        scale_factor = 768 / min_dimension\n        width = int(width * scale_factor)\n        height = int(height * scale_factor)\n\n        # Calculate the number of 512x512 tiles needed to cover the image\n        tiles_width = math.ceil(width / 512)\n        tiles_height = math.ceil(height / 512)\n        num_tiles = tiles_width * tiles_height\n\n        # Each tile costs 170 tokens, and there's an additional fixed cost of 85 tokens\n        token_cost = num_tiles * 170 + 85\n        return token_cost",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::11",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 178,
      "span_ids": [
        "Model.get_image_size",
        "Model.fast_validate_environment"
      ],
      "start_line": 1140,
      "end_line": 1161,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Model(ModelSettings):\n\n    def get_image_size(self, fname):\n        \"\"\"\n        Retrieve the size of an image.\n        :param fname: The filename of the image.\n        :return: A tuple (width, height) representing the image size in pixels.\n        \"\"\"\n        with Image.open(fname) as img:\n            return img.size\n\n    def fast_validate_environment(self):\n        \"\"\"Fast path for common models. Avoids forcing litellm import.\"\"\"\n\n        model = self.name\n        if model in OPENAI_MODELS or model.startswith(\"openai/\"):\n            var = \"OPENAI_API_KEY\"\n        elif model in ANTHROPIC_MODELS or model.startswith(\"anthropic/\"):\n            var = \"ANTHROPIC_API_KEY\"\n        else:\n            return\n\n        if os.environ.get(var):\n            return dict(keys_in_environment=[var], missing_keys=[])",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::12",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 235,
      "span_ids": [
        "Model.get_repo_map_tokens",
        "Model.validate_environment"
      ],
      "start_line": 1163,
      "end_line": 1194,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Model(ModelSettings):\n\n    def validate_environment(self):\n        res = self.fast_validate_environment()\n        if res:\n            return res\n\n        # https://github.com/BerriAI/litellm/issues/3190\n\n        model = self.name\n        res = litellm.validate_environment(model)\n        if res[\"keys_in_environment\"]:\n            return res\n        if res[\"missing_keys\"]:\n            return res\n\n        provider = self.info.get(\"litellm_provider\", \"\").lower()\n        if provider == \"cohere_chat\":\n            return validate_variables([\"COHERE_API_KEY\"])\n        if provider == \"gemini\":\n            return validate_variables([\"GEMINI_API_KEY\"])\n        if provider == \"groq\":\n            return validate_variables([\"GROQ_API_KEY\"])\n\n        return res\n\n    def get_repo_map_tokens(self):\n        map_tokens = 1024\n        max_inp_tokens = self.info.get(\"max_input_tokens\")\n        if max_inp_tokens:\n            map_tokens = max_inp_tokens / 8\n            map_tokens = min(map_tokens, 4096)\n            map_tokens = max(map_tokens, 1024)\n        return map_tokens",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::13",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 188,
      "span_ids": [
        "register_models"
      ],
      "start_line": 1197,
      "end_line": 1223,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def register_models(model_settings_fnames):\n    files_loaded = []\n    for model_settings_fname in model_settings_fnames:\n        if not os.path.exists(model_settings_fname):\n            continue\n\n        if not Path(model_settings_fname).read_text().strip():\n            continue\n\n        try:\n            with open(model_settings_fname, \"r\") as model_settings_file:\n                model_settings_list = yaml.safe_load(model_settings_file)\n\n            for model_settings_dict in model_settings_list:\n                model_settings = ModelSettings(**model_settings_dict)\n                existing_model_settings = next(\n                    (ms for ms in MODEL_SETTINGS if ms.name == model_settings.name), None\n                )\n\n                if existing_model_settings:\n                    MODEL_SETTINGS.remove(existing_model_settings)\n                MODEL_SETTINGS.append(model_settings)\n        except Exception as e:\n            raise Exception(f\"Error loading model settings from {model_settings_fname}: {e}\")\n        files_loaded.append(model_settings_fname)\n\n    return files_loaded",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::14",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 142,
      "span_ids": [
        "register_litellm_models"
      ],
      "start_line": 1226,
      "end_line": 1248,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def register_litellm_models(model_fnames):\n    files_loaded = []\n    for model_fname in model_fnames:\n        if not os.path.exists(model_fname):\n            continue\n\n        try:\n            data = Path(model_fname).read_text()\n            if not data.strip():\n                continue\n            model_def = json5.loads(data)\n            if not model_def:\n                continue\n\n            # only load litellm if we have actual data\n            litellm._load_litellm()\n            litellm.register_model(model_def)\n        except Exception as e:\n            raise Exception(f\"Error loading model definition from {model_fname}: {e}\")\n\n        files_loaded.append(model_fname)\n\n    return files_loaded",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::15",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 184,
      "span_ids": [
        "validate_variables",
        "sanity_check_models"
      ],
      "start_line": 1251,
      "end_line": 1276,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def validate_variables(vars):\n    missing = []\n    for var in vars:\n        if var not in os.environ:\n            missing.append(var)\n    if missing:\n        return dict(keys_in_environment=False, missing_keys=missing)\n    return dict(keys_in_environment=True, missing_keys=missing)\n\n\ndef sanity_check_models(io, main_model):\n    problem_main = sanity_check_model(io, main_model)\n\n    problem_weak = None\n    if main_model.weak_model and main_model.weak_model is not main_model:\n        problem_weak = sanity_check_model(io, main_model.weak_model)\n\n    problem_editor = None\n    if (\n        main_model.editor_model\n        and main_model.editor_model is not main_model\n        and main_model.editor_model is not main_model.weak_model\n    ):\n        problem_editor = sanity_check_model(io, main_model.editor_model)\n\n    return problem_main or problem_weak or problem_editor",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::16",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 240,
      "span_ids": [
        "sanity_check_model"
      ],
      "start_line": 1279,
      "end_line": 1312,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def sanity_check_model(io, model):\n    show = False\n\n    if model.missing_keys:\n        show = True\n        io.tool_warning(f\"Warning: {model} expects these environment variables\")\n        for key in model.missing_keys:\n            value = os.environ.get(key, \"\")\n            status = \"Set\" if value else \"Not set\"\n            io.tool_output(f\"- {key}: {status}\")\n\n        if platform.system() == \"Windows\":\n            io.tool_output(\n                \"Note: You may need to restart your terminal or command prompt for `setx` to take\"\n                \" effect.\"\n            )\n\n    elif not model.keys_in_environment:\n        show = True\n        io.tool_warning(f\"Warning for {model}: Unknown which environment variables are required.\")\n\n    if not model.info:\n        show = True\n        io.tool_warning(\n            f\"Warning for {model}: Unknown context window size and costs, using sane defaults.\"\n        )\n\n        possible_matches = fuzzy_match_models(model.name)\n        if possible_matches:\n            io.tool_output(\"Did you mean one of these?\")\n            for match in possible_matches:\n                io.tool_output(f\"- {match}\")\n\n    return show",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::17",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 264,
      "span_ids": [
        "fuzzy_match_models"
      ],
      "start_line": 1315,
      "end_line": 1354,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def fuzzy_match_models(name):\n    name = name.lower()\n\n    chat_models = set()\n    for model, attrs in litellm.model_cost.items():\n        model = model.lower()\n        if attrs.get(\"mode\") != \"chat\":\n            continue\n        provider = attrs.get(\"litellm_provider\", \"\").lower()\n        if not provider:\n            continue\n        provider += \"/\"\n\n        if model.startswith(provider):\n            fq_model = model\n        else:\n            fq_model = provider + model\n\n        chat_models.add(fq_model)\n        chat_models.add(model)\n\n    chat_models = sorted(chat_models)\n    # exactly matching model\n    # matching_models = [\n    #    (fq,m) for fq,m in chat_models\n    #    if name == fq or name == m\n    # ]\n    # if matching_models:\n    #    return matching_models\n\n    # Check for model names containing the name\n    matching_models = [m for m in chat_models if name in m]\n    if matching_models:\n        return sorted(set(matching_models))\n\n    # Check for slight misspellings\n    models = set(chat_models)\n    matching_models = difflib.get_close_matches(name, models, n=3, cutoff=0.8)\n\n    return sorted(set(matching_models))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::18",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 138,
      "span_ids": [
        "print_matching_models",
        "get_model_settings_as_yaml"
      ],
      "start_line": 1357,
      "end_line": 1377,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def print_matching_models(io, search):\n    matches = fuzzy_match_models(search)\n    if matches:\n        io.tool_output(f'Models which match \"{search}\":')\n        for model in matches:\n            io.tool_output(f\"- {model}\")\n    else:\n        io.tool_output(f'No models match \"{search}\".')\n\n\ndef get_model_settings_as_yaml():\n    import yaml\n\n    model_settings_list = []\n    for ms in MODEL_SETTINGS:\n        model_settings_dict = {\n            field.name: getattr(ms, field.name) for field in fields(ModelSettings)\n        }\n        model_settings_list.append(model_settings_dict)\n\n    return yaml.dump(model_settings_list, default_flow_style=False)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/models.py::19",
    "metadata": {
      "file_path": "aider/models.py",
      "file_name": "models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 144,
      "span_ids": [
        "main",
        "impl:19"
      ],
      "start_line": 1380,
      "end_line": 1402,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main():\n    if len(sys.argv) < 2:\n        print(\"Usage: python models.py <model_name> or python models.py --yaml\")\n        sys.exit(1)\n\n    if sys.argv[1] == \"--yaml\":\n        yaml_string = get_model_settings_as_yaml()\n        print(yaml_string)\n    else:\n        model_name = sys.argv[1]\n        matching_models = fuzzy_match_models(model_name)\n\n        if matching_models:\n            print(f\"Matching models for '{model_name}':\")\n            for model in matching_models:\n                print(model)\n        else:\n            print(f\"No matching models found for '{model_name}'.\")\n\n\nif __name__ == \"__main__\":\n    main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/prompts.py::1",
    "metadata": {
      "file_path": "aider/prompts.py",
      "file_name": "prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 525,
      "span_ids": [
        "impl:11",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 63,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# flake8: noqa: E501\n\n\n# COMMIT\n\n# Conventional Commits text adapted from:\n# https://www.conventionalcommits.org/en/v1.0.0/#summary\ncommit_system = \"\"\"You are an expert software engineer that generates concise, \\\none-line Git commit messages based on the provided diffs.\nReview the provided context and diffs which are about to be committed to a git repo.\nReview the diffs carefully.\nGenerate a one-line commit message for those changes.\nThe commit message should be structured as follows: <type>: <description>\nUse these for <type>: fix, feat, build, chore, ci, docs, style, refactor, perf, test\n\nEnsure the commit message:\n- Starts with the appropriate prefix.\n- Is in the imperative mood (e.g., \\\"Add feature\\\" not \\\"Added feature\\\" or \\\"Adding feature\\\").\n- Does not exceed 72 characters.\n\nReply only with the one-line commit message, without any additional text, explanations, \\\nor line breaks.\n\"\"\"\n\n# COMMANDS\nundo_command_reply = (\n    \"I did `git reset --hard HEAD~1` to discard the last edits. Please wait for further\"\n    \" instructions before attempting that change again. Feel free to ask relevant questions about\"\n    \" why the changes were reverted.\"\n)\n\nadded_files = (\n    \"I added these files to the chat: {fnames}\\nLet me know if there are others we should add.\"\n)\n\n\nrun_output = \"\"\"I ran this command:\n\n{command}\n\nAnd got this output:\n\n{output}\n\"\"\"\n\n# CHAT HISTORY\nsummarize = \"\"\"*Briefly* summarize this partial conversation about programming.\nInclude less detail about older parts and more detail about the most recent messages.\nStart a new paragraph every time the topic changes!\n\nThis is only part of a longer conversation so *DO NOT* conclude the summary with language like \"Finally, ...\". Because the conversation continues after the summary.\nThe summary *MUST* include the function names, libraries, packages that are being discussed.\nThe summary *MUST* include the filenames that are being referenced by the assistant inside the ```...``` fenced code blocks!\nThe summaries *MUST NOT* include ```...``` fenced code blocks!\n\nPhrase the summary with the USER in first person, telling the ASSISTANT about the conversation.\nWrite *as* the user.\nThe user should refer to the assistant as *you*.\nStart the summary with \"I asked you...\".\n\"\"\"\n\nsummary_prefix = \"I spoke to you previously about a number of things.\\n\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repo.py::1",
    "metadata": {
      "file_path": "aider/repo.py",
      "file_name": "repo.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 143,
      "span_ids": [
        "imports"
      ],
      "start_line": 1,
      "end_line": 33,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import os\nimport time\nfrom pathlib import Path, PurePosixPath\n\ntry:\n    import git\n\n    ANY_GIT_ERROR = [\n        git.exc.ODBError,\n        git.exc.GitError,\n        git.exc.InvalidGitRepositoryError,\n    ]\nexcept ImportError:\n    git = None\n    ANY_GIT_ERROR = []\n\nimport pathspec\n\nfrom aider import prompts, utils\nfrom aider.sendchat import simple_send_with_retries\n\nfrom .dump import dump  # noqa: F401\n\nANY_GIT_ERROR += [\n    OSError,\n    IndexError,\n    BufferError,\n    TypeError,\n    ValueError,\n    AttributeError,\n    AssertionError,\n]\nANY_GIT_ERROR = tuple(ANY_GIT_ERROR)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repo.py::2",
    "metadata": {
      "file_path": "aider/repo.py",
      "file_name": "repo.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 471,
      "span_ids": [
        "GitRepo",
        "GitRepo.__init__"
      ],
      "start_line": 36,
      "end_line": 109,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GitRepo:\n    repo = None\n    aider_ignore_file = None\n    aider_ignore_spec = None\n    aider_ignore_ts = 0\n    aider_ignore_last_check = 0\n    subtree_only = False\n    ignore_file_cache = {}\n    git_repo_error = None\n\n    def __init__(\n        self,\n        io,\n        fnames,\n        git_dname,\n        aider_ignore_file=None,\n        models=None,\n        attribute_author=True,\n        attribute_committer=True,\n        attribute_commit_message_author=False,\n        attribute_commit_message_committer=False,\n        commit_prompt=None,\n        subtree_only=False,\n    ):\n        self.io = io\n        self.models = models\n\n        self.normalized_path = {}\n        self.tree_files = {}\n\n        self.attribute_author = attribute_author\n        self.attribute_committer = attribute_committer\n        self.attribute_commit_message_author = attribute_commit_message_author\n        self.attribute_commit_message_committer = attribute_commit_message_committer\n        self.commit_prompt = commit_prompt\n        self.subtree_only = subtree_only\n        self.ignore_file_cache = {}\n\n        if git_dname:\n            check_fnames = [git_dname]\n        elif fnames:\n            check_fnames = fnames\n        else:\n            check_fnames = [\".\"]\n\n        repo_paths = []\n        for fname in check_fnames:\n            fname = Path(fname)\n            fname = fname.resolve()\n\n            if not fname.exists() and fname.parent.exists():\n                fname = fname.parent\n\n            try:\n                repo_path = git.Repo(fname, search_parent_directories=True).working_dir\n                repo_path = utils.safe_abs_path(repo_path)\n                repo_paths.append(repo_path)\n            except ANY_GIT_ERROR:\n                pass\n\n        num_repos = len(set(repo_paths))\n\n        if num_repos == 0:\n            raise FileNotFoundError\n        if num_repos > 1:\n            self.io.tool_error(\"Files are in different git repos.\")\n            raise FileNotFoundError\n\n        # https://github.com/gitpython-developers/GitPython/issues/427\n        self.repo = git.Repo(repo_paths.pop(), odbt=git.GitDB)\n        self.root = utils.safe_abs_path(self.repo.working_tree_dir)\n\n        if aider_ignore_file:\n            self.aider_ignore_file = Path(aider_ignore_file)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repo.py::3",
    "metadata": {
      "file_path": "aider/repo.py",
      "file_name": "repo.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 566,
      "span_ids": [
        "GitRepo.commit"
      ],
      "start_line": 111,
      "end_line": 179,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GitRepo:\n\n    def commit(self, fnames=None, context=None, message=None, aider_edits=False):\n        if not fnames and not self.repo.is_dirty():\n            return\n\n        diffs = self.get_diffs(fnames)\n        if not diffs:\n            return\n\n        if message:\n            commit_message = message\n        else:\n            commit_message = self.get_commit_message(diffs, context)\n\n        if aider_edits and self.attribute_commit_message_author:\n            commit_message = \"aider: \" + commit_message\n        elif self.attribute_commit_message_committer:\n            commit_message = \"aider: \" + commit_message\n\n        if not commit_message:\n            commit_message = \"(no commit message provided)\"\n\n        full_commit_message = commit_message\n        # if context:\n        #    full_commit_message += \"\\n\\n# Aider chat conversation:\\n\\n\" + context\n\n        cmd = [\"-m\", full_commit_message, \"--no-verify\"]\n        if fnames:\n            fnames = [str(self.abs_root_path(fn)) for fn in fnames]\n            for fname in fnames:\n                try:\n                    self.repo.git.add(fname)\n                except ANY_GIT_ERROR as err:\n                    self.io.tool_error(f\"Unable to add {fname}: {err}\")\n            cmd += [\"--\"] + fnames\n        else:\n            cmd += [\"-a\"]\n\n        original_user_name = self.repo.config_reader().get_value(\"user\", \"name\")\n        original_committer_name_env = os.environ.get(\"GIT_COMMITTER_NAME\")\n        committer_name = f\"{original_user_name} (aider)\"\n\n        if self.attribute_committer:\n            os.environ[\"GIT_COMMITTER_NAME\"] = committer_name\n\n        if aider_edits and self.attribute_author:\n            original_auther_name_env = os.environ.get(\"GIT_AUTHOR_NAME\")\n            os.environ[\"GIT_AUTHOR_NAME\"] = committer_name\n\n        try:\n            self.repo.git.commit(cmd)\n            commit_hash = self.get_head_commit_sha(short=True)\n            self.io.tool_output(f\"Commit {commit_hash} {commit_message}\", bold=True)\n            return commit_hash, commit_message\n        except ANY_GIT_ERROR as err:\n            self.io.tool_error(f\"Unable to commit: {err}\")\n        finally:\n            # Restore the env\n\n            if self.attribute_committer:\n                if original_committer_name_env is not None:\n                    os.environ[\"GIT_COMMITTER_NAME\"] = original_committer_name_env\n                else:\n                    del os.environ[\"GIT_COMMITTER_NAME\"]\n\n            if aider_edits and self.attribute_author:\n                if original_auther_name_env is not None:\n                    os.environ[\"GIT_AUTHOR_NAME\"] = original_auther_name_env\n                else:\n                    del os.environ[\"GIT_AUTHOR_NAME\"]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repo.py::4",
    "metadata": {
      "file_path": "aider/repo.py",
      "file_name": "repo.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 267,
      "span_ids": [
        "GitRepo.get_rel_repo_dir",
        "GitRepo.get_commit_message"
      ],
      "start_line": 181,
      "end_line": 219,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GitRepo:\n\n    def get_rel_repo_dir(self):\n        try:\n            return os.path.relpath(self.repo.git_dir, os.getcwd())\n        except (ValueError, OSError):\n            return self.repo.git_dir\n\n    def get_commit_message(self, diffs, context):\n        diffs = \"# Diffs:\\n\" + diffs\n\n        content = \"\"\n        if context:\n            content += context + \"\\n\"\n        content += diffs\n\n        system_content = self.commit_prompt or prompts.commit_system\n        messages = [\n            dict(role=\"system\", content=system_content),\n            dict(role=\"user\", content=content),\n        ]\n\n        commit_message = None\n        for model in self.models:\n            num_tokens = model.token_count(messages)\n            max_tokens = model.info.get(\"max_input_tokens\") or 0\n            if max_tokens and num_tokens > max_tokens:\n                continue\n            commit_message = simple_send_with_retries(model, messages)\n            if commit_message:\n                break\n\n        if not commit_message:\n            self.io.tool_error(\"Failed to generate commit message!\")\n            return\n\n        commit_message = commit_message.strip()\n        if commit_message and commit_message[0] == '\"' and commit_message[-1] == '\"':\n            commit_message = commit_message[1:-1].strip()\n\n        return commit_message",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repo.py::5",
    "metadata": {
      "file_path": "aider/repo.py",
      "file_name": "repo.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 315,
      "span_ids": [
        "GitRepo.get_diffs",
        "GitRepo.diff_commits"
      ],
      "start_line": 221,
      "end_line": 269,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GitRepo:\n\n    def get_diffs(self, fnames=None):\n        # We always want diffs of index and working dir\n\n        current_branch_has_commits = False\n        try:\n            active_branch = self.repo.active_branch\n            try:\n                commits = self.repo.iter_commits(active_branch)\n                current_branch_has_commits = any(commits)\n            except ANY_GIT_ERROR:\n                pass\n        except (TypeError,) + ANY_GIT_ERROR:\n            pass\n\n        if not fnames:\n            fnames = []\n\n        diffs = \"\"\n        for fname in fnames:\n            if not self.path_in_repo(fname):\n                diffs += f\"Added {fname}\\n\"\n\n        try:\n            if current_branch_has_commits:\n                args = [\"HEAD\", \"--\"] + list(fnames)\n                diffs += self.repo.git.diff(*args)\n                return diffs\n\n            wd_args = [\"--\"] + list(fnames)\n            index_args = [\"--cached\"] + wd_args\n\n            diffs += self.repo.git.diff(*index_args)\n            diffs += self.repo.git.diff(*wd_args)\n\n            return diffs\n        except ANY_GIT_ERROR as err:\n            self.io.tool_error(f\"Unable to diff: {err}\")\n\n    def diff_commits(self, pretty, from_commit, to_commit):\n        args = []\n        if pretty:\n            args += [\"--color\"]\n        else:\n            args += [\"--color=never\"]\n\n        args += [from_commit, to_commit]\n        diffs = self.repo.git.diff(*args)\n\n        return diffs",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repo.py::6",
    "metadata": {
      "file_path": "aider/repo.py",
      "file_name": "repo.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 334,
      "span_ids": [
        "GitRepo.get_tracked_files"
      ],
      "start_line": 271,
      "end_line": 317,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GitRepo:\n\n    def get_tracked_files(self):\n        if not self.repo:\n            return []\n\n        try:\n            commit = self.repo.head.commit\n        except ValueError:\n            commit = None\n        except ANY_GIT_ERROR as err:\n            self.git_repo_error = err\n            self.io.tool_error(f\"Unable to list files in git repo: {err}\")\n            self.io.tool_output(\"Is your git repo corrupted?\")\n            return []\n\n        files = set()\n        if commit:\n            if commit in self.tree_files:\n                files = self.tree_files[commit]\n            else:\n                try:\n                    iterator = commit.tree.traverse()\n                    while True:\n                        try:\n                            blob = next(iterator)\n                            if blob.type == \"blob\":  # blob is a file\n                                files.add(blob.path)\n                        except IndexError:\n                            self.io.tool_warning(f\"GitRepo: read error skipping {blob.path}\")\n                            continue\n                        except StopIteration:\n                            break\n                except ANY_GIT_ERROR as err:\n                    self.git_repo_error = err\n                    self.io.tool_error(f\"Unable to list files in git repo: {err}\")\n                    self.io.tool_output(\"Is your git repo corrupted?\")\n                    return []\n                files = set(self.normalize_path(path) for path in files)\n                self.tree_files[commit] = set(files)\n\n        # Add staged files\n        index = self.repo.index\n        staged_files = [path for path, _ in index.entries.keys()]\n        files.update(self.normalize_path(path) for path in staged_files)\n\n        res = [fname for fname in files if not self.ignored_file(fname)]\n\n        return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repo.py::7",
    "metadata": {
      "file_path": "aider/repo.py",
      "file_name": "repo.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 230,
      "span_ids": [
        "GitRepo.normalize_path",
        "GitRepo.refresh_aider_ignore"
      ],
      "start_line": 319,
      "end_line": 350,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GitRepo:\n\n    def normalize_path(self, path):\n        orig_path = path\n        res = self.normalized_path.get(orig_path)\n        if res:\n            return res\n\n        path = str(Path(PurePosixPath((Path(self.root) / path).relative_to(self.root))))\n        self.normalized_path[orig_path] = path\n        return path\n\n    def refresh_aider_ignore(self):\n        if not self.aider_ignore_file:\n            return\n\n        current_time = time.time()\n        if current_time - self.aider_ignore_last_check < 1:\n            return\n\n        self.aider_ignore_last_check = current_time\n\n        if not self.aider_ignore_file.is_file():\n            return\n\n        mtime = self.aider_ignore_file.stat().st_mtime\n        if mtime != self.aider_ignore_ts:\n            self.aider_ignore_ts = mtime\n            self.ignore_file_cache = {}\n            lines = self.aider_ignore_file.read_text().splitlines()\n            self.aider_ignore_spec = pathspec.PathSpec.from_lines(\n                pathspec.patterns.GitWildMatchPattern,\n                lines,\n            )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repo.py::8",
    "metadata": {
      "file_path": "aider/repo.py",
      "file_name": "repo.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 288,
      "span_ids": [
        "GitRepo.ignored_file",
        "GitRepo.git_ignored_file",
        "GitRepo.ignored_file_raw"
      ],
      "start_line": 352,
      "end_line": 394,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GitRepo:\n\n    def git_ignored_file(self, path):\n        if not self.repo:\n            return\n        try:\n            if self.repo.ignored(path):\n                return True\n        except ANY_GIT_ERROR:\n            return False\n\n    def ignored_file(self, fname):\n        self.refresh_aider_ignore()\n\n        if fname in self.ignore_file_cache:\n            return self.ignore_file_cache[fname]\n\n        result = self.ignored_file_raw(fname)\n        self.ignore_file_cache[fname] = result\n        return result\n\n    def ignored_file_raw(self, fname):\n        if self.subtree_only:\n            try:\n                fname_path = Path(self.normalize_path(fname))\n                cwd_path = Path.cwd().resolve().relative_to(Path(self.root).resolve())\n            except ValueError:\n                # Issue #1524\n                # ValueError: 'C:\\\\dev\\\\squid-certbot' is not in the subpath of\n                # 'C:\\\\dev\\\\squid-certbot'\n                # Clearly, fname is not under cwd... so ignore it\n                return True\n\n            if cwd_path not in fname_path.parents and fname_path != cwd_path:\n                return True\n\n        if not self.aider_ignore_file or not self.aider_ignore_file.is_file():\n            return False\n\n        try:\n            fname = self.normalize_path(fname)\n        except ValueError:\n            return True\n\n        return self.aider_ignore_spec.match_file(fname)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repo.py::9",
    "metadata": {
      "file_path": "aider/repo.py",
      "file_name": "repo.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 191,
      "span_ids": [
        "GitRepo.get_dirty_files",
        "GitRepo.abs_root_path",
        "GitRepo.path_in_repo"
      ],
      "start_line": 396,
      "end_line": 424,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GitRepo:\n\n    def path_in_repo(self, path):\n        if not self.repo:\n            return\n        if not path:\n            return\n\n        tracked_files = set(self.get_tracked_files())\n        return self.normalize_path(path) in tracked_files\n\n    def abs_root_path(self, path):\n        res = Path(self.root) / path\n        return utils.safe_abs_path(res)\n\n    def get_dirty_files(self):\n        \"\"\"\n        Returns a list of all files which are dirty (not committed), either staged or in the working\n        directory.\n        \"\"\"\n        dirty_files = set()\n\n        # Get staged files\n        staged_files = self.repo.git.diff(\"--name-only\", \"--cached\").splitlines()\n        dirty_files.update(staged_files)\n\n        # Get unstaged files\n        unstaged_files = self.repo.git.diff(\"--name-only\").splitlines()\n        dirty_files.update(unstaged_files)\n\n        return list(dirty_files)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repo.py::10",
    "metadata": {
      "file_path": "aider/repo.py",
      "file_name": "repo.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 147,
      "span_ids": [
        "GitRepo.is_dirty",
        "GitRepo.get_head_commit_sha",
        "GitRepo.get_head_commit_message",
        "GitRepo.get_head_commit"
      ],
      "start_line": 426,
      "end_line": 451,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GitRepo:\n\n    def is_dirty(self, path=None):\n        if path and not self.path_in_repo(path):\n            return True\n\n        return self.repo.is_dirty(path=path)\n\n    def get_head_commit(self):\n        try:\n            return self.repo.head.commit\n        except (ValueError,) + ANY_GIT_ERROR:\n            return None\n\n    def get_head_commit_sha(self, short=False):\n        commit = self.get_head_commit()\n        if not commit:\n            return\n        if short:\n            return commit.hexsha[:7]\n        return commit.hexsha\n\n    def get_head_commit_message(self, default=None):\n        commit = self.get_head_commit()\n        if not commit:\n            return default\n        return commit.message",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repomap.py::1",
    "metadata": {
      "file_path": "aider/repomap.py",
      "file_name": "repomap.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 185,
      "span_ids": [
        "imports"
      ],
      "start_line": 1,
      "end_line": 31,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import colorsys\nimport math\nimport os\nimport random\nimport shutil\nimport sqlite3\nimport sys\nimport time\nimport warnings\nfrom collections import Counter, defaultdict, namedtuple\nfrom importlib import resources\nfrom pathlib import Path\n\nfrom diskcache import Cache\nfrom grep_ast import TreeContext, filename_to_lang\nfrom pygments.lexers import guess_lexer_for_filename\nfrom pygments.token import Token\nfrom tqdm import tqdm\n\nfrom aider.dump import dump\nfrom aider.special import filter_important_files\nfrom aider.utils import Spinner\n\n# tree_sitter is throwing a FutureWarning\nwarnings.simplefilter(\"ignore\", category=FutureWarning)\nfrom tree_sitter_languages import get_language, get_parser  # noqa: E402\n\nTag = namedtuple(\"Tag\", \"rel_fname fname line name kind\".split())\n\n\nSQLITE_ERRORS = (sqlite3.OperationalError, sqlite3.DatabaseError, OSError)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repomap.py::2",
    "metadata": {
      "file_path": "aider/repomap.py",
      "file_name": "repomap.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 371,
      "span_ids": [
        "RepoMap.token_count",
        "RepoMap",
        "RepoMap.__init__"
      ],
      "start_line": 34,
      "end_line": 94,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class RepoMap:\n    CACHE_VERSION = 3\n    TAGS_CACHE_DIR = f\".aider.tags.cache.v{CACHE_VERSION}\"\n\n    warned_files = set()\n\n    def __init__(\n        self,\n        map_tokens=1024,\n        root=None,\n        main_model=None,\n        io=None,\n        repo_content_prefix=None,\n        verbose=False,\n        max_context_window=None,\n        map_mul_no_files=8,\n        refresh=\"auto\",\n    ):\n        self.io = io\n        self.verbose = verbose\n        self.refresh = refresh\n\n        if not root:\n            root = os.getcwd()\n        self.root = root\n\n        self.load_tags_cache()\n        self.cache_threshold = 0.95\n\n        self.max_map_tokens = map_tokens\n        self.map_mul_no_files = map_mul_no_files\n        self.max_context_window = max_context_window\n\n        self.repo_content_prefix = repo_content_prefix\n\n        self.main_model = main_model\n\n        self.tree_cache = {}\n        self.tree_context_cache = {}\n        self.map_cache = {}\n        self.map_processing_time = 0\n        self.last_map = None\n\n        if self.verbose:\n            self.io.tool_output(\n                f\"RepoMap initialized with map_mul_no_files: {self.map_mul_no_files}\"\n            )\n\n    def token_count(self, text):\n        len_text = len(text)\n        if len_text < 200:\n            return self.main_model.token_count(text)\n\n        lines = text.splitlines(keepends=True)\n        num_lines = len(lines)\n        step = num_lines // 100 or 1\n        lines = lines[::step]\n        sample_text = \"\".join(lines)\n        sample_tokens = self.main_model.token_count(sample_text)\n        est_tokens = sample_tokens / len(sample_text) * len_text\n        return est_tokens",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repomap.py::3",
    "metadata": {
      "file_path": "aider/repomap.py",
      "file_name": "repomap.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 378,
      "span_ids": [
        "RepoMap.get_repo_map"
      ],
      "start_line": 96,
      "end_line": 160,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class RepoMap:\n\n    def get_repo_map(\n        self,\n        chat_files,\n        other_files,\n        mentioned_fnames=None,\n        mentioned_idents=None,\n        force_refresh=False,\n    ):\n        if self.max_map_tokens <= 0:\n            return\n        if not other_files:\n            return\n        if not mentioned_fnames:\n            mentioned_fnames = set()\n        if not mentioned_idents:\n            mentioned_idents = set()\n\n        max_map_tokens = self.max_map_tokens\n\n        # With no files in the chat, give a bigger view of the entire repo\n        padding = 4096\n        if max_map_tokens and self.max_context_window:\n            target = min(\n                int(max_map_tokens * self.map_mul_no_files),\n                self.max_context_window - padding,\n            )\n        else:\n            target = 0\n        if not chat_files and self.max_context_window and target > 0:\n            max_map_tokens = target\n\n        try:\n            files_listing = self.get_ranked_tags_map(\n                chat_files,\n                other_files,\n                max_map_tokens,\n                mentioned_fnames,\n                mentioned_idents,\n                force_refresh,\n            )\n        except RecursionError:\n            self.io.tool_error(\"Disabling repo map, git repo too large?\")\n            self.max_map_tokens = 0\n            return\n\n        if not files_listing:\n            return\n\n        if self.verbose:\n            num_tokens = self.token_count(files_listing)\n            self.io.tool_output(f\"Repo-map: {num_tokens / 1024:.1f} k-tokens\")\n\n        if chat_files:\n            other = \"other \"\n        else:\n            other = \"\"\n\n        if self.repo_content_prefix:\n            repo_content = self.repo_content_prefix.format(other=other)\n        else:\n            repo_content = \"\"\n\n        repo_content += files_listing\n\n        return repo_content",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repomap.py::4",
    "metadata": {
      "file_path": "aider/repomap.py",
      "file_name": "repomap.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 335,
      "span_ids": [
        "RepoMap.get_rel_fname",
        "RepoMap.tags_cache_error"
      ],
      "start_line": 162,
      "end_line": 208,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class RepoMap:\n\n    def get_rel_fname(self, fname):\n        try:\n            return os.path.relpath(fname, self.root)\n        except ValueError:\n            # Issue #1288: ValueError: path is on mount 'C:', start on mount 'D:'\n            # Just return the full fname.\n            return fname\n\n    def tags_cache_error(self, original_error=None):\n        \"\"\"Handle SQLite errors by trying to recreate cache, falling back to dict if needed\"\"\"\n\n        if self.verbose and original_error:\n            self.io.tool_warning(f\"Tags cache error: {str(original_error)}\")\n\n        if isinstance(getattr(self, \"TAGS_CACHE\", None), dict):\n            return\n\n        path = Path(self.root) / self.TAGS_CACHE_DIR\n\n        # Try to recreate the cache\n        try:\n            # Delete existing cache dir\n            if path.exists():\n                shutil.rmtree(path)\n\n            # Try to create new cache\n            new_cache = Cache(path)\n\n            # Test that it works\n            test_key = \"test\"\n            new_cache[test_key] = \"test\"\n            _ = new_cache[test_key]\n            del new_cache[test_key]\n\n            # If we got here, the new cache works\n            self.TAGS_CACHE = new_cache\n            return\n\n        except SQLITE_ERRORS as e:\n            # If anything goes wrong, warn and fall back to dict\n            self.io.tool_warning(\n                f\"Unable to use tags cache at {path}, falling back to memory cache\"\n            )\n            if self.verbose:\n                self.io.tool_warning(f\"Cache recreation error: {str(e)}\")\n\n        self.TAGS_CACHE = dict()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repomap.py::5",
    "metadata": {
      "file_path": "aider/repomap.py",
      "file_name": "repomap.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 348,
      "span_ids": [
        "RepoMap.load_tags_cache",
        "RepoMap.get_tags",
        "RepoMap.get_mtime",
        "RepoMap.save_tags_cache"
      ],
      "start_line": 210,
      "end_line": 257,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class RepoMap:\n\n    def load_tags_cache(self):\n        path = Path(self.root) / self.TAGS_CACHE_DIR\n        try:\n            self.TAGS_CACHE = Cache(path)\n        except SQLITE_ERRORS as e:\n            self.tags_cache_error(e)\n\n    def save_tags_cache(self):\n        pass\n\n    def get_mtime(self, fname):\n        try:\n            return os.path.getmtime(fname)\n        except FileNotFoundError:\n            self.io.tool_warning(f\"File not found error: {fname}\")\n\n    def get_tags(self, fname, rel_fname):\n        # Check if the file is in the cache and if the modification time has not changed\n        file_mtime = self.get_mtime(fname)\n        if file_mtime is None:\n            return []\n\n        cache_key = fname\n        try:\n            val = self.TAGS_CACHE.get(cache_key)  # Issue #1308\n        except SQLITE_ERRORS as e:\n            self.tags_cache_error(e)\n            val = self.TAGS_CACHE.get(cache_key)\n\n        if val is not None and val.get(\"mtime\") == file_mtime:\n            try:\n                return self.TAGS_CACHE[cache_key][\"data\"]\n            except SQLITE_ERRORS as e:\n                self.tags_cache_error(e)\n                return self.TAGS_CACHE[cache_key][\"data\"]\n\n        # miss!\n        data = list(self.get_tags_raw(fname, rel_fname))\n\n        # Update the cache\n        try:\n            self.TAGS_CACHE[cache_key] = {\"mtime\": file_mtime, \"data\": data}\n            self.save_tags_cache()\n        except SQLITE_ERRORS as e:\n            self.tags_cache_error(e)\n            self.TAGS_CACHE[cache_key] = {\"mtime\": file_mtime, \"data\": data}\n\n        return data",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repomap.py::6",
    "metadata": {
      "file_path": "aider/repomap.py",
      "file_name": "repomap.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 445,
      "span_ids": [
        "RepoMap.get_tags_raw"
      ],
      "start_line": 259,
      "end_line": 333,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class RepoMap:\n\n    def get_tags_raw(self, fname, rel_fname):\n        lang = filename_to_lang(fname)\n        if not lang:\n            return\n\n        try:\n            language = get_language(lang)\n            parser = get_parser(lang)\n        except Exception as err:\n            print(f\"Skipping file {fname}: {err}\")\n            return\n\n        query_scm = get_scm_fname(lang)\n        if not query_scm.exists():\n            return\n        query_scm = query_scm.read_text()\n\n        code = self.io.read_text(fname)\n        if not code:\n            return\n        tree = parser.parse(bytes(code, \"utf-8\"))\n\n        # Run the tags queries\n        query = language.query(query_scm)\n        captures = query.captures(tree.root_node)\n\n        captures = list(captures)\n\n        saw = set()\n        for node, tag in captures:\n            if tag.startswith(\"name.definition.\"):\n                kind = \"def\"\n            elif tag.startswith(\"name.reference.\"):\n                kind = \"ref\"\n            else:\n                continue\n\n            saw.add(kind)\n\n            result = Tag(\n                rel_fname=rel_fname,\n                fname=fname,\n                name=node.text.decode(\"utf-8\"),\n                kind=kind,\n                line=node.start_point[0],\n            )\n\n            yield result\n\n        if \"ref\" in saw:\n            return\n        if \"def\" not in saw:\n            return\n\n        # We saw defs, without any refs\n        # Some tags files only provide defs (cpp, for example)\n        # Use pygments to backfill refs\n\n        try:\n            lexer = guess_lexer_for_filename(fname, code)\n        except Exception:  # On Windows, bad ref to time.clock which is deprecated?\n            # self.io.tool_error(f\"Error lexing {fname}\")\n            return\n\n        tokens = list(lexer.get_tokens(code))\n        tokens = [token[1] for token in tokens if token[0] in Token.Name]\n\n        for token in tokens:\n            yield Tag(\n                rel_fname=rel_fname,\n                fname=fname,\n                name=token,\n                kind=\"ref\",\n                line=-1,\n            )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repomap.py::7",
    "metadata": {
      "file_path": "aider/repomap.py",
      "file_name": "repomap.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 845,
      "span_ids": [
        "RepoMap.get_ranked_tags"
      ],
      "start_line": 335,
      "end_line": 463,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class RepoMap:\n\n    def get_ranked_tags(\n        self, chat_fnames, other_fnames, mentioned_fnames, mentioned_idents, progress=None\n    ):\n        import networkx as nx\n\n        defines = defaultdict(set)\n        references = defaultdict(list)\n        definitions = defaultdict(set)\n\n        personalization = dict()\n\n        fnames = set(chat_fnames).union(set(other_fnames))\n        chat_rel_fnames = set()\n\n        fnames = sorted(fnames)\n\n        # Default personalization for unspecified files is 1/num_nodes\n        # https://networkx.org/documentation/stable/_modules/networkx/algorithms/link_analysis/pagerank_alg.html#pagerank\n        personalize = 100 / len(fnames)\n\n        try:\n            cache_size = len(self.TAGS_CACHE)\n        except SQLITE_ERRORS as e:\n            self.tags_cache_error(e)\n            cache_size = len(self.TAGS_CACHE)\n\n        if len(fnames) - cache_size > 100:\n            self.io.tool_output(\n                \"Initial repo scan can be slow in larger repos, but only happens once.\"\n            )\n            fnames = tqdm(fnames, desc=\"Scanning repo\")\n            showing_bar = True\n        else:\n            showing_bar = False\n\n        for fname in fnames:\n            if self.verbose:\n                self.io.tool_output(f\"Processing {fname}\")\n            if progress and not showing_bar:\n                progress()\n\n            try:\n                file_ok = Path(fname).is_file()\n            except OSError:\n                file_ok = False\n\n            if not file_ok:\n                if fname not in self.warned_files:\n                    self.io.tool_warning(f\"Repo-map can't include {fname}\")\n                    self.io.tool_output(\n                        \"Has it been deleted from the file system but not from git?\"\n                    )\n                    self.warned_files.add(fname)\n                continue\n\n            # dump(fname)\n            rel_fname = self.get_rel_fname(fname)\n\n            if fname in chat_fnames:\n                personalization[rel_fname] = personalize\n                chat_rel_fnames.add(rel_fname)\n\n            if rel_fname in mentioned_fnames:\n                personalization[rel_fname] = personalize\n\n            tags = list(self.get_tags(fname, rel_fname))\n            if tags is None:\n                continue\n\n            for tag in tags:\n                if tag.kind == \"def\":\n                    defines[tag.name].add(rel_fname)\n                    key = (rel_fname, tag.name)\n                    definitions[key].add(tag)\n\n                elif tag.kind == \"ref\":\n                    references[tag.name].append(rel_fname)\n\n        ##\n        # dump(defines)\n        # dump(references)\n        # dump(personalization)\n\n        if not references:\n            references = dict((k, list(v)) for k, v in defines.items())\n\n        idents = set(defines.keys()).intersection(set(references.keys()))\n\n        G = nx.MultiDiGraph()\n\n        for ident in idents:\n            if progress:\n                progress()\n\n            definers = defines[ident]\n            if ident in mentioned_idents:\n                mul = 10\n            elif ident.startswith(\"_\"):\n                mul = 0.1\n            else:\n                mul = 1\n\n            for referencer, num_refs in Counter(references[ident]).items():\n                for definer in definers:\n                    # dump(referencer, definer, num_refs, mul)\n                    # if referencer == definer:\n                    #    continue\n\n                    # scale down so high freq (low value) mentions don't dominate\n                    num_refs = math.sqrt(num_refs)\n\n                    G.add_edge(referencer, definer, weight=mul * num_refs, ident=ident)\n\n        if not references:\n            pass\n\n        if personalization:\n            pers_args = dict(personalization=personalization, dangling=personalization)\n        else:\n            pers_args = dict()\n\n        try:\n            ranked = nx.pagerank(G, weight=\"weight\", **pers_args)\n        except ZeroDivisionError:\n            # Issue #1536\n            try:\n                ranked = nx.pagerank(G, weight=\"weight\")\n            except ZeroDivisionError:\n                return []\n        # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repomap.py::8",
    "metadata": {
      "file_path": "aider/repomap.py",
      "file_name": "repomap.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 403,
      "span_ids": [
        "RepoMap.get_ranked_tags"
      ],
      "start_line": 465,
      "end_line": 506,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class RepoMap:\n\n    def get_ranked_tags(\n        self, chat_fnames, other_fnames, mentioned_fnames, mentioned_idents, progress=None\n    ):\n\n        # distribute the rank from each source node, across all of its out edges\n        ranked_definitions = defaultdict(float)\n        for src in G.nodes:\n            if progress:\n                progress()\n\n            src_rank = ranked[src]\n            total_weight = sum(data[\"weight\"] for _src, _dst, data in G.out_edges(src, data=True))\n            # dump(src, src_rank, total_weight)\n            for _src, dst, data in G.out_edges(src, data=True):\n                data[\"rank\"] = src_rank * data[\"weight\"] / total_weight\n                ident = data[\"ident\"]\n                ranked_definitions[(dst, ident)] += data[\"rank\"]\n\n        ranked_tags = []\n        ranked_definitions = sorted(\n            ranked_definitions.items(), reverse=True, key=lambda x: (x[1], x[0])\n        )\n\n        # dump(ranked_definitions)\n\n        for (fname, ident), rank in ranked_definitions:\n            # print(f\"{rank:.03f} {fname} {ident}\")\n            if fname in chat_rel_fnames:\n                continue\n            ranked_tags += list(definitions.get((fname, ident), []))\n\n        rel_other_fnames_without_tags = set(self.get_rel_fname(fname) for fname in other_fnames)\n\n        fnames_already_included = set(rt[0] for rt in ranked_tags)\n\n        top_rank = sorted([(rank, node) for (node, rank) in ranked.items()], reverse=True)\n        for rank, fname in top_rank:\n            if fname in rel_other_fnames_without_tags:\n                rel_other_fnames_without_tags.remove(fname)\n            if fname not in fnames_already_included:\n                ranked_tags.append((fname,))\n\n        for fname in rel_other_fnames_without_tags:\n            ranked_tags.append((fname,))\n\n        return ranked_tags",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repomap.py::9",
    "metadata": {
      "file_path": "aider/repomap.py",
      "file_name": "repomap.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 371,
      "span_ids": [
        "RepoMap.get_ranked_tags_map"
      ],
      "start_line": 508,
      "end_line": 559,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class RepoMap:\n\n    def get_ranked_tags_map(\n        self,\n        chat_fnames,\n        other_fnames=None,\n        max_map_tokens=None,\n        mentioned_fnames=None,\n        mentioned_idents=None,\n        force_refresh=False,\n    ):\n        # Create a cache key\n        cache_key = [\n            tuple(sorted(chat_fnames)) if chat_fnames else None,\n            tuple(sorted(other_fnames)) if other_fnames else None,\n            max_map_tokens,\n        ]\n\n        if self.refresh == \"auto\":\n            cache_key += [\n                tuple(sorted(mentioned_fnames)) if mentioned_fnames else None,\n                tuple(sorted(mentioned_idents)) if mentioned_idents else None,\n            ]\n        cache_key = tuple(cache_key)\n\n        use_cache = False\n        if not force_refresh:\n            if self.refresh == \"manual\" and self.last_map:\n                return self.last_map\n\n            if self.refresh == \"always\":\n                use_cache = False\n            elif self.refresh == \"files\":\n                use_cache = True\n            elif self.refresh == \"auto\":\n                use_cache = self.map_processing_time > 1.0\n\n            # Check if the result is in the cache\n            if use_cache and cache_key in self.map_cache:\n                return self.map_cache[cache_key]\n\n        # If not in cache or force_refresh is True, generate the map\n        start_time = time.time()\n        result = self.get_ranked_tags_map_uncached(\n            chat_fnames, other_fnames, max_map_tokens, mentioned_fnames, mentioned_idents\n        )\n        end_time = time.time()\n        self.map_processing_time = end_time - start_time\n\n        # Store the result in the cache\n        self.map_cache[cache_key] = result\n        self.last_map = result\n\n        return result",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repomap.py::10",
    "metadata": {
      "file_path": "aider/repomap.py",
      "file_name": "repomap.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 515,
      "span_ids": [
        "RepoMap.get_ranked_tags_map_uncached"
      ],
      "start_line": 561,
      "end_line": 634,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class RepoMap:\n\n    def get_ranked_tags_map_uncached(\n        self,\n        chat_fnames,\n        other_fnames=None,\n        max_map_tokens=None,\n        mentioned_fnames=None,\n        mentioned_idents=None,\n    ):\n        if not other_fnames:\n            other_fnames = list()\n        if not max_map_tokens:\n            max_map_tokens = self.max_map_tokens\n        if not mentioned_fnames:\n            mentioned_fnames = set()\n        if not mentioned_idents:\n            mentioned_idents = set()\n\n        spin = Spinner(\"Updating repo map\")\n\n        ranked_tags = self.get_ranked_tags(\n            chat_fnames,\n            other_fnames,\n            mentioned_fnames,\n            mentioned_idents,\n            progress=spin.step,\n        )\n\n        other_rel_fnames = sorted(set(self.get_rel_fname(fname) for fname in other_fnames))\n        special_fnames = filter_important_files(other_rel_fnames)\n        ranked_tags_fnames = set(tag[0] for tag in ranked_tags)\n        special_fnames = [fn for fn in special_fnames if fn not in ranked_tags_fnames]\n        special_fnames = [(fn,) for fn in special_fnames]\n\n        ranked_tags = special_fnames + ranked_tags\n\n        spin.step()\n\n        num_tags = len(ranked_tags)\n        lower_bound = 0\n        upper_bound = num_tags\n        best_tree = None\n        best_tree_tokens = 0\n\n        chat_rel_fnames = set(self.get_rel_fname(fname) for fname in chat_fnames)\n\n        self.tree_cache = dict()\n\n        middle = min(int(max_map_tokens // 25), num_tags)\n        while lower_bound <= upper_bound:\n            # dump(lower_bound, middle, upper_bound)\n\n            spin.step()\n\n            tree = self.to_tree(ranked_tags[:middle], chat_rel_fnames)\n            num_tokens = self.token_count(tree)\n\n            pct_err = abs(num_tokens - max_map_tokens) / max_map_tokens\n            ok_err = 0.15\n            if (num_tokens <= max_map_tokens and num_tokens > best_tree_tokens) or pct_err < ok_err:\n                best_tree = tree\n                best_tree_tokens = num_tokens\n\n                if pct_err < ok_err:\n                    break\n\n            if num_tokens < max_map_tokens:\n                lower_bound = middle + 1\n            else:\n                upper_bound = middle - 1\n\n            middle = int((lower_bound + upper_bound) // 2)\n\n        spin.end()\n        return best_tree",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repomap.py::11",
    "metadata": {
      "file_path": "aider/repomap.py",
      "file_name": "repomap.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 263,
      "span_ids": [
        "RepoMap:8",
        "RepoMap.render_tree"
      ],
      "start_line": 636,
      "end_line": 674,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class RepoMap:\n\n    tree_cache = dict()\n\n    def render_tree(self, abs_fname, rel_fname, lois):\n        mtime = self.get_mtime(abs_fname)\n        key = (rel_fname, tuple(sorted(lois)), mtime)\n\n        if key in self.tree_cache:\n            return self.tree_cache[key]\n\n        if (\n            rel_fname not in self.tree_context_cache\n            or self.tree_context_cache[rel_fname][\"mtime\"] != mtime\n        ):\n            code = self.io.read_text(abs_fname) or \"\"\n            if not code.endswith(\"\\n\"):\n                code += \"\\n\"\n\n            context = TreeContext(\n                rel_fname,\n                code,\n                color=False,\n                line_number=False,\n                child_context=False,\n                last_line=False,\n                margin=0,\n                mark_lois=False,\n                loi_pad=0,\n                # header_max=30,\n                show_top_of_file_parent_scope=False,\n            )\n            self.tree_context_cache[rel_fname] = {\"context\": context, \"mtime\": mtime}\n\n        context = self.tree_context_cache[rel_fname][\"context\"]\n        context.lines_of_interest = set()\n        context.add_lines_of_interest(lois)\n        context.add_context()\n        res = context.format()\n        self.tree_cache[key] = res\n        return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repomap.py::12",
    "metadata": {
      "file_path": "aider/repomap.py",
      "file_name": "repomap.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 286,
      "span_ids": [
        "RepoMap.to_tree"
      ],
      "start_line": 676,
      "end_line": 712,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class RepoMap:\n\n    def to_tree(self, tags, chat_rel_fnames):\n        if not tags:\n            return \"\"\n\n        cur_fname = None\n        cur_abs_fname = None\n        lois = None\n        output = \"\"\n\n        # add a bogus tag at the end so we trip the this_fname != cur_fname...\n        dummy_tag = (None,)\n        for tag in sorted(tags) + [dummy_tag]:\n            this_rel_fname = tag[0]\n            if this_rel_fname in chat_rel_fnames:\n                continue\n\n            # ... here ... to output the final real entry in the list\n            if this_rel_fname != cur_fname:\n                if lois is not None:\n                    output += \"\\n\"\n                    output += cur_fname + \":\\n\"\n                    output += self.render_tree(cur_abs_fname, cur_fname, lois)\n                    lois = None\n                elif cur_fname:\n                    output += \"\\n\" + cur_fname + \"\\n\"\n                if type(tag) is Tag:\n                    lois = []\n                    cur_abs_fname = tag.fname\n                cur_fname = this_rel_fname\n\n            if lois is not None:\n                lois.append(tag.line)\n\n        # truncate long lines, in case we get minified js or something else crazy\n        output = \"\\n\".join([line[:100] for line in output.splitlines()]) + \"\\n\"\n\n        return output",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repomap.py::13",
    "metadata": {
      "file_path": "aider/repomap.py",
      "file_name": "repomap.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 175,
      "span_ids": [
        "get_random_color",
        "get_scm_fname",
        "find_src_files"
      ],
      "start_line": 715,
      "end_line": 738,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def find_src_files(directory):\n    if not os.path.isdir(directory):\n        return [directory]\n\n    src_files = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            src_files.append(os.path.join(root, file))\n    return src_files\n\n\ndef get_random_color():\n    hue = random.random()\n    r, g, b = [int(x * 255) for x in colorsys.hsv_to_rgb(hue, 1, 0.75)]\n    res = f\"#{r:02x}{g:02x}{b:02x}\"\n    return res\n\n\ndef get_scm_fname(lang):\n    # Load the tags queries\n    try:\n        return resources.files(__package__).joinpath(\"queries\", f\"tree-sitter-{lang}-tags.scm\")\n    except KeyError:\n        return",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/repomap.py::14",
    "metadata": {
      "file_path": "aider/repomap.py",
      "file_name": "repomap.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 258,
      "span_ids": [
        "get_supported_languages_md",
        "impl:6"
      ],
      "start_line": 741,
      "end_line": 777,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_supported_languages_md():\n    from grep_ast.parsers import PARSERS\n\n    res = \"\"\"\n| Language | File extension | Repo map | Linter |\n|:--------:|:--------------:|:--------:|:------:|\n\"\"\"\n    data = sorted((lang, ex) for ex, lang in PARSERS.items())\n\n    for lang, ext in data:\n        fn = get_scm_fname(lang)\n        repo_map = \"\u2713\" if Path(fn).exists() else \"\"\n        linter_support = \"\u2713\"\n        res += f\"| {lang:20} | {ext:20} | {repo_map:^8} | {linter_support:^6} |\\n\"\n\n    res += \"\\n\"\n\n    return res\n\n\nif __name__ == \"__main__\":\n    fnames = sys.argv[1:]\n\n    chat_fnames = []\n    other_fnames = []\n    for fname in sys.argv[1:]:\n        if Path(fname).is_dir():\n            chat_fnames += find_src_files(fname)\n        else:\n            chat_fnames.append(fname)\n\n    rm = RepoMap(root=\".\")\n    repo_map = rm.get_ranked_tags_map(chat_fnames, other_fnames)\n\n    dump(len(repo_map))\n    print(repo_map)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/report.py::1",
    "metadata": {
      "file_path": "aider/report.py",
      "file_name": "report.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 191,
      "span_ids": [
        "get_os_info",
        "imports",
        "get_git_info",
        "get_python_info"
      ],
      "start_line": 1,
      "end_line": 34,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import os\nimport platform\nimport subprocess\nimport sys\nimport traceback\nimport urllib.parse\nimport webbrowser\n\nfrom aider import __version__\nfrom aider.urls import github_issues\nfrom aider.versioncheck import VERSION_CHECK_FNAME\n\nFENCE = \"`\" * 3\n\n\ndef get_python_info():\n    implementation = platform.python_implementation()\n    is_venv = sys.prefix != sys.base_prefix\n    return (\n        f\"Python implementation: {implementation}\\nVirtual environment:\"\n        f\" {'Yes' if is_venv else 'No'}\"\n    )\n\n\ndef get_os_info():\n    return f\"OS: {platform.system()} {platform.release()} ({platform.architecture()[0]})\"\n\n\ndef get_git_info():\n    try:\n        git_version = subprocess.check_output([\"git\", \"--version\"]).decode().strip()\n        return f\"Git version: {git_version}\"\n    except Exception:\n        return \"Git information unavailable\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/report.py::2",
    "metadata": {
      "file_path": "aider/report.py",
      "file_name": "report.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 443,
      "span_ids": [
        "report_github_issue"
      ],
      "start_line": 37,
      "end_line": 91,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def report_github_issue(issue_text, title=None, confirm=True):\n    \"\"\"\n    Compose a URL to open a new GitHub issue with the given text prefilled,\n    and attempt to launch it in the default web browser.\n\n    :param issue_text: The text of the issue to file\n    :param title: The title of the issue (optional)\n    :param confirm: Whether to ask for confirmation before opening the browser (default: True)\n    :return: None\n    \"\"\"\n    version_info = f\"Aider version: {__version__}\\n\"\n    python_version = f\"Python version: {sys.version.split()[0]}\\n\"\n    platform_info = f\"Platform: {platform.platform()}\\n\"\n    python_info = get_python_info() + \"\\n\"\n    os_info = get_os_info() + \"\\n\"\n    git_info = get_git_info() + \"\\n\"\n\n    system_info = (\n        version_info + python_version + platform_info + python_info + os_info + git_info + \"\\n\"\n    )\n\n    issue_text = system_info + issue_text\n    params = {\"body\": issue_text}\n    if title is None:\n        title = \"Bug report\"\n    params[\"title\"] = title\n    issue_url = f\"{github_issues}?{urllib.parse.urlencode(params)}\"\n\n    if confirm:\n        print(f\"\\n# {title}\\n\")\n        print(issue_text.strip())\n        print()\n        print(\"Please consider reporting this bug to help improve aider!\")\n        prompt = \"Open a GitHub Issue pre-filled with the above error in your browser? (Y/n) \"\n        confirmation = input(prompt).strip().lower()\n\n        yes = not confirmation or confirmation.startswith(\"y\")\n        if not yes:\n            return\n\n    print(\"Attempting to open the issue URL in your default web browser...\")\n    try:\n        if webbrowser.open(issue_url):\n            print(\"Browser window should be opened.\")\n    except Exception:\n        pass\n\n    if confirm:\n        print()\n        print()\n        print(\"You can also use this URL to file the GitHub Issue:\")\n        print()\n        print(issue_url)\n        print()\n        print()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/report.py::3",
    "metadata": {
      "file_path": "aider/report.py",
      "file_name": "report.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 469,
      "span_ids": [
        "exception_handler"
      ],
      "start_line": 94,
      "end_line": 154,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def exception_handler(exc_type, exc_value, exc_traceback):\n    # If it's a KeyboardInterrupt, just call the default handler\n    if issubclass(exc_type, KeyboardInterrupt):\n        return sys.__excepthook__(exc_type, exc_value, exc_traceback)\n\n    # We don't want any more exceptions\n    sys.excepthook = None\n\n    # Check if VERSION_CHECK_FNAME exists and delete it if so\n    try:\n        if VERSION_CHECK_FNAME.exists():\n            VERSION_CHECK_FNAME.unlink()\n    except Exception:\n        pass  # Swallow any errors\n\n    # Format the traceback\n    tb_lines = traceback.format_exception(exc_type, exc_value, exc_traceback)\n\n    # Replace full paths with basenames in the traceback\n    tb_lines_with_basenames = []\n    for line in tb_lines:\n        try:\n            if \"File \" in line:\n                parts = line.split('\"')\n                if len(parts) > 1:\n                    full_path = parts[1]\n                    basename = os.path.basename(full_path)\n                    line = line.replace(full_path, basename)\n        except Exception:\n            pass\n        tb_lines_with_basenames.append(line)\n\n    tb_text = \"\".join(tb_lines_with_basenames)\n\n    # Find the innermost frame\n    innermost_tb = exc_traceback\n    while innermost_tb.tb_next:\n        innermost_tb = innermost_tb.tb_next\n\n    # Get the filename and line number from the innermost frame\n    filename = innermost_tb.tb_frame.f_code.co_filename\n    line_number = innermost_tb.tb_lineno\n    try:\n        basename = os.path.basename(filename)\n    except Exception:\n        basename = filename\n\n    # Get the exception type name\n    exception_type = exc_type.__name__\n\n    # Prepare the issue text\n    issue_text = f\"An uncaught exception occurred:\\n\\n{FENCE}\\n{tb_text}\\n{FENCE}\"\n\n    # Prepare the title\n    title = f\"Uncaught {exception_type} in {basename} line {line_number}\"\n\n    # Report the issue\n    report_github_issue(issue_text, title=title)\n\n    # Call the default exception handler\n    sys.__excepthook__(exc_type, exc_value, exc_traceback)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/report.py::4",
    "metadata": {
      "file_path": "aider/report.py",
      "file_name": "report.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 256,
      "span_ids": [
        "impl:3",
        "report_uncaught_exceptions",
        "dummy_function1",
        "main"
      ],
      "start_line": 157,
      "end_line": 201,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def report_uncaught_exceptions():\n    \"\"\"\n    Set up the global exception handler to report uncaught exceptions.\n    \"\"\"\n    sys.excepthook = exception_handler\n\n\ndef dummy_function1():\n    def dummy_function2():\n        def dummy_function3():\n            raise ValueError(\"boo\")\n\n        dummy_function3()\n\n    dummy_function2()\n\n\ndef main():\n    report_uncaught_exceptions()\n\n    dummy_function1()\n\n    title = None\n    if len(sys.argv) > 2:\n        # Use the first command-line argument as the title and the second as the issue text\n        title = sys.argv[1]\n        issue_text = sys.argv[2]\n    elif len(sys.argv) > 1:\n        # Use the first command-line argument as the issue text\n        issue_text = sys.argv[1]\n    else:\n        # Read from stdin if no argument is provided\n        print(\"Enter the issue title (optional, press Enter to skip):\")\n        title = input().strip()\n        if not title:\n            title = None\n        print(\"Enter the issue text (Ctrl+D to finish):\")\n        issue_text = sys.stdin.read().strip()\n\n    report_github_issue(issue_text, title)\n\n\nif __name__ == \"__main__\":\n    main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "resources/__init__.py::1",
    "metadata": {
      "file_path": "aider/resources/__init__.py",
      "file_name": "__init__.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 33,
      "span_ids": [
        "docstring"
      ],
      "start_line": 1,
      "end_line": 4,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# This ensures that importlib_resources.files(\"aider.resources\")\n# doesn't raise ImportError, even if there are no other files in this\n# dir.\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/run_cmd.py::1",
    "metadata": {
      "file_path": "aider/run_cmd.py",
      "file_name": "run_cmd.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 228,
      "span_ids": [
        "imports",
        "run_cmd",
        "get_windows_parent_process_name"
      ],
      "start_line": 1,
      "end_line": 39,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import os\nimport platform\nimport subprocess\nimport sys\nfrom io import BytesIO\n\nimport pexpect\nimport psutil\n\n\ndef run_cmd(command, verbose=False, error_print=None, cwd=None):\n    try:\n        if sys.stdin.isatty() and hasattr(pexpect, \"spawn\") and platform.system() != \"Windows\":\n            return run_cmd_pexpect(command, verbose, cwd)\n\n        return run_cmd_subprocess(command, verbose, cwd)\n    except OSError as e:\n        error_message = f\"Error occurred while running command '{command}': {str(e)}\"\n        if error_print is None:\n            print(error_message)\n        else:\n            error_print(error_message)\n        return 1, error_message\n\n\ndef get_windows_parent_process_name():\n    try:\n        current_process = psutil.Process()\n        while True:\n            parent = current_process.parent()\n            if parent is None:\n                break\n            parent_name = parent.name().lower()\n            if parent_name in [\"powershell.exe\", \"cmd.exe\"]:\n                return parent_name\n            current_process = parent\n        return None\n    except Exception:\n        return None",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/run_cmd.py::2",
    "metadata": {
      "file_path": "aider/run_cmd.py",
      "file_name": "run_cmd.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 297,
      "span_ids": [
        "run_cmd_subprocess"
      ],
      "start_line": 42,
      "end_line": 86,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def run_cmd_subprocess(command, verbose=False, cwd=None, encoding=sys.stdout.encoding):\n    if verbose:\n        print(\"Using run_cmd_subprocess:\", command)\n\n    try:\n        shell = os.environ.get(\"SHELL\", \"/bin/sh\")\n        parent_process = None\n\n        # Determine the appropriate shell\n        if platform.system() == \"Windows\":\n            parent_process = get_windows_parent_process_name()\n            if parent_process == \"powershell.exe\":\n                command = f\"powershell -Command {command}\"\n\n        if verbose:\n            print(\"Running command:\", command)\n            print(\"SHELL:\", shell)\n            if platform.system() == \"Windows\":\n                print(\"Parent process:\", parent_process)\n\n        process = subprocess.Popen(\n            command,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True,\n            shell=True,\n            encoding=encoding,\n            errors=\"replace\",\n            bufsize=0,  # Set bufsize to 0 for unbuffered output\n            universal_newlines=True,\n            cwd=cwd,\n        )\n\n        output = []\n        while True:\n            chunk = process.stdout.read(1)\n            if not chunk:\n                break\n            print(chunk, end=\"\", flush=True)  # Print the chunk in real-time\n            output.append(chunk)  # Store the chunk for later use\n\n        process.wait()\n        return process.returncode, \"\".join(output)\n    except Exception as e:\n        return 1, str(e)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/run_cmd.py::3",
    "metadata": {
      "file_path": "aider/run_cmd.py",
      "file_name": "run_cmd.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 358,
      "span_ids": [
        "run_cmd_pexpect"
      ],
      "start_line": 89,
      "end_line": 133,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def run_cmd_pexpect(command, verbose=False, cwd=None):\n    \"\"\"\n    Run a shell command interactively using pexpect, capturing all output.\n\n    :param command: The command to run as a string.\n    :param verbose: If True, print output in real-time.\n    :return: A tuple containing (exit_status, output)\n    \"\"\"\n    if verbose:\n        print(\"Using run_cmd_pexpect:\", command)\n\n    output = BytesIO()\n\n    def output_callback(b):\n        output.write(b)\n        return b\n\n    try:\n        # Use the SHELL environment variable, falling back to /bin/sh if not set\n        shell = os.environ.get(\"SHELL\", \"/bin/sh\")\n        if verbose:\n            print(\"With shell:\", shell)\n\n        if os.path.exists(shell):\n            # Use the shell from SHELL environment variable\n            if verbose:\n                print(\"Running pexpect.spawn with shell:\", shell)\n            child = pexpect.spawn(shell, args=[\"-i\", \"-c\", command], encoding=\"utf-8\", cwd=cwd)\n        else:\n            # Fall back to spawning the command directly\n            if verbose:\n                print(\"Running pexpect.spawn without shell.\")\n            child = pexpect.spawn(command, encoding=\"utf-8\", cwd=cwd)\n\n        # Transfer control to the user, capturing output\n        child.interact(output_filter=output_callback)\n\n        # Wait for the command to finish and get the exit status\n        child.close()\n        return child.exitstatus, output.getvalue().decode(\"utf-8\", errors=\"replace\")\n\n    except (pexpect.ExceptionPexpect, TypeError, ValueError) as e:\n        error_msg = f\"Error running command {command}: {e}\"\n        return 1, error_msg",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/scrape.py::1",
    "metadata": {
      "file_path": "aider/scrape.py",
      "file_name": "scrape.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 374,
      "span_ids": [
        "docstring",
        "install_playwright"
      ],
      "start_line": 1,
      "end_line": 66,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "#!/usr/bin/env python\n\nimport re\nimport sys\n\nimport pypandoc\n\nfrom aider import __version__, urls, utils\nfrom aider.dump import dump  # noqa: F401\n\naider_user_agent = f\"Aider/{__version__} +{urls.website}\"\n\n# Playwright is nice because it has a simple way to install dependencies on most\n# platforms.\n\n\ndef install_playwright(io):\n    try:\n        from playwright.sync_api import sync_playwright\n\n        has_pip = True\n    except ImportError:\n        has_pip = False\n\n    try:\n        with sync_playwright() as p:\n            p.chromium.launch()\n            has_chromium = True\n    except Exception:\n        has_chromium = False\n\n    if has_pip and has_chromium:\n        return True\n\n    pip_cmd = utils.get_pip_install([\"aider-chat[playwright]\"])\n    chromium_cmd = \"-m playwright install --with-deps chromium\"\n    chromium_cmd = [sys.executable] + chromium_cmd.split()\n\n    cmds = \"\"\n    if not has_pip:\n        cmds += \" \".join(pip_cmd) + \"\\n\"\n    if not has_chromium:\n        cmds += \" \".join(chromium_cmd) + \"\\n\"\n\n    text = f\"\"\"For the best web scraping, install Playwright:\n\n{cmds}\nSee {urls.enable_playwright} for more info.\n\"\"\"\n\n    io.tool_output(text)\n    if not io.confirm_ask(\"Install playwright?\", default=\"y\"):\n        return\n\n    if not has_pip:\n        success, output = utils.run_install(pip_cmd)\n        if not success:\n            io.tool_error(output)\n            return\n\n    success, output = utils.run_install(chromium_cmd)\n    if not success:\n        io.tool_error(output)\n        return\n\n    return True",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/scrape.py::2",
    "metadata": {
      "file_path": "aider/scrape.py",
      "file_name": "scrape.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 124,
      "span_ids": [
        "Scraper.__init__",
        "Scraper"
      ],
      "start_line": 69,
      "end_line": 86,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Scraper:\n    pandoc_available = None\n    playwright_available = None\n    playwright_instructions_shown = False\n\n    # Public API...\n    def __init__(self, print_error=None, playwright_available=None, verify_ssl=True):\n        \"\"\"\n        `print_error` - a function to call to print error/debug info.\n        `verify_ssl` - if False, disable SSL certificate verification when scraping.\n        \"\"\"\n        if print_error:\n            self.print_error = print_error\n        else:\n            self.print_error = print\n\n        self.playwright_available = playwright_available\n        self.verify_ssl = verify_ssl",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/scrape.py::3",
    "metadata": {
      "file_path": "aider/scrape.py",
      "file_name": "scrape.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 183,
      "span_ids": [
        "Scraper.scrape"
      ],
      "start_line": 88,
      "end_line": 112,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Scraper:\n\n    def scrape(self, url):\n        \"\"\"\n        Scrape a url and turn it into readable markdown if it's HTML.\n        If it's plain text or non-HTML, return it as-is.\n\n        `url` - the URL to scrape.\n        \"\"\"\n\n        if self.playwright_available:\n            content, mime_type = self.scrape_with_playwright(url)\n        else:\n            content, mime_type = self.scrape_with_httpx(url)\n\n        if not content:\n            self.print_error(f\"Failed to retrieve content from {url}\")\n            return None\n\n        # Check if the content is HTML based on MIME type or content\n        if (mime_type and mime_type.startswith(\"text/html\")) or (\n            mime_type is None and self.looks_like_html(content)\n        ):\n            self.try_pandoc()\n            content = self.html_to_markdown(content)\n\n        return content",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/scrape.py::4",
    "metadata": {
      "file_path": "aider/scrape.py",
      "file_name": "scrape.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 121,
      "span_ids": [
        "Scraper.looks_like_html"
      ],
      "start_line": 114,
      "end_line": 132,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Scraper:\n\n    def looks_like_html(self, content):\n        \"\"\"\n        Check if the content looks like HTML.\n        \"\"\"\n        if isinstance(content, str):\n            # Check for common HTML tags\n            html_patterns = [\n                r\"<!DOCTYPE\\s+html\",\n                r\"<html\",\n                r\"<head\",\n                r\"<body\",\n                r\"<div\",\n                r\"<p>\",\n                r\"<a\\s+href=\",\n            ]\n            return any(re.search(pattern, content, re.IGNORECASE) for pattern in html_patterns)\n        return False\n\n    # Internals...\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/scrape.py::5",
    "metadata": {
      "file_path": "aider/scrape.py",
      "file_name": "scrape.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 360,
      "span_ids": [
        "Scraper.scrape_with_playwright"
      ],
      "start_line": 133,
      "end_line": 181,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Scraper:\n    def scrape_with_playwright(self, url):\n        import playwright  # noqa: F401\n        from playwright.sync_api import Error as PlaywrightError\n        from playwright.sync_api import TimeoutError as PlaywrightTimeoutError\n        from playwright.sync_api import sync_playwright\n\n        with sync_playwright() as p:\n            try:\n                browser = p.chromium.launch()\n            except Exception as e:\n                self.playwright_available = False\n                self.print_error(str(e))\n                return None, None\n\n            try:\n                context = browser.new_context(ignore_https_errors=not self.verify_ssl)\n                page = context.new_page()\n\n                user_agent = page.evaluate(\"navigator.userAgent\")\n                user_agent = user_agent.replace(\"Headless\", \"\")\n                user_agent = user_agent.replace(\"headless\", \"\")\n                user_agent += \" \" + aider_user_agent\n\n                page.set_extra_http_headers({\"User-Agent\": user_agent})\n\n                response = None\n                try:\n                    response = page.goto(url, wait_until=\"networkidle\", timeout=5000)\n                except PlaywrightTimeoutError:\n                    self.print_error(f\"Timeout while loading {url}\")\n                except PlaywrightError as e:\n                    self.print_error(f\"Error navigating to {url}: {str(e)}\")\n                    return None, None\n\n                try:\n                    content = page.content()\n                    mime_type = None\n                    if response:\n                        content_type = response.header_value(\"content-type\")\n                        if content_type:\n                            mime_type = content_type.split(\";\")[0]\n                except PlaywrightError as e:\n                    self.print_error(f\"Error retrieving page content: {str(e)}\")\n                    content = None\n                    mime_type = None\n            finally:\n                browser.close()\n\n        return content, mime_type",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/scrape.py::6",
    "metadata": {
      "file_path": "aider/scrape.py",
      "file_name": "scrape.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 147,
      "span_ids": [
        "Scraper.scrape_with_httpx"
      ],
      "start_line": 183,
      "end_line": 198,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Scraper:\n\n    def scrape_with_httpx(self, url):\n        import httpx\n\n        headers = {\"User-Agent\": f\"Mozilla./5.0 ({aider_user_agent})\"}\n        try:\n            with httpx.Client(\n                headers=headers, verify=self.verify_ssl, follow_redirects=True\n            ) as client:\n                response = client.get(url)\n                response.raise_for_status()\n                return response.text, response.headers.get(\"content-type\", \"\").split(\";\")[0]\n        except httpx.HTTPError as http_err:\n            self.print_error(f\"HTTP error occurred: {http_err}\")\n        except Exception as err:\n            self.print_error(f\"An error occurred: {err}\")\n        return None, None",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/scrape.py::7",
    "metadata": {
      "file_path": "aider/scrape.py",
      "file_name": "scrape.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 246,
      "span_ids": [
        "Scraper.html_to_markdown",
        "Scraper.try_pandoc"
      ],
      "start_line": 200,
      "end_line": 239,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Scraper:\n\n    def try_pandoc(self):\n        if self.pandoc_available:\n            return\n\n        try:\n            pypandoc.get_pandoc_version()\n            self.pandoc_available = True\n            return\n        except OSError:\n            pass\n\n        try:\n            pypandoc.download_pandoc(delete_installer=True)\n        except Exception as err:\n            self.print_error(f\"Unable to install pandoc: {err}\")\n            return\n\n        self.pandoc_available = True\n\n    def html_to_markdown(self, page_source):\n        from bs4 import BeautifulSoup\n\n        soup = BeautifulSoup(page_source, \"html.parser\")\n        soup = slimdown_html(soup)\n        page_source = str(soup)\n\n        if not self.pandoc_available:\n            return page_source\n\n        try:\n            md = pypandoc.convert_text(page_source, \"markdown\", format=\"html\")\n        except OSError:\n            return page_source\n\n        md = re.sub(r\"</div>\", \"      \", md)\n        md = re.sub(r\"<div>\", \"     \", md)\n\n        md = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", md)\n\n        return md",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/scrape.py::8",
    "metadata": {
      "file_path": "aider/scrape.py",
      "file_name": "scrape.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 184,
      "span_ids": [
        "impl:3",
        "slimdown_html",
        "main"
      ],
      "start_line": 242,
      "end_line": 274,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def slimdown_html(soup):\n    for svg in soup.find_all(\"svg\"):\n        svg.decompose()\n\n    if soup.img:\n        soup.img.decompose()\n\n    for tag in soup.find_all(href=lambda x: x and x.startswith(\"data:\")):\n        tag.decompose()\n\n    for tag in soup.find_all(src=lambda x: x and x.startswith(\"data:\")):\n        tag.decompose()\n\n    for tag in soup.find_all(True):\n        for attr in list(tag.attrs):\n            if attr != \"href\":\n                tag.attrs.pop(attr, None)\n\n    return soup\n\n\ndef main(url):\n    scraper = Scraper()\n    content = scraper.scrape(url)\n    print(content)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        print(\"Usage: python playw.py <URL>\")\n        sys.exit(1)\n    main(sys.argv[1])",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/sendchat.py::1",
    "metadata": {
      "file_path": "aider/sendchat.py",
      "file_name": "sendchat.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 306,
      "span_ids": [
        "send_completion",
        "imports"
      ],
      "start_line": 1,
      "end_line": 56,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import hashlib\nimport json\nimport time\n\nfrom aider.dump import dump  # noqa: F401\nfrom aider.exceptions import LiteLLMExceptions\nfrom aider.llm import litellm\n\n# from diskcache import Cache\n\n\nCACHE_PATH = \"~/.aider.send.cache.v1\"\nCACHE = None\n# CACHE = Cache(CACHE_PATH)\n\nRETRY_TIMEOUT = 60\n\n\ndef send_completion(\n    model_name,\n    messages,\n    functions,\n    stream,\n    temperature=0,\n    extra_params=None,\n):\n    kwargs = dict(\n        model=model_name,\n        messages=messages,\n        stream=stream,\n    )\n    if temperature is not None:\n        kwargs[\"temperature\"] = temperature\n\n    if functions is not None:\n        function = functions[0]\n        kwargs[\"tools\"] = [dict(type=\"function\", function=function)]\n        kwargs[\"tool_choice\"] = {\"type\": \"function\", \"function\": {\"name\": function[\"name\"]}}\n\n    if extra_params is not None:\n        kwargs.update(extra_params)\n\n    key = json.dumps(kwargs, sort_keys=True).encode()\n\n    # Generate SHA1 hash of kwargs and append it to chat_completion_call_hashes\n    hash_object = hashlib.sha1(key)\n\n    if not stream and CACHE is not None and key in CACHE:\n        return hash_object, CACHE[key]\n\n    res = litellm.completion(**kwargs)\n\n    if not stream and CACHE is not None:\n        CACHE[key] = res\n\n    return hash_object, res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/sendchat.py::2",
    "metadata": {
      "file_path": "aider/sendchat.py",
      "file_name": "sendchat.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 254,
      "span_ids": [
        "simple_send_with_retries"
      ],
      "start_line": 59,
      "end_line": 99,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def simple_send_with_retries(model, messages):\n    litellm_ex = LiteLLMExceptions()\n\n    retry_delay = 0.125\n    while True:\n        try:\n            kwargs = {\n                \"model_name\": model.name,\n                \"messages\": messages,\n                \"functions\": None,\n                \"stream\": False,\n                \"temperature\": None if not model.use_temperature else 0,\n                \"extra_params\": model.extra_params,\n            }\n\n            _hash, response = send_completion(**kwargs)\n            if not response or not hasattr(response, \"choices\") or not response.choices:\n                return None\n            return response.choices[0].message.content\n        except litellm_ex.exceptions_tuple() as err:\n            ex_info = litellm_ex.get_ex_info(err)\n\n            print(str(err))\n            if ex_info.description:\n                print(ex_info.description)\n\n            should_retry = ex_info.retry\n            if should_retry:\n                retry_delay *= 2\n                if retry_delay > RETRY_TIMEOUT:\n                    should_retry = False\n\n            if not should_retry:\n                return None\n\n            print(f\"Retrying in {retry_delay:.1f} seconds...\")\n            time.sleep(retry_delay)\n            continue\n        except AttributeError:\n            return None",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/special.py::1",
    "metadata": {
      "file_path": "aider/special.py",
      "file_name": "special.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 1027,
      "span_ids": [
        "imports"
      ],
      "start_line": 1,
      "end_line": 179,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import os\n\nROOT_IMPORTANT_FILES = [\n    # Version Control\n    \".gitignore\",\n    \".gitattributes\",\n    # Documentation\n    \"README\",\n    \"README.md\",\n    \"README.txt\",\n    \"README.rst\",\n    \"CONTRIBUTING\",\n    \"CONTRIBUTING.md\",\n    \"CONTRIBUTING.txt\",\n    \"CONTRIBUTING.rst\",\n    \"LICENSE\",\n    \"LICENSE.md\",\n    \"LICENSE.txt\",\n    \"CHANGELOG\",\n    \"CHANGELOG.md\",\n    \"CHANGELOG.txt\",\n    \"CHANGELOG.rst\",\n    \"SECURITY\",\n    \"SECURITY.md\",\n    \"SECURITY.txt\",\n    \"CODEOWNERS\",\n    # Package Management and Dependencies\n    \"requirements.txt\",\n    \"Pipfile\",\n    \"Pipfile.lock\",\n    \"pyproject.toml\",\n    \"setup.py\",\n    \"setup.cfg\",\n    \"package.json\",\n    \"package-lock.json\",\n    \"yarn.lock\",\n    \"npm-shrinkwrap.json\",\n    \"Gemfile\",\n    \"Gemfile.lock\",\n    \"composer.json\",\n    \"composer.lock\",\n    \"pom.xml\",\n    \"build.gradle\",\n    \"build.sbt\",\n    \"go.mod\",\n    \"go.sum\",\n    \"Cargo.toml\",\n    \"Cargo.lock\",\n    \"mix.exs\",\n    \"rebar.config\",\n    \"project.clj\",\n    \"Podfile\",\n    \"Cartfile\",\n    \"dub.json\",\n    \"dub.sdl\",\n    # Configuration and Settings\n    \".env\",\n    \".env.example\",\n    \".editorconfig\",\n    \"tsconfig.json\",\n    \"jsconfig.json\",\n    \".babelrc\",\n    \"babel.config.js\",\n    \".eslintrc\",\n    \".eslintignore\",\n    \".prettierrc\",\n    \".stylelintrc\",\n    \"tslint.json\",\n    \".pylintrc\",\n    \".flake8\",\n    \".rubocop.yml\",\n    \".scalafmt.conf\",\n    \".dockerignore\",\n    \".gitpod.yml\",\n    \"sonar-project.properties\",\n    \"renovate.json\",\n    \"dependabot.yml\",\n    \".pre-commit-config.yaml\",\n    \"mypy.ini\",\n    \"tox.ini\",\n    \".yamllint\",\n    \"pyrightconfig.json\",\n    # Build and Compilation\n    \"webpack.config.js\",\n    \"rollup.config.js\",\n    \"parcel.config.js\",\n    \"gulpfile.js\",\n    \"Gruntfile.js\",\n    \"build.xml\",\n    \"build.boot\",\n    \"project.json\",\n    \"build.cake\",\n    \"MANIFEST.in\",\n    # Testing\n    \"pytest.ini\",\n    \"phpunit.xml\",\n    \"karma.conf.js\",\n    \"jest.config.js\",\n    \"cypress.json\",\n    \".nycrc\",\n    \".nycrc.json\",\n    # CI/CD\n    \".travis.yml\",\n    \".gitlab-ci.yml\",\n    \"Jenkinsfile\",\n    \"azure-pipelines.yml\",\n    \"bitbucket-pipelines.yml\",\n    \"appveyor.yml\",\n    \"circle.yml\",\n    \".circleci/config.yml\",\n    \".github/dependabot.yml\",\n    \"codecov.yml\",\n    \".coveragerc\",\n    # Docker and Containers\n    \"Dockerfile\",\n    \"docker-compose.yml\",\n    \"docker-compose.override.yml\",\n    # Cloud and Serverless\n    \"serverless.yml\",\n    \"firebase.json\",\n    \"now.json\",\n    \"netlify.toml\",\n    \"vercel.json\",\n    \"app.yaml\",\n    \"terraform.tf\",\n    \"main.tf\",\n    \"cloudformation.yaml\",\n    \"cloudformation.json\",\n    \"ansible.cfg\",\n    \"kubernetes.yaml\",\n    \"k8s.yaml\",\n    # Database\n    \"schema.sql\",\n    \"liquibase.properties\",\n    \"flyway.conf\",\n    # Framework-specific\n    \"next.config.js\",\n    \"nuxt.config.js\",\n    \"vue.config.js\",\n    \"angular.json\",\n    \"gatsby-config.js\",\n    \"gridsome.config.js\",\n    # API Documentation\n    \"swagger.yaml\",\n    \"swagger.json\",\n    \"openapi.yaml\",\n    \"openapi.json\",\n    # Development environment\n    \".nvmrc\",\n    \".ruby-version\",\n    \".python-version\",\n    \"Vagrantfile\",\n    # Quality and metrics\n    \".codeclimate.yml\",\n    \"codecov.yml\",\n    # Documentation\n    \"mkdocs.yml\",\n    \"_config.yml\",\n    \"book.toml\",\n    \"readthedocs.yml\",\n    \".readthedocs.yaml\",\n    # Package registries\n    \".npmrc\",\n    \".yarnrc\",\n    # Linting and formatting\n    \".isort.cfg\",\n    \".markdownlint.json\",\n    \".markdownlint.yaml\",\n    # Security\n    \".bandit\",\n    \".secrets.baseline\",\n    # Misc\n    \".pypirc\",\n    \".gitkeep\",\n    \".npmignore\",\n]\n\n\n# Normalize the lists once\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/special.py::2",
    "metadata": {
      "file_path": "aider/special.py",
      "file_name": "special.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 180,
      "span_ids": [
        "filter_important_files",
        "impl:3",
        "is_important"
      ],
      "start_line": 180,
      "end_line": 203,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "NORMALIZED_ROOT_IMPORTANT_FILES = set(os.path.normpath(path) for path in ROOT_IMPORTANT_FILES)\n\n\ndef is_important(file_path):\n    file_name = os.path.basename(file_path)\n    dir_name = os.path.normpath(os.path.dirname(file_path))\n    normalized_path = os.path.normpath(file_path)\n\n    # Check for GitHub Actions workflow files\n    if dir_name == os.path.normpath(\".github/workflows\") and file_name.endswith(\".yml\"):\n        return True\n\n    return normalized_path in NORMALIZED_ROOT_IMPORTANT_FILES\n\n\ndef filter_important_files(file_paths):\n    \"\"\"\n    Filter a list of file paths to return only those that are commonly important in codebases.\n\n    :param file_paths: List of file paths to check\n    :return: List of file paths that match important file patterns\n    \"\"\"\n    return list(filter(is_important, file_paths))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/urls.py::1",
    "metadata": {
      "file_path": "aider/urls.py",
      "file_name": "urls.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 282,
      "span_ids": [
        "impl"
      ],
      "start_line": 1,
      "end_line": 17,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "website = \"https://aider.chat/\"\nadd_all_files = \"https://aider.chat/docs/faq.html#how-can-i-add-all-the-files-to-the-chat\"\nedit_errors = \"https://aider.chat/docs/troubleshooting/edit-errors.html\"\ngit = \"https://aider.chat/docs/git.html\"\nenable_playwright = \"https://aider.chat/docs/install/optional.html#enable-playwright\"\nfavicon = \"https://aider.chat/assets/icons/favicon-32x32.png\"\nmodel_warnings = \"https://aider.chat/docs/llms/warnings.html\"\ntoken_limits = \"https://aider.chat/docs/troubleshooting/token-limits.html\"\nllms = \"https://aider.chat/docs/llms.html\"\nlarge_repos = \"https://aider.chat/docs/faq.html#can-i-use-aider-in-a-large-mono-repo\"\ngithub_issues = \"https://github.com/Aider-AI/aider/issues/new\"\ngit_index_version = \"https://github.com/Aider-AI/aider/issues/211\"\ninstall_properly = \"https://aider.chat/docs/troubleshooting/imports.html\"\nanalytics = \"https://aider.chat/docs/more/analytics.html\"\nrelease_notes = \"https://aider.chat/HISTORY.html#release-notes\"\nedit_formats = \"https://aider.chat/docs/more/edit-formats.html\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/utils.py::1",
    "metadata": {
      "file_path": "aider/utils.py",
      "file_name": "utils.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 219,
      "span_ids": [
        "IgnorantTemporaryDirectory.__getattr__",
        "IgnorantTemporaryDirectory.__enter__",
        "IgnorantTemporaryDirectory.cleanup",
        "IgnorantTemporaryDirectory.__init__",
        "IgnorantTemporaryDirectory.__exit__",
        "imports",
        "IgnorantTemporaryDirectory"
      ],
      "start_line": 1,
      "end_line": 36,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import itertools\nimport os\nimport platform\nimport shlex\nimport subprocess\nimport sys\nimport tempfile\nimport time\nfrom pathlib import Path\n\nfrom aider.dump import dump  # noqa: F401\n\nIMAGE_EXTENSIONS = {\".png\", \".jpg\", \".jpeg\", \".gif\", \".bmp\", \".tiff\", \".webp\", \".pdf\"}\n\n\nclass IgnorantTemporaryDirectory:\n    def __init__(self):\n        if sys.version_info >= (3, 10):\n            self.temp_dir = tempfile.TemporaryDirectory(ignore_cleanup_errors=True)\n        else:\n            self.temp_dir = tempfile.TemporaryDirectory()\n\n    def __enter__(self):\n        return self.temp_dir.__enter__()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.cleanup()\n\n    def cleanup(self):\n        try:\n            self.temp_dir.cleanup()\n        except (OSError, PermissionError, RecursionError):\n            pass  # Ignore errors (Windows and potential recursion)\n\n    def __getattr__(self, item):\n        return getattr(self.temp_dir, item)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/utils.py::2",
    "metadata": {
      "file_path": "aider/utils.py",
      "file_name": "utils.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 124,
      "span_ids": [
        "ChdirTemporaryDirectory",
        "ChdirTemporaryDirectory.__enter__",
        "ChdirTemporaryDirectory.__init__",
        "ChdirTemporaryDirectory.__exit__"
      ],
      "start_line": 39,
      "end_line": 59,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class ChdirTemporaryDirectory(IgnorantTemporaryDirectory):\n    def __init__(self):\n        try:\n            self.cwd = os.getcwd()\n        except FileNotFoundError:\n            self.cwd = None\n\n        super().__init__()\n\n    def __enter__(self):\n        res = super().__enter__()\n        os.chdir(Path(self.temp_dir.name).resolve())\n        return res\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.cwd:\n            try:\n                os.chdir(self.cwd)\n            except FileNotFoundError:\n                pass\n        super().__exit__(exc_type, exc_val, exc_tb)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/utils.py::3",
    "metadata": {
      "file_path": "aider/utils.py",
      "file_name": "utils.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 313,
      "span_ids": [
        "GitTemporaryDirectory",
        "is_image_file",
        "format_content",
        "GitTemporaryDirectory.__enter__",
        "make_repo",
        "safe_abs_path",
        "GitTemporaryDirectory.__exit__"
      ],
      "start_line": 62,
      "end_line": 106,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class GitTemporaryDirectory(ChdirTemporaryDirectory):\n    def __enter__(self):\n        dname = super().__enter__()\n        self.repo = make_repo(dname)\n        return dname\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        del self.repo\n        super().__exit__(exc_type, exc_val, exc_tb)\n\n\ndef make_repo(path=None):\n    import git\n\n    if not path:\n        path = \".\"\n    repo = git.Repo.init(path)\n    repo.config_writer().set_value(\"user\", \"name\", \"Test User\").release()\n    repo.config_writer().set_value(\"user\", \"email\", \"testuser@example.com\").release()\n\n    return repo\n\n\ndef is_image_file(file_name):\n    \"\"\"\n    Check if the given file name has an image file extension.\n\n    :param file_name: The name of the file to check.\n    :return: True if the file is an image, False otherwise.\n    \"\"\"\n    file_name = str(file_name)  # Convert file_name to string\n    return any(file_name.endswith(ext) for ext in IMAGE_EXTENSIONS)\n\n\ndef safe_abs_path(res):\n    \"Gives an abs path, which safely returns a full (not 8.3) windows path\"\n    res = Path(res).resolve()\n    return str(res)\n\n\ndef format_content(role, content):\n    formatted_lines = []\n    for line in content.splitlines():\n        formatted_lines.append(f\"{role} {line}\")\n    return \"\\n\".join(formatted_lines)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/utils.py::4",
    "metadata": {
      "file_path": "aider/utils.py",
      "file_name": "utils.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 255,
      "span_ids": [
        "format_messages",
        "show_messages"
      ],
      "start_line": 109,
      "end_line": 142,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def format_messages(messages, title=None):\n    output = []\n    if title:\n        output.append(f\"{title.upper()} {'*' * 50}\")\n\n    for msg in messages:\n        output.append(\"\")\n        role = msg[\"role\"].upper()\n        content = msg.get(\"content\")\n        if isinstance(content, list):  # Handle list content (e.g., image messages)\n            for item in content:\n                if isinstance(item, dict):\n                    for key, value in item.items():\n                        if isinstance(value, dict) and \"url\" in value:\n                            output.append(f\"{role} {key.capitalize()} URL: {value['url']}\")\n                        else:\n                            output.append(f\"{role} {key}: {value}\")\n                else:\n                    output.append(f\"{role} {item}\")\n        elif isinstance(content, str):  # Handle string content\n            output.append(format_content(role, content))\n        function_call = msg.get(\"function_call\")\n        if function_call:\n            output.append(f\"{role} Function Call: {function_call}\")\n\n    return \"\\n\".join(output)\n\n\ndef show_messages(messages, title=None, functions=None):\n    formatted_output = format_messages(messages, title)\n    print(formatted_output)\n\n    if functions:\n        dump(functions)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/utils.py::5",
    "metadata": {
      "file_path": "aider/utils.py",
      "file_name": "utils.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 261,
      "span_ids": [
        "split_chat_history_markdown"
      ],
      "start_line": 145,
      "end_line": 193,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def split_chat_history_markdown(text, include_tool=False):\n    messages = []\n    user = []\n    assistant = []\n    tool = []\n    lines = text.splitlines(keepends=True)\n\n    def append_msg(role, lines):\n        lines = \"\".join(lines)\n        if lines.strip():\n            messages.append(dict(role=role, content=lines))\n\n    for line in lines:\n        if line.startswith(\"# \"):\n            continue\n        if line.startswith(\"> \"):\n            append_msg(\"assistant\", assistant)\n            assistant = []\n            append_msg(\"user\", user)\n            user = []\n            tool.append(line[2:])\n            continue\n        # if line.startswith(\"#### /\"):\n        #    continue\n\n        if line.startswith(\"#### \"):\n            append_msg(\"assistant\", assistant)\n            assistant = []\n            append_msg(\"tool\", tool)\n            tool = []\n\n            content = line[5:]\n            user.append(content)\n            continue\n\n        append_msg(\"user\", user)\n        user = []\n        append_msg(\"tool\", tool)\n        tool = []\n\n        assistant.append(line)\n\n    append_msg(\"assistant\", assistant)\n    append_msg(\"user\", user)\n\n    if not include_tool:\n        messages = [m for m in messages if m[\"role\"] != \"tool\"]\n\n    return messages",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/utils.py::6",
    "metadata": {
      "file_path": "aider/utils.py",
      "file_name": "utils.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 245,
      "span_ids": [
        "run_install",
        "get_pip_install"
      ],
      "start_line": 196,
      "end_line": 250,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_pip_install(args):\n    cmd = [\n        sys.executable,\n        \"-m\",\n        \"pip\",\n        \"install\",\n        \"--upgrade\",\n        \"--upgrade-strategy\",\n        \"only-if-needed\",\n    ]\n    cmd += args\n    return cmd\n\n\ndef run_install(cmd):\n    print()\n    print(\"Installing:\", printable_shell_command(cmd))\n\n    try:\n        output = []\n        process = subprocess.Popen(\n            cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True,\n            bufsize=1,\n            universal_newlines=True,\n            encoding=sys.stdout.encoding,\n            errors=\"replace\",\n        )\n        spinner = Spinner(\"Installing...\")\n\n        while True:\n            char = process.stdout.read(1)\n            if not char:\n                break\n\n            output.append(char)\n            spinner.step()\n\n        spinner.end()\n        return_code = process.wait()\n        output = \"\".join(output)\n\n        if return_code == 0:\n            print(\"Installation complete.\")\n            print()\n            return True, output\n\n    except subprocess.CalledProcessError as e:\n        print(f\"\\nError running pip install: {e}\")\n\n    print(\"\\nInstallation failed.\\n\")\n\n    return False, output",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/utils.py::7",
    "metadata": {
      "file_path": "aider/utils.py",
      "file_name": "utils.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 386,
      "span_ids": [
        "Spinner.test_charset",
        "Spinner.step",
        "Spinner.__init__",
        "Spinner._step",
        "Spinner.end",
        "Spinner"
      ],
      "start_line": 253,
      "end_line": 299,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Spinner:\n    unicode_spinner = [\"\u280b\", \"\u2819\", \"\u2839\", \"\u2838\", \"\u283c\", \"\u2834\", \"\u2826\", \"\u2827\", \"\u2807\", \"\u280f\"]\n    ascii_spinner = [\"|\", \"/\", \"-\", \"\\\\\"]\n\n    def __init__(self, text):\n        self.text = text\n        self.start_time = time.time()\n        self.last_update = 0\n        self.visible = False\n        self.is_tty = sys.stdout.isatty()\n        self.tested = False\n\n    def test_charset(self):\n        if self.tested:\n            return\n        self.tested = True\n        # Try unicode first, fall back to ascii if needed\n        try:\n            # Test if we can print unicode characters\n            print(self.unicode_spinner[0], end=\"\", flush=True)\n            print(\"\\r\", end=\"\", flush=True)\n            self.spinner_chars = itertools.cycle(self.unicode_spinner)\n        except UnicodeEncodeError:\n            self.spinner_chars = itertools.cycle(self.ascii_spinner)\n\n    def step(self):\n        if not self.is_tty:\n            return\n\n        current_time = time.time()\n        if not self.visible and current_time - self.start_time >= 0.5:\n            self.visible = True\n            self._step()\n        elif self.visible and current_time - self.last_update >= 0.1:\n            self._step()\n        self.last_update = current_time\n\n    def _step(self):\n        if not self.visible:\n            return\n\n        self.test_charset()\n        print(f\"\\r{self.text} {next(self.spinner_chars)}\\r{self.text} \", end=\"\", flush=True)\n\n    def end(self):\n        if self.visible and self.is_tty:\n            print(\"\\r\" + \" \" * (len(self.text) + 3))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/utils.py::8",
    "metadata": {
      "file_path": "aider/utils.py",
      "file_name": "utils.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 171,
      "span_ids": [
        "find_common_root",
        "format_tokens",
        "touch_file"
      ],
      "start_line": 302,
      "end_line": 330,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def find_common_root(abs_fnames):\n    try:\n        if len(abs_fnames) == 1:\n            return safe_abs_path(os.path.dirname(list(abs_fnames)[0]))\n        elif abs_fnames:\n            return safe_abs_path(os.path.commonpath(list(abs_fnames)))\n    except OSError:\n        pass\n\n    return safe_abs_path(os.getcwd())\n\n\ndef format_tokens(count):\n    if count < 1000:\n        return f\"{count}\"\n    elif count < 10000:\n        return f\"{count / 1000:.1f}k\"\n    else:\n        return f\"{round(count / 1000)}k\"\n\n\ndef touch_file(fname):\n    fname = Path(fname)\n    try:\n        fname.parent.mkdir(parents=True, exist_ok=True)\n        fname.touch()\n        return True\n    except OSError:\n        return False",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/utils.py::9",
    "metadata": {
      "file_path": "aider/utils.py",
      "file_name": "utils.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 235,
      "span_ids": [
        "check_pip_install_extra"
      ],
      "start_line": 333,
      "end_line": 370,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def check_pip_install_extra(io, module, prompt, pip_install_cmd, self_update=False):\n    if module:\n        try:\n            __import__(module)\n            return True\n        except (ImportError, ModuleNotFoundError, RuntimeError):\n            pass\n\n    cmd = get_pip_install(pip_install_cmd)\n\n    if prompt:\n        io.tool_warning(prompt)\n\n    if self_update and platform.system() == \"Windows\":\n        io.tool_output(\"Run this command to update:\")\n        print()\n        print(printable_shell_command(cmd))  # plain print so it doesn't line-wrap\n        return\n\n    if not io.confirm_ask(\"Run pip install?\", default=\"y\", subject=printable_shell_command(cmd)):\n        return\n\n    success, output = run_install(cmd)\n    if success:\n        if not module:\n            return True\n        try:\n            __import__(module)\n            return True\n        except (ImportError, ModuleNotFoundError, RuntimeError) as err:\n            io.tool_error(str(err))\n            pass\n\n    io.tool_error(output)\n\n    print()\n    print(\"Install failed, try running this command manually:\")\n    print(printable_shell_command(cmd))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/utils.py::10",
    "metadata": {
      "file_path": "aider/utils.py",
      "file_name": "utils.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 144,
      "span_ids": [
        "impl:3",
        "main",
        "printable_shell_command"
      ],
      "start_line": 373,
      "end_line": 399,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def printable_shell_command(cmd_list):\n    \"\"\"\n    Convert a list of command arguments to a properly shell-escaped string.\n\n    Args:\n        cmd_list (list): List of command arguments.\n\n    Returns:\n        str: Shell-escaped command string.\n    \"\"\"\n    if platform.system() == \"Windows\":\n        return subprocess.list2cmdline(cmd_list)\n    else:\n        return shlex.join(cmd_list)\n\n\ndef main():\n    spinner = Spinner(\"Running spinner...\")\n    for _ in range(40):  # 40 steps * 0.25 seconds = 10 seconds\n        time.sleep(0.25)\n        spinner.step()\n    spinner.end()\n\n\nif __name__ == \"__main__\":\n    main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/versioncheck.py::1",
    "metadata": {
      "file_path": "aider/versioncheck.py",
      "file_name": "versioncheck.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 140,
      "span_ids": [
        "imports",
        "install_from_main_branch"
      ],
      "start_line": 1,
      "end_line": 26,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import os\nimport sys\nimport time\nfrom pathlib import Path\n\nimport packaging.version\n\nimport aider\nfrom aider import utils\nfrom aider.dump import dump  # noqa: F401\n\nVERSION_CHECK_FNAME = Path.home() / \".aider\" / \"caches\" / \"versioncheck\"\n\n\ndef install_from_main_branch(io):\n    \"\"\"\n    Install the latest version of aider from the main branch of the GitHub repository.\n    \"\"\"\n\n    return utils.check_pip_install_extra(\n        io,\n        None,\n        \"Install the development version of aider from the main branch?\",\n        [\"git+https://github.com/Aider-AI/aider.git\"],\n        self_update=True,\n    )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/versioncheck.py::2",
    "metadata": {
      "file_path": "aider/versioncheck.py",
      "file_name": "versioncheck.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 173,
      "span_ids": [
        "install_upgrade"
      ],
      "start_line": 29,
      "end_line": 61,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def install_upgrade(io, latest_version=None):\n    \"\"\"\n    Install the latest version of aider from PyPI.\n    \"\"\"\n\n    if latest_version:\n        new_ver_text = f\"Newer aider version v{latest_version} is available.\"\n    else:\n        new_ver_text = \"Install latest version of aider?\"\n\n    docker_image = os.environ.get(\"AIDER_DOCKER_IMAGE\")\n    if docker_image:\n        text = f\"\"\"\n{new_ver_text} To upgrade, run:\n\n    docker pull {docker_image}\n\"\"\"\n        io.tool_warning(text)\n        return True\n\n    success = utils.check_pip_install_extra(\n        io,\n        None,\n        new_ver_text,\n        [\"aider-chat\"],\n        self_update=True,\n    )\n\n    if success:\n        io.tool_output(\"Re-run aider to use new version.\")\n        sys.exit()\n\n    return",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/versioncheck.py::3",
    "metadata": {
      "file_path": "aider/versioncheck.py",
      "file_name": "versioncheck.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 347,
      "span_ids": [
        "check_version"
      ],
      "start_line": 64,
      "end_line": 114,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def check_version(io, just_check=False, verbose=False):\n    if not just_check and VERSION_CHECK_FNAME.exists():\n        day = 60 * 60 * 24\n        since = time.time() - os.path.getmtime(VERSION_CHECK_FNAME)\n        if 0 < since < day:\n            if verbose:\n                hours = since / 60 / 60\n                io.tool_output(f\"Too soon to check version: {hours:.1f} hours\")\n            return\n\n    # To keep startup fast, avoid importing this unless needed\n    import requests\n\n    try:\n        response = requests.get(\"https://pypi.org/pypi/aider-chat/json\")\n        data = response.json()\n        latest_version = data[\"info\"][\"version\"]\n        current_version = aider.__version__\n\n        if just_check or verbose:\n            io.tool_output(f\"Current version: {current_version}\")\n            io.tool_output(f\"Latest version: {latest_version}\")\n\n        is_update_available = packaging.version.parse(latest_version) > packaging.version.parse(\n            current_version\n        )\n    except Exception as err:\n        io.tool_error(f\"Error checking pypi for new version: {err}\")\n        return False\n    finally:\n        VERSION_CHECK_FNAME.parent.mkdir(parents=True, exist_ok=True)\n        VERSION_CHECK_FNAME.touch()\n\n    ###\n    # is_update_available = True\n\n    if just_check or verbose:\n        if is_update_available:\n            io.tool_output(\"Update available\")\n        else:\n            io.tool_output(\"No update available\")\n\n    if just_check:\n        return is_update_available\n\n    if not is_update_available:\n        return False\n\n    install_upgrade(io, latest_version)\n    return True",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/voice.py::1",
    "metadata": {
      "file_path": "aider/voice.py",
      "file_name": "voice.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 147,
      "span_ids": [
        "imports",
        "SoundDeviceError"
      ],
      "start_line": 1,
      "end_line": 30,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import math\nimport os\nimport queue\nimport tempfile\nimport time\nimport warnings\n\nfrom prompt_toolkit.shortcuts import prompt\n\nfrom aider.llm import litellm\n\nfrom .dump import dump  # noqa: F401\n\nwarnings.filterwarnings(\n    \"ignore\", message=\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\"\n)\nwarnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n\n\nfrom pydub import AudioSegment  # noqa\nfrom pydub.exceptions import CouldntDecodeError, CouldntEncodeError  # noqa\n\ntry:\n    import soundfile as sf\nexcept (OSError, ModuleNotFoundError):\n    sf = None\n\n\nclass SoundDeviceError(Exception):\n    pass",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/voice.py::2",
    "metadata": {
      "file_path": "aider/voice.py",
      "file_name": "voice.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 292,
      "span_ids": [
        "Voice.__init__",
        "Voice"
      ],
      "start_line": 33,
      "end_line": 75,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Voice:\n    max_rms = 0\n    min_rms = 1e5\n    pct = 0\n\n    threshold = 0.15\n\n    def __init__(self, audio_format=\"wav\", device_name=None):\n        if sf is None:\n            raise SoundDeviceError\n        try:\n            print(\"Initializing sound device...\")\n            import sounddevice as sd\n\n            self.sd = sd\n\n            devices = sd.query_devices()\n\n            if device_name:\n                # Find the device with matching name\n                device_id = None\n                for i, device in enumerate(devices):\n                    if device_name in device[\"name\"]:\n                        device_id = i\n                        break\n                if device_id is None:\n                    available_inputs = [d[\"name\"] for d in devices if d[\"max_input_channels\"] > 0]\n                    raise ValueError(\n                        f\"Device '{device_name}' not found. Available input devices:\"\n                        f\" {available_inputs}\"\n                    )\n\n                print(f\"Using input device: {device_name} (ID: {device_id})\")\n\n                self.device_id = device_id\n            else:\n                self.device_id = None\n\n        except (OSError, ModuleNotFoundError):\n            raise SoundDeviceError\n        if audio_format not in [\"wav\", \"mp3\", \"webm\"]:\n            raise ValueError(f\"Unsupported audio format: {audio_format}\")\n        self.audio_format = audio_format",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/voice.py::3",
    "metadata": {
      "file_path": "aider/voice.py",
      "file_name": "voice.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 138,
      "span_ids": [
        "Voice.callback"
      ],
      "start_line": 77,
      "end_line": 91,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Voice:\n\n    def callback(self, indata, frames, time, status):\n        \"\"\"This is called (from a separate thread) for each audio block.\"\"\"\n        import numpy as np\n\n        rms = np.sqrt(np.mean(indata**2))\n        self.max_rms = max(self.max_rms, rms)\n        self.min_rms = min(self.min_rms, rms)\n\n        rng = self.max_rms - self.min_rms\n        if rng > 0.001:\n            self.pct = (rms - self.min_rms) / rng\n        else:\n            self.pct = 0.5\n\n        self.q.put(indata.copy())",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/voice.py::4",
    "metadata": {
      "file_path": "aider/voice.py",
      "file_name": "voice.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 180,
      "span_ids": [
        "Voice.get_prompt",
        "Voice.record_and_transcribe"
      ],
      "start_line": 93,
      "end_line": 114,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Voice:\n\n    def get_prompt(self):\n        num = 10\n        if math.isnan(self.pct) or self.pct < self.threshold:\n            cnt = 0\n        else:\n            cnt = int(self.pct * 10)\n\n        bar = \"\u2591\" * cnt + \"\u2588\" * (num - cnt)\n        bar = bar[:num]\n\n        dur = time.time() - self.start_time\n        return f\"Recording, press ENTER when done... {dur:.1f}sec {bar}\"\n\n    def record_and_transcribe(self, history=None, language=None):\n        try:\n            return self.raw_record_and_transcribe(history, language)\n        except KeyboardInterrupt:\n            return\n        except SoundDeviceError as e:\n            print(f\"Error: {e}\")\n            print(\"Please ensure you have a working audio input device connected and try again.\")\n            return",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/voice.py::5",
    "metadata": {
      "file_path": "aider/voice.py",
      "file_name": "voice.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 611,
      "span_ids": [
        "Voice.raw_record_and_transcribe",
        "impl:7"
      ],
      "start_line": 116,
      "end_line": 188,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Voice:\n\n    def raw_record_and_transcribe(self, history, language):\n        self.q = queue.Queue()\n\n        temp_wav = tempfile.mktemp(suffix=\".wav\")\n\n        try:\n            sample_rate = int(self.sd.query_devices(self.device_id, \"input\")[\"default_samplerate\"])\n        except (TypeError, ValueError):\n            sample_rate = 16000  # fallback to 16kHz if unable to query device\n        except self.sd.PortAudioError:\n            raise SoundDeviceError(\n                \"No audio input device detected. Please check your audio settings and try again.\"\n            )\n\n        self.start_time = time.time()\n\n        try:\n            with self.sd.InputStream(\n                samplerate=sample_rate, channels=1, callback=self.callback, device=self.device_id\n            ):\n                prompt(self.get_prompt, refresh_interval=0.1)\n        except self.sd.PortAudioError as err:\n            raise SoundDeviceError(f\"Error accessing audio input device: {err}\")\n\n        with sf.SoundFile(temp_wav, mode=\"x\", samplerate=sample_rate, channels=1) as file:\n            while not self.q.empty():\n                file.write(self.q.get())\n\n        use_audio_format = self.audio_format\n\n        # Check file size and offer to convert to mp3 if too large\n        file_size = os.path.getsize(temp_wav)\n        if file_size > 24.9 * 1024 * 1024 and self.audio_format == \"wav\":\n            print(\"\\nWarning: {temp_wav} is too large, switching to mp3 format.\")\n            use_audio_format = \"mp3\"\n\n        filename = temp_wav\n        if use_audio_format != \"wav\":\n            try:\n                new_filename = tempfile.mktemp(suffix=f\".{use_audio_format}\")\n                audio = AudioSegment.from_wav(temp_wav)\n                audio.export(new_filename, format=use_audio_format)\n                os.remove(temp_wav)\n                filename = new_filename\n            except (CouldntDecodeError, CouldntEncodeError) as e:\n                print(f\"Error converting audio: {e}\")\n            except (OSError, FileNotFoundError) as e:\n                print(f\"File system error during conversion: {e}\")\n            except Exception as e:\n                print(f\"Unexpected error during audio conversion: {e}\")\n\n        with open(filename, \"rb\") as fh:\n            try:\n                transcript = litellm.transcription(\n                    model=\"whisper-1\", file=fh, prompt=history, language=language\n                )\n            except Exception as err:\n                print(f\"Unable to transcribe {filename}: {err}\")\n                return\n\n        if filename != temp_wav:\n            os.remove(filename)\n\n        text = transcript.text\n        return text\n\n\nif __name__ == \"__main__\":\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    if not api_key:\n        raise ValueError(\"Please set the OPENAI_API_KEY environment variable.\")\n    print(Voice().record_and_transcribe())",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/watch.py::1",
    "metadata": {
      "file_path": "aider/watch.py",
      "file_name": "watch.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 482,
      "span_ids": [
        "imports",
        "load_gitignores"
      ],
      "start_line": 1,
      "end_line": 60,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import re\nimport threading\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom grep_ast import TreeContext\nfrom pathspec import PathSpec\nfrom pathspec.patterns import GitWildMatchPattern\nfrom watchfiles import watch\n\nfrom aider.dump import dump  # noqa\nfrom aider.watch_prompts import watch_ask_prompt, watch_code_prompt\n\n\ndef load_gitignores(gitignore_paths: list[Path]) -> Optional[PathSpec]:\n    \"\"\"Load and parse multiple .gitignore files into a single PathSpec\"\"\"\n    if not gitignore_paths:\n        return None\n\n    patterns = [\n        \".aider*\",\n        \".git\",\n        # Common editor backup/temp files\n        \"*~\",  # Emacs/vim backup\n        \"*.bak\",  # Generic backup\n        \"*.swp\",  # Vim swap\n        \"*.swo\",  # Vim swap\n        \"\\\\#*\\\\#\",  # Emacs auto-save\n        \".#*\",  # Emacs lock files\n        \"*.tmp\",  # Generic temp files\n        \"*.temp\",  # Generic temp files\n        \"*.orig\",  # Merge conflict originals\n        \"*.pyc\",  # Python bytecode\n        \"__pycache__/\",  # Python cache dir\n        \".DS_Store\",  # macOS metadata\n        \"Thumbs.db\",  # Windows thumbnail cache\n        # IDE files\n        \".idea/\",  # JetBrains IDEs\n        \".vscode/\",  # VS Code\n        \"*.sublime-*\",  # Sublime Text\n        \".project\",  # Eclipse\n        \".settings/\",  # Eclipse\n        \"*.code-workspace\",  # VS Code workspace\n        # Environment files\n        \".env\",  # Environment variables\n        \".venv/\",  # Python virtual environments\n        \"node_modules/\",  # Node.js dependencies\n        \"vendor/\",  # Various dependencies\n        # Logs and caches\n        \"*.log\",  # Log files\n        \".cache/\",  # Cache directories\n        \".pytest_cache/\",  # Python test cache\n        \"coverage/\",  # Code coverage reports\n    ]  # Always ignore\n    for path in gitignore_paths:\n        if path.exists():\n            with open(path) as f:\n                patterns.extend(f.readlines())\n\n    return PathSpec.from_lines(GitWildMatchPattern, patterns) if patterns else None",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/watch.py::2",
    "metadata": {
      "file_path": "aider/watch.py",
      "file_name": "watch.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 198,
      "span_ids": [
        "FileWatcher",
        "FileWatcher.__init__"
      ],
      "start_line": 63,
      "end_line": 84,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class FileWatcher:\n    \"\"\"Watches source files for changes and AI comments\"\"\"\n\n    # Compiled regex pattern for AI comments\n    ai_comment_pattern = re.compile(r\"(?:#|//|--) *(ai\\b.*|ai\\b.*|.*\\bai[?!]?) *$\", re.IGNORECASE)\n\n    def __init__(self, coder, gitignores=None, verbose=False, analytics=None, root=None):\n        self.coder = coder\n        self.io = coder.io\n        self.root = Path(root) if root else Path(coder.root)\n        self.verbose = verbose\n        self.analytics = analytics\n        self.stop_event = None\n        self.watcher_thread = None\n        self.changed_files = set()\n        self.gitignores = gitignores\n\n        self.gitignore_spec = load_gitignores(\n            [Path(g) for g in self.gitignores] if self.gitignores else []\n        )\n\n        coder.io.file_watcher = self",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/watch.py::3",
    "metadata": {
      "file_path": "aider/watch.py",
      "file_name": "watch.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 148,
      "span_ids": [
        "FileWatcher.filter_func"
      ],
      "start_line": 86,
      "end_line": 109,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class FileWatcher:\n\n    def filter_func(self, change_type, path):\n        \"\"\"Filter function for the file watcher\"\"\"\n        path_obj = Path(path)\n        path_abs = path_obj.absolute()\n\n        if not path_abs.is_relative_to(self.root.absolute()):\n            return False\n\n        rel_path = path_abs.relative_to(self.root)\n        if self.verbose:\n            dump(rel_path)\n\n        if self.gitignore_spec and self.gitignore_spec.match_file(str(rel_path)):\n            return False\n\n        if self.verbose:\n            dump(\"ok\", rel_path)\n\n        # Check if file contains AI markers\n        try:\n            comments, _, _ = self.get_ai_comments(str(path_abs))\n            return bool(comments)\n        except Exception:\n            return",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/watch.py::4",
    "metadata": {
      "file_path": "aider/watch.py",
      "file_name": "watch.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 210,
      "span_ids": [
        "FileWatcher.start",
        "FileWatcher.stop"
      ],
      "start_line": 111,
      "end_line": 142,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class FileWatcher:\n\n    def start(self):\n        \"\"\"Start watching for file changes\"\"\"\n        self.stop_event = threading.Event()\n        self.changed_files = set()\n\n        def watch_files():\n            try:\n                for changes in watch(\n                    str(self.root), watch_filter=self.filter_func, stop_event=self.stop_event\n                ):\n                    if not changes:\n                        continue\n                    changed_files = {str(Path(change[1])) for change in changes}\n                    self.changed_files.update(changed_files)\n                    self.io.interrupt_input()\n                    return\n            except Exception as e:\n                if self.verbose:\n                    dump(f\"File watcher error: {e}\")\n                raise e\n\n        self.watcher_thread = threading.Thread(target=watch_files, daemon=True)\n        self.watcher_thread.start()\n\n    def stop(self):\n        \"\"\"Stop watching for file changes\"\"\"\n        if self.stop_event:\n            self.stop_event.set()\n        if self.watcher_thread:\n            self.watcher_thread.join()\n            self.watcher_thread = None\n            self.stop_event = None",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/watch.py::5",
    "metadata": {
      "file_path": "aider/watch.py",
      "file_name": "watch.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 488,
      "span_ids": [
        "FileWatcher.process_changes"
      ],
      "start_line": 144,
      "end_line": 218,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class FileWatcher:\n\n    def process_changes(self):\n        \"\"\"Get any detected file changes\"\"\"\n\n        has_action = None\n        added = False\n        for fname in self.changed_files:\n            _, _, action = self.get_ai_comments(fname)\n            if action in (\"!\", \"?\"):\n                has_action = action\n\n            if fname in self.coder.abs_fnames:\n                continue\n            if self.analytics:\n                self.analytics.event(\"ai-comments file-add\")\n            self.coder.abs_fnames.add(fname)\n            rel_fname = self.coder.get_rel_fname(fname)\n            if not added:\n                self.io.tool_output()\n                added = True\n            self.io.tool_output(f\"Added {rel_fname} to the chat\")\n\n        if not has_action:\n            if added:\n                self.io.tool_output(\n                    \"End your comment with AI! to request changes or AI? to ask questions\"\n                )\n            return \"\"\n\n        if self.analytics:\n            self.analytics.event(\"ai-comments execute\")\n        self.io.tool_output(\"Processing your request...\")\n\n        if has_action == \"!\":\n            res = watch_code_prompt\n        elif has_action == \"?\":\n            res = watch_ask_prompt\n\n        # Refresh all AI comments from tracked files\n        for fname in self.coder.abs_fnames:\n            line_nums, comments, _action = self.get_ai_comments(fname)\n            if not line_nums:\n                continue\n\n            code = self.io.read_text(fname)\n            if not code:\n                continue\n\n            rel_fname = self.coder.get_rel_fname(fname)\n            res += f\"\\n{rel_fname}:\\n\"\n\n            # Convert comment line numbers to line indices (0-based)\n            lois = [ln - 1 for ln, _ in zip(line_nums, comments) if ln > 0]\n\n            try:\n                context = TreeContext(\n                    rel_fname,\n                    code,\n                    color=False,\n                    line_number=False,\n                    child_context=False,\n                    last_line=False,\n                    margin=0,\n                    mark_lois=True,\n                    loi_pad=3,\n                    show_top_of_file_parent_scope=False,\n                )\n                context.lines_of_interest = set()\n                context.add_lines_of_interest(lois)\n                context.add_context()\n                res += context.format()\n            except ValueError:\n                for ln, comment in zip(line_nums, comments):\n                    res += f\"  Line {ln}: {comment}\\n\"\n\n        return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/watch.py::6",
    "metadata": {
      "file_path": "aider/watch.py",
      "file_name": "watch.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 213,
      "span_ids": [
        "FileWatcher.get_ai_comments"
      ],
      "start_line": 220,
      "end_line": 244,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class FileWatcher:\n\n    def get_ai_comments(self, filepath):\n        \"\"\"Extract AI comment line numbers, comments and action status from a file\"\"\"\n        line_nums = []\n        comments = []\n        has_action = None  # None, \"!\" or \"?\"\n        content = self.io.read_text(filepath, silent=True)\n        if not content:\n            return None, None, None\n\n        for i, line in enumerate(content.splitlines(), 1):\n            if match := self.ai_comment_pattern.search(line):\n                comment = match.group(0).strip()\n                if comment:\n                    line_nums.append(i)\n                    comments.append(comment)\n                    comment = comment.lower()\n                    comment = comment.lstrip(\"/#-\")\n                    comment = comment.strip()\n                    if comment.startswith(\"ai!\") or comment.endswith(\"ai!\"):\n                        has_action = \"!\"\n                    elif comment.startswith(\"ai?\") or comment.endswith(\"ai?\"):\n                        has_action = \"?\"\n        if not line_nums:\n            return None, None, None\n        return line_nums, comments, has_action",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/watch.py::7",
    "metadata": {
      "file_path": "aider/watch.py",
      "file_name": "watch.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 213,
      "span_ids": [
        "impl",
        "main"
      ],
      "start_line": 247,
      "end_line": 282,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main():\n    \"\"\"Example usage of the file watcher\"\"\"\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\"Watch source files for changes\")\n    parser.add_argument(\"directory\", help=\"Directory to watch\")\n    parser.add_argument(\n        \"--gitignore\",\n        action=\"append\",\n        help=\"Path to .gitignore file (can be specified multiple times)\",\n    )\n    args = parser.parse_args()\n\n    directory = args.directory\n    print(f\"Watching source files in {directory}...\")\n\n    # Example ignore function that ignores files with \"test\" in the name\n    def ignore_test_files(path):\n        return \"test\" in path.name.lower()\n\n    watcher = FileWatcher(directory, gitignores=args.gitignore)\n    try:\n        watcher.start()\n        while True:\n            if changes := watcher.get_changes():\n                for file in sorted(changes.keys()):\n                    print(file)\n                watcher.changed_files = None\n    except KeyboardInterrupt:\n        print(\"\\nStopped watching files\")\n        watcher.stop()\n\n\nif __name__ == \"__main__\":\n    main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "aider/watch_prompts.py::1",
    "metadata": {
      "file_path": "aider/watch_prompts.py",
      "file_name": "watch_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 123,
      "span_ids": [
        "impl"
      ],
      "start_line": 1,
      "end_line": 13,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "watch_code_prompt = \"\"\"\nI've written your instructions in comments in the code and marked them with \"ai\"\nYou can see the \"AI\" comments shown below (marked with \u2588).\nFind them in the code files I've shared with you, and follow their instructions.\n\nAfter completing those instructions, also be sure to remove all the \"AI\" comments from the code too.\n\"\"\"\n\nwatch_ask_prompt = \"\"\"/ask\nFind the \"AI\" comments below (marked with \u2588) in the code files I've shared with you.\nThey contain my questions that I need you to answer and other instructions for you.\n\"\"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::1",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 204,
      "span_ids": [
        "docstring"
      ],
      "start_line": 1,
      "end_line": 39,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "#!/usr/bin/env python3\nimport datetime\nimport json\nimport os\nimport random\nimport re\nimport shutil\nimport subprocess\nimport sys\nimport time\nimport traceback\nfrom collections import defaultdict\nfrom json.decoder import JSONDecodeError\nfrom pathlib import Path\nfrom types import SimpleNamespace\nfrom typing import List, Optional\n\nimport git\nimport lox\nimport pandas as pd\nimport prompts\nimport typer\nfrom dotenv import load_dotenv\nfrom plots import plot_refactoring\nfrom rich.console import Console\n\nfrom aider import models, sendchat\nfrom aider.coders import Coder, base_coder\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import InputOutput\n\nBENCHMARK_DNAME = Path(os.environ.get(\"AIDER_BENCHMARK_DIR\", \"tmp.benchmarks\"))\n\nEXERCISES_DIR_DEFAULT = \"polyglot-benchmark\"\n\napp = typer.Typer(add_completion=False, pretty_exceptions_enable=False)\n\n\nload_dotenv(override=True)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::2",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 379,
      "span_ids": [
        "find_latest_benchmark_dir"
      ],
      "start_line": 42,
      "end_line": 87,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def find_latest_benchmark_dir():\n    benchmark_dirs = [d for d in BENCHMARK_DNAME.iterdir() if d.is_dir()]\n    if not benchmark_dirs:\n        print(\"Error: No benchmark directories found under tmp.benchmarks.\")\n        sys.exit(1)\n\n    # Get current time and 24 hours ago\n    now = datetime.datetime.now()\n    day_ago = now - datetime.timedelta(days=1)\n\n    # Filter directories by name pattern YYYY-MM-DD-HH-MM-SS--\n    recent_dirs = []\n    for d in benchmark_dirs:\n        try:\n            # Extract datetime from directory name\n            date_str = d.name[:19]  # Takes YYYY-MM-DD-HH-MM-SS\n            dir_date = datetime.datetime.strptime(date_str, \"%Y-%m-%d-%H-%M-%S\")\n            if dir_date >= day_ago:\n                recent_dirs.append(d)\n        except ValueError:\n            # Skip directories that don't match the expected format\n            continue\n\n    if not recent_dirs:\n        print(\"Error: No benchmark directories found from the last 24 hours.\")\n        sys.exit(1)\n\n    # Find directory with most recently modified .md file\n    latest_dir = None\n    latest_time = 0\n\n    for d in recent_dirs:\n        # Look for .md files in subdirectories\n        for md_file in d.glob(\"*/exercises/practice/*/.*.md\"):\n            if md_file.is_file():\n                mtime = md_file.stat().st_mtime\n                if mtime > latest_time:\n                    latest_time = mtime\n                    latest_dir = d\n\n    if not latest_dir:\n        print(\"Error: No .md files found in recent benchmark directories.\")\n        sys.exit(1)\n\n    print(f\"Using the most recently updated benchmark directory: {latest_dir.name}\")\n    return latest_dir",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::3",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 260,
      "span_ids": [
        "show_stats"
      ],
      "start_line": 90,
      "end_line": 132,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def show_stats(dirnames, graphs, stats_languages=None):\n    raw_rows = []\n    for dirname in dirnames:\n        row = summarize_results(dirname, stats_languages)\n        raw_rows.append(row)\n\n    # return\n\n    seen = dict()\n    rows = []\n    for row in raw_rows:\n        if not row:\n            continue\n\n        if row.completed_tests != row.total_tests:\n            print(\n                f\"Warning: {row.dir_name} is incomplete: {row.completed_tests} of {row.total_tests}\"\n            )\n\n        try:\n            kind = (row.model, row.edit_format)\n        except AttributeError:\n            return\n\n        if kind in seen:\n            dump(row.dir_name)\n            dump(seen[kind])\n            return\n\n        seen[kind] = row.dir_name\n        rows.append(vars(row))\n\n    repeat_hi = repeat_lo = repeat_avg = None  # noqa: F841\n\n    df = pd.DataFrame.from_records(rows)\n    # df.sort_values(by=[\"model\", \"edit_format\"], inplace=True)\n\n    # dump(df)\n    if graphs:\n        # plot_timing(df)\n        # plot_outcomes(df, repeats, repeat_hi, repeat_lo, repeat_avg)\n        # plot_outcomes_claude(df)\n        plot_refactoring(df)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::4",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 196,
      "span_ids": [
        "resolve_dirname"
      ],
      "start_line": 135,
      "end_line": 157,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def resolve_dirname(dirname, use_single_prior, make_new):\n    if len(dirname.parts) > 1:\n        return dirname\n\n    priors = list(BENCHMARK_DNAME.glob(f\"*--{dirname}\"))\n    if len(priors) == 1 and use_single_prior:\n        dirname = priors[0].name\n        print(f\"Using pre-existing {dirname}\")\n    elif len(priors):\n        if not make_new:\n            print(f\"Prior runs of {dirname} exist, use --new or name one explicitly\")\n            print()\n            for prior in priors:\n                print(prior)\n            return\n\n    if not re.match(r\"\\d\\d\\d\\d-\\d\\d-\\d\\d-\", str(dirname)):\n        now = datetime.datetime.now()\n        now = now.strftime(\"%Y-%m-%d-%H-%M-%S--\")\n        dirname = now + dirname.name\n\n    dirname = BENCHMARK_DNAME / dirname\n    return dirname",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::5",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 837,
      "span_ids": [
        "main"
      ],
      "start_line": 160,
      "end_line": 231,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "@app.command()\ndef main(\n    dirnames: Optional[List[str]] = typer.Argument(None, help=\"Directory names\"),\n    graphs: bool = typer.Option(False, \"--graphs\", help=\"Generate graphs\"),\n    model: str = typer.Option(\"gpt-3.5-turbo\", \"--model\", \"-m\", help=\"Model name\"),\n    sleep: float = typer.Option(\n        0, \"--sleep\", help=\"Sleep seconds between tests when single threaded\"\n    ),\n    languages: str = typer.Option(\n        None, \"--languages\", \"-l\", help=\"Only run tests for specific languages (comma separated)\"\n    ),\n    edit_format: str = typer.Option(None, \"--edit-format\", \"-e\", help=\"Edit format\"),\n    editor_model: str = typer.Option(None, \"--editor-model\", help=\"Editor model name\"),\n    editor_edit_format: str = typer.Option(None, \"--editor-edit-format\", help=\"Editor edit format\"),\n    replay: str = typer.Option(\n        None,\n        \"--replay\",\n        help=\"Replay previous .aider.chat.history.md responses from previous benchmark run\",\n    ),\n    keywords: str = typer.Option(\n        None, \"--keywords\", \"-k\", help=\"Only run tests that contain keywords (comma sep)\"\n    ),\n    clean: bool = typer.Option(\n        False, \"--clean\", \"-c\", help=\"Discard the existing testdir and make a clean copy\"\n    ),\n    cont: bool = typer.Option(False, \"--cont\", help=\"Continue the (single) matching testdir\"),\n    make_new: bool = typer.Option(False, \"--new\", \"-n\", help=\"Make a new dated testdir\"),\n    no_unit_tests: bool = typer.Option(False, \"--no-unit-tests\", help=\"Do not run unit tests\"),\n    no_aider: bool = typer.Option(False, \"--no-aider\", help=\"Do not run aider\"),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\", help=\"Verbose output\"),\n    stats_only: bool = typer.Option(\n        False, \"--stats\", \"-s\", help=\"Do not run tests, just collect stats on completed tests\"\n    ),\n    stats_languages: str = typer.Option(\n        None,\n        \"--stats-languages\",\n        help=\"Only include stats for specific languages (comma separated)\",\n    ),\n    diffs_only: bool = typer.Option(False, \"--diffs\", help=\"Just diff the provided stats dirs\"),\n    tries: int = typer.Option(2, \"--tries\", \"-r\", help=\"Number of tries for running tests\"),\n    threads: int = typer.Option(1, \"--threads\", \"-t\", help=\"Number of threads to run in parallel\"),\n    num_tests: int = typer.Option(-1, \"--num-tests\", \"-n\", help=\"Number of tests to run\"),\n    num_ctx: Optional[int] = typer.Option(\n        None, \"--num-ctx\", help=\"Override model context window size\"\n    ),\n    exercises_dir: str = typer.Option(\n        EXERCISES_DIR_DEFAULT, \"--exercises-dir\", help=\"Directory with exercise files\"\n    ),\n):\n    repo = git.Repo(search_parent_directories=True)\n    commit_hash = repo.head.object.hexsha[:7]\n    if repo.is_dirty():\n        commit_hash += \"-dirty\"\n\n    if stats_only and not dirnames:\n        latest_dir = find_latest_benchmark_dir()\n        dirnames = [str(latest_dir)]\n\n    if dirnames is None:\n        dirnames = []\n\n    if len(dirnames) > 1 and not (stats_only or diffs_only):\n        print(\"Only provide 1 dirname unless running with --stats or --diffs\")\n        return 1\n\n    updated_dirnames = []\n    for dirname in dirnames:\n        dirname = Path(dirname)\n        dirname = resolve_dirname(dirname, stats_only or cont, make_new)\n        if not dirname:\n            return 1\n        updated_dirnames.append(dirname)\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::6",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 788,
      "span_ids": [
        "main"
      ],
      "start_line": 233,
      "end_line": 246,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "@app.command()\ndef main(\n    dirnames: Optional[List[str]] = typer.Argument(None, help=\"Directory names\"),\n    graphs: bool = typer.Option(False, \"--graphs\", help=\"Generate graphs\"),\n    model: str = typer.Option(\"gpt-3.5-turbo\", \"--model\", \"-m\", help=\"Model name\"),\n    sleep: float = typer.Option(\n        0, \"--sleep\", help=\"Sleep seconds between tests when single threaded\"\n    ),\n    languages: str = typer.Option(\n        None, \"--languages\", \"-l\", help=\"Only run tests for specific languages (comma separated)\"\n    ),\n    edit_format: str = typer.Option(None, \"--edit-format\", \"-e\", help=\"Edit format\"),\n    editor_model: str = typer.Option(None, \"--editor-model\", help=\"Editor model name\"),\n    editor_edit_format: str = typer.Option(None, \"--editor-edit-format\", help=\"Editor edit format\"),\n    replay: str = typer.Option(\n        None,\n        \"--replay\",\n        help=\"Replay previous .aider.chat.history.md responses from previous benchmark run\",\n    ),\n    keywords: str = typer.Option(\n        None, \"--keywords\", \"-k\", help=\"Only run tests that contain keywords (comma sep)\"\n    ),\n    clean: bool = typer.Option(\n        False, \"--clean\", \"-c\", help=\"Discard the existing testdir and make a clean copy\"\n    ),\n    cont: bool = typer.Option(False, \"--cont\", help=\"Continue the (single) matching testdir\"),\n    make_new: bool = typer.Option(False, \"--new\", \"-n\", help=\"Make a new dated testdir\"),\n    no_unit_tests: bool = typer.Option(False, \"--no-unit-tests\", help=\"Do not run unit tests\"),\n    no_aider: bool = typer.Option(False, \"--no-aider\", help=\"Do not run aider\"),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\", help=\"Verbose output\"),\n    stats_only: bool = typer.Option(\n        False, \"--stats\", \"-s\", help=\"Do not run tests, just collect stats on completed tests\"\n    ),\n    stats_languages: str = typer.Option(\n        None,\n        \"--stats-languages\",\n        help=\"Only include stats for specific languages (comma separated)\",\n    ),\n    diffs_only: bool = typer.Option(False, \"--diffs\", help=\"Just diff the provided stats dirs\"),\n    tries: int = typer.Option(2, \"--tries\", \"-r\", help=\"Number of tries for running tests\"),\n    threads: int = typer.Option(1, \"--threads\", \"-t\", help=\"Number of threads to run in parallel\"),\n    num_tests: int = typer.Option(-1, \"--num-tests\", \"-n\", help=\"Number of tests to run\"),\n    num_ctx: Optional[int] = typer.Option(\n        None, \"--num-ctx\", help=\"Override model context window size\"\n    ),\n    exercises_dir: str = typer.Option(\n        EXERCISES_DIR_DEFAULT, \"--exercises-dir\", help=\"Directory with exercise files\"\n    ),\n):\n    # ... other code\n\n    if stats_only:\n        return show_stats(updated_dirnames, graphs, stats_languages)\n\n    if diffs_only:\n        return show_diffs(updated_dirnames)\n\n    assert len(updated_dirnames) == 1, updated_dirnames\n    dirname = updated_dirnames[0]\n\n    if \"AIDER_DOCKER\" not in os.environ:\n        print(\"Warning: benchmarking runs unvetted code from GPT, run in a docker container\")\n        return\n\n    assert BENCHMARK_DNAME.exists() and BENCHMARK_DNAME.is_dir(), BENCHMARK_DNAME\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::7",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 881,
      "span_ids": [
        "main"
      ],
      "start_line": 248,
      "end_line": 271,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "@app.command()\ndef main(\n    dirnames: Optional[List[str]] = typer.Argument(None, help=\"Directory names\"),\n    graphs: bool = typer.Option(False, \"--graphs\", help=\"Generate graphs\"),\n    model: str = typer.Option(\"gpt-3.5-turbo\", \"--model\", \"-m\", help=\"Model name\"),\n    sleep: float = typer.Option(\n        0, \"--sleep\", help=\"Sleep seconds between tests when single threaded\"\n    ),\n    languages: str = typer.Option(\n        None, \"--languages\", \"-l\", help=\"Only run tests for specific languages (comma separated)\"\n    ),\n    edit_format: str = typer.Option(None, \"--edit-format\", \"-e\", help=\"Edit format\"),\n    editor_model: str = typer.Option(None, \"--editor-model\", help=\"Editor model name\"),\n    editor_edit_format: str = typer.Option(None, \"--editor-edit-format\", help=\"Editor edit format\"),\n    replay: str = typer.Option(\n        None,\n        \"--replay\",\n        help=\"Replay previous .aider.chat.history.md responses from previous benchmark run\",\n    ),\n    keywords: str = typer.Option(\n        None, \"--keywords\", \"-k\", help=\"Only run tests that contain keywords (comma sep)\"\n    ),\n    clean: bool = typer.Option(\n        False, \"--clean\", \"-c\", help=\"Discard the existing testdir and make a clean copy\"\n    ),\n    cont: bool = typer.Option(False, \"--cont\", help=\"Continue the (single) matching testdir\"),\n    make_new: bool = typer.Option(False, \"--new\", \"-n\", help=\"Make a new dated testdir\"),\n    no_unit_tests: bool = typer.Option(False, \"--no-unit-tests\", help=\"Do not run unit tests\"),\n    no_aider: bool = typer.Option(False, \"--no-aider\", help=\"Do not run aider\"),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\", help=\"Verbose output\"),\n    stats_only: bool = typer.Option(\n        False, \"--stats\", \"-s\", help=\"Do not run tests, just collect stats on completed tests\"\n    ),\n    stats_languages: str = typer.Option(\n        None,\n        \"--stats-languages\",\n        help=\"Only include stats for specific languages (comma separated)\",\n    ),\n    diffs_only: bool = typer.Option(False, \"--diffs\", help=\"Just diff the provided stats dirs\"),\n    tries: int = typer.Option(2, \"--tries\", \"-r\", help=\"Number of tries for running tests\"),\n    threads: int = typer.Option(1, \"--threads\", \"-t\", help=\"Number of threads to run in parallel\"),\n    num_tests: int = typer.Option(-1, \"--num-tests\", \"-n\", help=\"Number of tests to run\"),\n    num_ctx: Optional[int] = typer.Option(\n        None, \"--num-ctx\", help=\"Override model context window size\"\n    ),\n    exercises_dir: str = typer.Option(\n        EXERCISES_DIR_DEFAULT, \"--exercises-dir\", help=\"Directory with exercise files\"\n    ),\n):\n    # ... other code\n\n    def get_exercise_dirs(base_dir, languages=None):\n        \"\"\"Get all exercise directories for specified languages (or all if none specified)\"\"\"\n        base_dir = Path(base_dir)\n\n        # Get available language dirs\n        lang_dirs = [d for d in base_dir.iterdir() if d.is_dir()]\n\n        # Filter to requested languages if specified\n        if languages:\n            requested = set(lang.strip().lower() for lang in languages.split(\",\"))\n            lang_dirs = [d for d in lang_dirs if d.name.lower() in requested]\n            dump(lang_dirs)\n            if not lang_dirs:\n                print(f\"No matching language directories found for: {languages}\")\n                return []\n\n        # Get all exercise dirs under exercises/practice for each language\n        exercise_dirs = []\n        for lang_dir in lang_dirs:\n            practice_dir = lang_dir / \"exercises\" / \"practice\"\n            if practice_dir.exists():\n                exercise_dirs.extend(d for d in practice_dir.iterdir() if d.is_dir())\n\n        return exercise_dirs\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::8",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 875,
      "span_ids": [
        "main"
      ],
      "start_line": 273,
      "end_line": 295,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "@app.command()\ndef main(\n    dirnames: Optional[List[str]] = typer.Argument(None, help=\"Directory names\"),\n    graphs: bool = typer.Option(False, \"--graphs\", help=\"Generate graphs\"),\n    model: str = typer.Option(\"gpt-3.5-turbo\", \"--model\", \"-m\", help=\"Model name\"),\n    sleep: float = typer.Option(\n        0, \"--sleep\", help=\"Sleep seconds between tests when single threaded\"\n    ),\n    languages: str = typer.Option(\n        None, \"--languages\", \"-l\", help=\"Only run tests for specific languages (comma separated)\"\n    ),\n    edit_format: str = typer.Option(None, \"--edit-format\", \"-e\", help=\"Edit format\"),\n    editor_model: str = typer.Option(None, \"--editor-model\", help=\"Editor model name\"),\n    editor_edit_format: str = typer.Option(None, \"--editor-edit-format\", help=\"Editor edit format\"),\n    replay: str = typer.Option(\n        None,\n        \"--replay\",\n        help=\"Replay previous .aider.chat.history.md responses from previous benchmark run\",\n    ),\n    keywords: str = typer.Option(\n        None, \"--keywords\", \"-k\", help=\"Only run tests that contain keywords (comma sep)\"\n    ),\n    clean: bool = typer.Option(\n        False, \"--clean\", \"-c\", help=\"Discard the existing testdir and make a clean copy\"\n    ),\n    cont: bool = typer.Option(False, \"--cont\", help=\"Continue the (single) matching testdir\"),\n    make_new: bool = typer.Option(False, \"--new\", \"-n\", help=\"Make a new dated testdir\"),\n    no_unit_tests: bool = typer.Option(False, \"--no-unit-tests\", help=\"Do not run unit tests\"),\n    no_aider: bool = typer.Option(False, \"--no-aider\", help=\"Do not run aider\"),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\", help=\"Verbose output\"),\n    stats_only: bool = typer.Option(\n        False, \"--stats\", \"-s\", help=\"Do not run tests, just collect stats on completed tests\"\n    ),\n    stats_languages: str = typer.Option(\n        None,\n        \"--stats-languages\",\n        help=\"Only include stats for specific languages (comma separated)\",\n    ),\n    diffs_only: bool = typer.Option(False, \"--diffs\", help=\"Just diff the provided stats dirs\"),\n    tries: int = typer.Option(2, \"--tries\", \"-r\", help=\"Number of tries for running tests\"),\n    threads: int = typer.Option(1, \"--threads\", \"-t\", help=\"Number of threads to run in parallel\"),\n    num_tests: int = typer.Option(-1, \"--num-tests\", \"-n\", help=\"Number of tests to run\"),\n    num_ctx: Optional[int] = typer.Option(\n        None, \"--num-ctx\", help=\"Override model context window size\"\n    ),\n    exercises_dir: str = typer.Option(\n        EXERCISES_DIR_DEFAULT, \"--exercises-dir\", help=\"Directory with exercise files\"\n    ),\n):\n    # ... other code\n\n    original_dname = BENCHMARK_DNAME / exercises_dir\n    assert original_dname.exists() and original_dname.is_dir(), original_dname\n\n    exercise_dirs = get_exercise_dirs(original_dname, languages)\n\n    if not exercise_dirs:\n        print(\"No exercise directories found\")\n        return 1\n\n    if clean and dirname.exists():\n        print(\"Cleaning up and replacing\", dirname)\n        dir_files = set(fn.name for fn in dirname.glob(\"*\"))\n        original_files = set(fn.name for fn in original_dname.glob(\"*\"))\n        if dir_files != original_files:\n            print(\"ERROR: will not delete dir that does not look like original tests\", dirname)\n            return\n\n        dest = dirname.parent / \"OLD\" / dirname.name\n        if dest.exists():\n            old_now = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n            dest = dirname.parent / \"OLD\" / (old_now + dirname.name)\n\n        dirname.rename(dest)\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::9",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 806,
      "span_ids": [
        "main"
      ],
      "start_line": 297,
      "end_line": 309,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "@app.command()\ndef main(\n    dirnames: Optional[List[str]] = typer.Argument(None, help=\"Directory names\"),\n    graphs: bool = typer.Option(False, \"--graphs\", help=\"Generate graphs\"),\n    model: str = typer.Option(\"gpt-3.5-turbo\", \"--model\", \"-m\", help=\"Model name\"),\n    sleep: float = typer.Option(\n        0, \"--sleep\", help=\"Sleep seconds between tests when single threaded\"\n    ),\n    languages: str = typer.Option(\n        None, \"--languages\", \"-l\", help=\"Only run tests for specific languages (comma separated)\"\n    ),\n    edit_format: str = typer.Option(None, \"--edit-format\", \"-e\", help=\"Edit format\"),\n    editor_model: str = typer.Option(None, \"--editor-model\", help=\"Editor model name\"),\n    editor_edit_format: str = typer.Option(None, \"--editor-edit-format\", help=\"Editor edit format\"),\n    replay: str = typer.Option(\n        None,\n        \"--replay\",\n        help=\"Replay previous .aider.chat.history.md responses from previous benchmark run\",\n    ),\n    keywords: str = typer.Option(\n        None, \"--keywords\", \"-k\", help=\"Only run tests that contain keywords (comma sep)\"\n    ),\n    clean: bool = typer.Option(\n        False, \"--clean\", \"-c\", help=\"Discard the existing testdir and make a clean copy\"\n    ),\n    cont: bool = typer.Option(False, \"--cont\", help=\"Continue the (single) matching testdir\"),\n    make_new: bool = typer.Option(False, \"--new\", \"-n\", help=\"Make a new dated testdir\"),\n    no_unit_tests: bool = typer.Option(False, \"--no-unit-tests\", help=\"Do not run unit tests\"),\n    no_aider: bool = typer.Option(False, \"--no-aider\", help=\"Do not run aider\"),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\", help=\"Verbose output\"),\n    stats_only: bool = typer.Option(\n        False, \"--stats\", \"-s\", help=\"Do not run tests, just collect stats on completed tests\"\n    ),\n    stats_languages: str = typer.Option(\n        None,\n        \"--stats-languages\",\n        help=\"Only include stats for specific languages (comma separated)\",\n    ),\n    diffs_only: bool = typer.Option(False, \"--diffs\", help=\"Just diff the provided stats dirs\"),\n    tries: int = typer.Option(2, \"--tries\", \"-r\", help=\"Number of tries for running tests\"),\n    threads: int = typer.Option(1, \"--threads\", \"-t\", help=\"Number of threads to run in parallel\"),\n    num_tests: int = typer.Option(-1, \"--num-tests\", \"-n\", help=\"Number of tests to run\"),\n    num_ctx: Optional[int] = typer.Option(\n        None, \"--num-ctx\", help=\"Override model context window size\"\n    ),\n    exercises_dir: str = typer.Option(\n        EXERCISES_DIR_DEFAULT, \"--exercises-dir\", help=\"Directory with exercise files\"\n    ),\n):\n    # ... other code\n\n    if not dirname.exists():\n        print(f\"Copying {original_dname} -> {dirname} ...\")\n        # Only copy the practice subdirs with exercises\n        os.makedirs(dirname, exist_ok=True)\n        for lang_dir in original_dname.iterdir():\n            if not lang_dir.is_dir():\n                continue\n            practice_dir = lang_dir / \"exercises\" / \"practice\"\n            if practice_dir.exists():\n                dest_lang_dir = dirname / lang_dir.name / \"exercises\" / \"practice\"\n                os.makedirs(dest_lang_dir.parent, exist_ok=True)\n                shutil.copytree(practice_dir, dest_lang_dir)\n        print(\"...done\")\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::10",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 795,
      "span_ids": [
        "main"
      ],
      "start_line": 311,
      "end_line": 324,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "@app.command()\ndef main(\n    dirnames: Optional[List[str]] = typer.Argument(None, help=\"Directory names\"),\n    graphs: bool = typer.Option(False, \"--graphs\", help=\"Generate graphs\"),\n    model: str = typer.Option(\"gpt-3.5-turbo\", \"--model\", \"-m\", help=\"Model name\"),\n    sleep: float = typer.Option(\n        0, \"--sleep\", help=\"Sleep seconds between tests when single threaded\"\n    ),\n    languages: str = typer.Option(\n        None, \"--languages\", \"-l\", help=\"Only run tests for specific languages (comma separated)\"\n    ),\n    edit_format: str = typer.Option(None, \"--edit-format\", \"-e\", help=\"Edit format\"),\n    editor_model: str = typer.Option(None, \"--editor-model\", help=\"Editor model name\"),\n    editor_edit_format: str = typer.Option(None, \"--editor-edit-format\", help=\"Editor edit format\"),\n    replay: str = typer.Option(\n        None,\n        \"--replay\",\n        help=\"Replay previous .aider.chat.history.md responses from previous benchmark run\",\n    ),\n    keywords: str = typer.Option(\n        None, \"--keywords\", \"-k\", help=\"Only run tests that contain keywords (comma sep)\"\n    ),\n    clean: bool = typer.Option(\n        False, \"--clean\", \"-c\", help=\"Discard the existing testdir and make a clean copy\"\n    ),\n    cont: bool = typer.Option(False, \"--cont\", help=\"Continue the (single) matching testdir\"),\n    make_new: bool = typer.Option(False, \"--new\", \"-n\", help=\"Make a new dated testdir\"),\n    no_unit_tests: bool = typer.Option(False, \"--no-unit-tests\", help=\"Do not run unit tests\"),\n    no_aider: bool = typer.Option(False, \"--no-aider\", help=\"Do not run aider\"),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\", help=\"Verbose output\"),\n    stats_only: bool = typer.Option(\n        False, \"--stats\", \"-s\", help=\"Do not run tests, just collect stats on completed tests\"\n    ),\n    stats_languages: str = typer.Option(\n        None,\n        \"--stats-languages\",\n        help=\"Only include stats for specific languages (comma separated)\",\n    ),\n    diffs_only: bool = typer.Option(False, \"--diffs\", help=\"Just diff the provided stats dirs\"),\n    tries: int = typer.Option(2, \"--tries\", \"-r\", help=\"Number of tries for running tests\"),\n    threads: int = typer.Option(1, \"--threads\", \"-t\", help=\"Number of threads to run in parallel\"),\n    num_tests: int = typer.Option(-1, \"--num-tests\", \"-n\", help=\"Number of tests to run\"),\n    num_ctx: Optional[int] = typer.Option(\n        None, \"--num-ctx\", help=\"Override model context window size\"\n    ),\n    exercises_dir: str = typer.Option(\n        EXERCISES_DIR_DEFAULT, \"--exercises-dir\", help=\"Directory with exercise files\"\n    ),\n):\n    # ... other code\n\n    test_dnames = sorted(str(d.relative_to(original_dname)) for d in exercise_dirs)\n\n    if keywords:\n        keywords = keywords.split(\",\")\n        test_dnames = [dn for dn in test_dnames for keyword in keywords if keyword in dn]\n\n    random.shuffle(test_dnames)\n    if num_tests > 0:\n        test_dnames = test_dnames[:num_tests]\n\n    # Don't give up when benchmarking\n    LONG_TIMEOUT = 24 * 60 * 60\n    sendchat.RETRY_TIMEOUT = LONG_TIMEOUT\n    base_coder.RETRY_TIMEOUT = LONG_TIMEOUT\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::11",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 893,
      "span_ids": [
        "main"
      ],
      "start_line": 326,
      "end_line": 374,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "@app.command()\ndef main(\n    dirnames: Optional[List[str]] = typer.Argument(None, help=\"Directory names\"),\n    graphs: bool = typer.Option(False, \"--graphs\", help=\"Generate graphs\"),\n    model: str = typer.Option(\"gpt-3.5-turbo\", \"--model\", \"-m\", help=\"Model name\"),\n    sleep: float = typer.Option(\n        0, \"--sleep\", help=\"Sleep seconds between tests when single threaded\"\n    ),\n    languages: str = typer.Option(\n        None, \"--languages\", \"-l\", help=\"Only run tests for specific languages (comma separated)\"\n    ),\n    edit_format: str = typer.Option(None, \"--edit-format\", \"-e\", help=\"Edit format\"),\n    editor_model: str = typer.Option(None, \"--editor-model\", help=\"Editor model name\"),\n    editor_edit_format: str = typer.Option(None, \"--editor-edit-format\", help=\"Editor edit format\"),\n    replay: str = typer.Option(\n        None,\n        \"--replay\",\n        help=\"Replay previous .aider.chat.history.md responses from previous benchmark run\",\n    ),\n    keywords: str = typer.Option(\n        None, \"--keywords\", \"-k\", help=\"Only run tests that contain keywords (comma sep)\"\n    ),\n    clean: bool = typer.Option(\n        False, \"--clean\", \"-c\", help=\"Discard the existing testdir and make a clean copy\"\n    ),\n    cont: bool = typer.Option(False, \"--cont\", help=\"Continue the (single) matching testdir\"),\n    make_new: bool = typer.Option(False, \"--new\", \"-n\", help=\"Make a new dated testdir\"),\n    no_unit_tests: bool = typer.Option(False, \"--no-unit-tests\", help=\"Do not run unit tests\"),\n    no_aider: bool = typer.Option(False, \"--no-aider\", help=\"Do not run aider\"),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\", help=\"Verbose output\"),\n    stats_only: bool = typer.Option(\n        False, \"--stats\", \"-s\", help=\"Do not run tests, just collect stats on completed tests\"\n    ),\n    stats_languages: str = typer.Option(\n        None,\n        \"--stats-languages\",\n        help=\"Only include stats for specific languages (comma separated)\",\n    ),\n    diffs_only: bool = typer.Option(False, \"--diffs\", help=\"Just diff the provided stats dirs\"),\n    tries: int = typer.Option(2, \"--tries\", \"-r\", help=\"Number of tries for running tests\"),\n    threads: int = typer.Option(1, \"--threads\", \"-t\", help=\"Number of threads to run in parallel\"),\n    num_tests: int = typer.Option(-1, \"--num-tests\", \"-n\", help=\"Number of tests to run\"),\n    num_ctx: Optional[int] = typer.Option(\n        None, \"--num-ctx\", help=\"Override model context window size\"\n    ),\n    exercises_dir: str = typer.Option(\n        EXERCISES_DIR_DEFAULT, \"--exercises-dir\", help=\"Directory with exercise files\"\n    ),\n):\n    # ... other code\n\n    if threads == 1:\n        all_results = []\n        for test_path in test_dnames:\n            results = run_test(\n                original_dname,\n                dirname / test_path,\n                model,\n                edit_format,\n                tries,\n                no_unit_tests,\n                no_aider,\n                verbose,\n                commit_hash,\n                replay,\n                editor_model,\n                editor_edit_format,\n                num_ctx,\n                sleep,\n            )\n\n            all_results.append(results)\n            summarize_results(dirname)\n            if sleep:\n                time.sleep(sleep)\n    else:\n        run_test_threaded = lox.thread(threads)(run_test)\n        for test_path in test_dnames:\n            run_test_threaded.scatter(\n                original_dname,\n                dirname / test_path,\n                model,\n                edit_format,\n                tries,\n                no_unit_tests,\n                no_aider,\n                verbose,\n                commit_hash,\n                replay,\n                editor_model,\n                editor_edit_format,\n            )\n        all_results = run_test_threaded.gather(tqdm=True)\n\n    print()\n    print()\n    print()\n    summarize_results(dirname)\n\n    return 0",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::12",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 254,
      "span_ids": [
        "show_diffs"
      ],
      "start_line": 377,
      "end_line": 411,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def show_diffs(dirnames):\n    dirnames = sorted(dirnames)\n\n    all_results = dict((dirname, load_results(dirname)) for dirname in dirnames)\n    testcases = set()\n    for results in all_results.values():\n        testcases.update(result[\"testcase\"] for result in results)\n\n    testcases = sorted(testcases)\n\n    unchanged = set()\n\n    for testcase in testcases:\n        all_outcomes = []\n        for dirname in dirnames:\n            results = all_results[dirname]\n            result = [r for r in results if r[\"testcase\"] == testcase][0]\n\n            outcomes = tuple(result[\"tests_outcomes\"])\n            all_outcomes.append(True in outcomes)\n\n        if len(set(all_outcomes)) == 1:\n            unchanged.add(testcase)\n            continue\n\n        print()\n        print(testcase)\n        for outcome, dirname in zip(all_outcomes, dirnames):\n            print(outcome, f\"{dirname}/{testcase}/.aider.chat.history.md\")\n\n    changed = set(testcases) - unchanged\n    print()\n    print(\"changed:\", len(changed), \",\".join(sorted(changed)))\n    print()\n    print(\"unchanged:\", len(unchanged), \",\".join(sorted(unchanged)))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::13",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 144,
      "span_ids": [
        "load_results"
      ],
      "start_line": 414,
      "end_line": 432,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def load_results(dirname, stats_languages=None):\n    dirname = Path(dirname)\n    all_results = []\n\n    if stats_languages:\n        languages = [lang.strip().lower() for lang in stats_languages.split(\",\")]\n        glob_patterns = [f\"{lang}/exercises/practice/*/.aider.results.json\" for lang in languages]\n    else:\n        glob_patterns = [\"*/exercises/practice/*/.aider.results.json\"]\n\n    for pattern in glob_patterns:\n        for fname in dirname.glob(pattern):\n            try:\n                results = json.loads(fname.read_text())\n                all_results.append(results)\n            except json.JSONDecodeError:\n                print(\"json.JSONDecodeError\", fname)\n                continue\n    return all_results",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::14",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 859,
      "span_ids": [
        "summarize_results"
      ],
      "start_line": 435,
      "end_line": 535,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def summarize_results(dirname, stats_languages=None):\n    all_results = load_results(dirname, stats_languages)\n\n    res = SimpleNamespace()\n    res.total_tests = len(list(Path(dirname).glob(\"*/exercises/practice/*\")))\n\n    try:\n        tries = max(len(results.get(\"tests_outcomes\", [])) for results in all_results if results)\n    except ValueError:\n        tries = 0\n\n    res.dir_name = str(dirname)\n\n    passed_tests = [0] * tries\n\n    res.completed_tests = 0\n    res.duration = 0\n    res.cost = 0\n    res.error_outputs = 0\n    res.user_asks = 0\n    res.test_timeouts = 0\n    res.exhausted_context_windows = 0\n    res.num_malformed_responses = 0\n    res.num_with_malformed_responses = 0\n    res.syntax_errors = 0\n    res.indentation_errors = 0\n    res.lazy_comments = 0\n\n    variants = defaultdict(set)\n\n    for results in all_results:\n        if not results:\n            continue\n\n        res.completed_tests += 1\n        tests_outcomes = results.get(\"tests_outcomes\", [])\n        passed = tests_outcomes and tests_outcomes[-1]\n        if passed:\n            for i in range(len(tests_outcomes) - 1, tries):\n                passed_tests[i] += 1\n\n        res.cost += results.get(\"cost\", 0)\n        res.duration += results.get(\"duration\", 0)\n        res.test_timeouts += results.get(\"test_timeouts\", 0)\n\n        res.error_outputs += results.get(\"num_error_outputs\", 0)\n        res.user_asks += results.get(\"num_user_asks\", 0)\n        res.exhausted_context_windows += results.get(\"num_exhausted_context_windows\", 0)\n        res.num_malformed_responses += results.get(\"num_malformed_responses\", 0)\n        if results.get(\"num_malformed_responses\"):\n            res.num_with_malformed_responses += 1\n        res.lazy_comments += results.get(\"lazy_comments\", 0)\n\n        res.syntax_errors += results.get(\"syntax_errors\", 0)\n        res.indentation_errors += results.get(\"indentation_errors\", 0)\n\n        for key in \"model edit_format commit_hash editor_model editor_edit_format\".split():\n            val = results.get(key)\n            if val:\n                variants[key].add(val)\n\n    if not res.completed_tests:\n        return\n\n    # if res.completed_tests < 133:\n    #    return\n\n    console = Console(highlight=False)\n    console.rule(title=str(dirname))\n\n    commit_hashes = variants[\"commit_hash\"]\n    versions = get_versions(commit_hashes)\n    date = dirname.name[:10]\n\n    def show(stat, red=\"red\"):\n        val = getattr(res, stat)\n        style = red if val else None\n        console.print(f\"  {stat}: {val}\", style=style)\n\n    percents = dict()\n    for i in range(tries):\n        pass_rate = 100 * passed_tests[i] / res.completed_tests\n        percents[i] = pass_rate\n        # console.print(f\"{pass_rate:.1f}% correct after try {i+1}\")\n        setattr(res, f\"pass_rate_{i + 1}\", f\"{pass_rate:.1f}\")\n        setattr(res, f\"pass_num_{i + 1}\", passed_tests[i])\n\n    print(f\"- dirname: {dirname.name}\")\n    style = None if res.completed_tests == res.total_tests else \"red\"\n    console.print(f\"  test_cases: {res.completed_tests}\", style=style)\n    for key, val in variants.items():\n        if len(val) > 1:\n            style = \"red\"\n        else:\n            style = None\n        val = \", \".join(map(str, val))\n        setattr(res, key, val)\n        console.print(f\"  {key}: {val}\", style=style)\n\n    for i in range(tries):\n        print(f\"  pass_rate_{i + 1}: {percents[i]:.1f}\")\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::15",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 373,
      "span_ids": [
        "summarize_results"
      ],
      "start_line": 536,
      "end_line": 579,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def summarize_results(dirname, stats_languages=None):\n    # ... other code\n    for i in range(tries):\n        print(f\"  pass_num_{i + 1}: {passed_tests[i]}\")\n\n    pct_well_formed = 1.0 - res.num_with_malformed_responses / res.completed_tests\n    print(f\"  percent_cases_well_formed: {pct_well_formed * 100:.1f}\")\n\n    show(\"error_outputs\")\n    show(\"num_malformed_responses\")\n    show(\"num_with_malformed_responses\")\n    show(\"user_asks\")\n    show(\"lazy_comments\")\n    show(\"syntax_errors\")\n    show(\"indentation_errors\")\n    show(\"exhausted_context_windows\")\n    show(\"test_timeouts\")\n    print(f\"  total_tests: {res.total_tests}\")\n\n    if variants[\"model\"]:\n        a_model = set(variants[\"model\"]).pop()\n        command = f\"aider --model {a_model}\"\n        print(f\"  command: {command}\")\n\n    print(f\"  date: {date}\")\n    print(\"  versions:\", \",\".join(versions))\n\n    res.avg_duration = res.duration / res.completed_tests\n    print(f\"  seconds_per_case: {res.avg_duration:.1f}\")\n\n    print(f\"  total_cost: {res.cost:.4f}\")\n\n    res.avg_cost = res.cost / res.completed_tests\n\n    projected_cost = res.avg_cost * res.total_tests\n\n    print()\n    print(\n        f\"costs: ${res.avg_cost:.4f}/test-case, ${res.cost:.2f} total,\"\n        f\" ${projected_cost:.2f} projected\"\n    )\n\n    console.rule()\n\n    # print(json.dumps(vars(res), indent=4, sort_keys=True))\n    return res",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::16",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 114,
      "span_ids": [
        "get_versions"
      ],
      "start_line": 582,
      "end_line": 596,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_versions(commit_hashes):\n    versions = set()\n    for hsh in commit_hashes:\n        if not hsh:\n            continue\n        hsh = hsh.split(\"-\")[0]\n        try:\n            version = subprocess.check_output(\n                [\"git\", \"show\", f\"{hsh}:aider/__init__.py\"], universal_newlines=True\n            )\n            version = re.search(r'__version__ = \"(.*)\"', version).group(1)\n            versions.add(version)\n        except subprocess.CalledProcessError:\n            pass\n    return versions",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::17",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 229,
      "span_ids": [
        "run_test",
        "get_replayed_content"
      ],
      "start_line": 599,
      "end_line": 627,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_replayed_content(replay_dname, test_dname):\n    replay_dname = Path(replay_dname)\n    test_dname = Path(test_dname)\n    dump(replay_dname, test_dname)\n\n    test_name = test_dname.name\n    replay_fname = replay_dname / test_name / \".aider.chat.history.md\"\n    dump(replay_fname)\n\n    res = replay_fname.read_text()\n    return res\n\n    res = res.splitlines(keepends=True)\n    res = [line for line in res if not line.startswith(\"> \") and not line.startswith(\"#### \")]\n    return \"\".join(res)\n\n\ndef run_test(original_dname, testdir, *args, **kwargs):\n    try:\n        return run_test_real(original_dname, testdir, *args, **kwargs)\n    except Exception as err:\n        print(\"=\" * 40)\n        print(\"Test failed\")\n        print(err)\n        traceback.print_exc()\n\n        testdir = Path(testdir)\n        results_fname = testdir / \".aider.results.json\"\n        results_fname.write_text(json.dumps(dict(exception=str(err))))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::18",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 816,
      "span_ids": [
        "run_test_real"
      ],
      "start_line": 630,
      "end_line": 748,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def run_test_real(\n    original_dname,\n    testdir,\n    model_name,\n    edit_format,\n    tries,\n    no_unit_tests,\n    no_aider,\n    verbose,\n    commit_hash,\n    replay,\n    editor_model,\n    editor_edit_format,\n    num_ctx=None,\n    sleep=0,\n):\n    if not os.path.isdir(testdir):\n        print(\"Not a dir:\", testdir)\n        return\n\n    testdir = Path(testdir)\n\n    history_fname = testdir / \".aider.chat.history.md\"\n\n    results_fname = testdir / \".aider.results.json\"\n    if results_fname.exists():\n        try:\n            res = json.loads(results_fname.read_text())\n            # if res.get(\"test_timeouts\", 0) > 0:\n            #    print(f\"{results_fname} test timeouts, redoing...\")\n            # else:\n            return res\n        except JSONDecodeError:\n            print(f\"{results_fname} failed to parse, redoing...\")\n\n    # Read solution and test files from config\n    fnames = []\n    config_file = testdir / \".meta/config.json\"\n    if not config_file.exists():\n        raise ValueError(f\"No config file found: {config_file}\")\n\n    with open(config_file) as f:\n        config = json.loads(f.read())\n\n    # Get file sets from config\n    test_files = config.get(\"files\", {}).get(\"test\", [])\n    example_files = config.get(\"files\", {}).get(\"example\", [])\n    solution_files = set(config.get(\"files\", {}).get(\"solution\", []))\n\n    # Forcibly ignore certain files not covered by test_files and example_files\n    ignore_files = set(\n        [\n            \"CMakeLists.txt\",\n            \"Cargo.toml\",\n        ]\n    )\n\n    # Add all files under .meta and .docs directories\n    ignore_files.update(str(p.relative_to(testdir)) for p in testdir.glob(\".meta/**/*\"))\n    ignore_files.update(str(p.relative_to(testdir)) for p in testdir.glob(\".docs/**/*\"))\n\n    # Also ignore test & example files\n    ignore_files.update(test_files)\n    ignore_files.update(example_files)\n\n    # Remove any ignore files from the solution set that LLM will edit\n    solution_files.difference_update(ignore_files)\n\n    # Copy all solution files\n    for file_path in solution_files:\n        src = testdir / Path(file_path)\n        if src.exists():\n            fnames.append(src)\n            # restore the original file, in case we interrupted a prev run\n            # Find the original file in the language-specific practice dir\n            lang_part = str(testdir).split(\"/exercises/practice/\")[0]\n            original_fname = (\n                original_dname\n                / Path(lang_part).name\n                / \"exercises\"\n                / \"practice\"\n                / testdir.name\n                / file_path\n            )\n            if original_fname.exists():\n                os.makedirs(src.parent, exist_ok=True)\n                shutil.copy(original_fname, src)\n        else:\n            print(f\"Warning: Solution file not found: {src}\")\n\n    file_list = \" \".join(fname.name for fname in fnames)\n\n    instructions = \"\"\n\n    introduction = testdir / \".docs/introduction.md\"\n    if introduction.exists():\n        instructions += introduction.read_text()\n    instructions += (testdir / \".docs/instructions.md\").read_text()\n    instructions_append = testdir / \".docs/instructions.append.md\"\n    if instructions_append.exists():\n        instructions += instructions_append.read_text()\n\n    instructions += prompts.instructions_addendum.format(file_list=file_list)\n\n    io = InputOutput(\n        pretty=True,\n        yes=True,\n        chat_history_file=history_fname,\n    )\n\n    # weak_model_name = model_name\n    weak_model_name = None\n\n    main_model = models.Model(\n        model_name,\n        weak_model=weak_model_name,\n        editor_model=editor_model,\n        editor_edit_format=editor_edit_format,\n    )\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::19",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 802,
      "span_ids": [
        "run_test_real"
      ],
      "start_line": 750,
      "end_line": 860,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def run_test_real(\n    original_dname,\n    testdir,\n    model_name,\n    edit_format,\n    tries,\n    no_unit_tests,\n    no_aider,\n    verbose,\n    commit_hash,\n    replay,\n    editor_model,\n    editor_edit_format,\n    num_ctx=None,\n    sleep=0,\n):\n    # ... other code\n\n    if num_ctx:\n        if not main_model.extra_params:\n            main_model.extra_params = {}\n        main_model.extra_params[\"num_ctx\"] = num_ctx\n    edit_format = edit_format or main_model.edit_format\n\n    dump(main_model)\n    dump(edit_format)\n    show_fnames = \",\".join(map(str, fnames))\n    print(\"fnames:\", show_fnames)\n\n    coder = Coder.create(\n        main_model,\n        edit_format,\n        io,\n        fnames=fnames,\n        use_git=False,\n        stream=False,\n        verbose=verbose,\n        # auto_lint=False,  # disabled for code-in-json experiments\n        cache_prompts=True,\n        suggest_shell_commands=False,\n        ignore_mentions=ignore_files,\n    )\n    dump(coder.ignore_mentions)\n\n    coder.show_announcements()\n    coder.get_file_mentions = lambda x: set()  # No loading of any other files\n\n    timeouts = 0\n\n    syntax_errors = 0\n    indentation_errors = 0\n    lazy_comments = 0\n\n    dur = 0\n    test_outcomes = []\n    for i in range(tries):\n        start = time.time()\n\n        if no_aider:\n            pass\n        elif replay:\n            response = get_replayed_content(replay, testdir)\n            coder.partial_response_content = response\n\n            show = response.splitlines(keepends=True)\n            show = [\">> \" + line for line in show]\n            io.append_chat_history(\"\".join(show))\n\n            coder.apply_updates()\n        else:\n            response = coder.run(with_message=instructions, preproc=False)\n\n        dur += time.time() - start\n\n        if not no_aider:\n            pat = r\"^[+]? *[#].* [.][.][.] \"\n            # Count the number of lines that match pat in response\n            dump(response)\n            lazy_comments += len(re.findall(pat, response, re.MULTILINE))\n            dump(lazy_comments)\n\n        if coder.last_keyboard_interrupt:\n            raise KeyboardInterrupt\n\n        if no_unit_tests:\n            break\n\n        try:\n            errors = run_unit_tests(original_dname, testdir, history_fname, test_files)\n        except subprocess.TimeoutExpired:\n            # try:\n            #    errors = run_unit_tests(original_dname, testdir, history_fname, test_files)\n            # except subprocess.TimeoutExpired:\n            errors = \"Tests timed out!\"\n            timeouts += 1\n\n        if errors:\n            test_outcomes.append(False)\n        else:\n            test_outcomes.append(True)\n            break\n\n        if replay:\n            io.append_chat_history(errors)\n\n        errors = errors.splitlines()\n\n        syntax_errors += sum(1 for line in errors if line.startswith(\"SyntaxError\"))\n        indentation_errors += sum(1 for line in errors if line.startswith(\"IndentationError\"))\n\n        print(errors[-1])\n        errors = \"\\n\".join(errors)\n        instructions = errors\n        instructions += prompts.test_failures.format(file_list=file_list)\n\n    # Clean up build directories after all attempts\n    # Rust target/debug\n    target_dir = testdir / \"target\" / \"debug\"\n    if target_dir.exists():\n        try:\n            shutil.rmtree(target_dir)\n            if verbose:\n                print(f\"Cleaned up Rust target/debug directory: {target_dir}\")\n        except (OSError, shutil.Error, PermissionError) as e:\n            if verbose:\n                print(f\"Failed to clean up Rust target/debug directory: {e}\")\n\n    # Java build directories\n    java_build_dir = testdir / \"build\"\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::20",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 480,
      "span_ids": [
        "run_test_real"
      ],
      "start_line": 861,
      "end_line": 913,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def run_test_real(\n    original_dname,\n    testdir,\n    model_name,\n    edit_format,\n    tries,\n    no_unit_tests,\n    no_aider,\n    verbose,\n    commit_hash,\n    replay,\n    editor_model,\n    editor_edit_format,\n    num_ctx=None,\n    sleep=0,\n):\n    # ... other code\n    if java_build_dir.exists():\n        try:\n            shutil.rmtree(java_build_dir)\n            if verbose:\n                print(f\"Cleaned up Java build directory: {java_build_dir}\")\n        except (OSError, shutil.Error, PermissionError) as e:\n            if verbose:\n                print(f\"Failed to clean up Java build directory: {e}\")\n\n    # Node.js node_modules directories\n    node_modules_dir = testdir / \"node_modules\"\n    if node_modules_dir.exists():\n        try:\n            shutil.rmtree(node_modules_dir)\n            if verbose:\n                print(f\"Cleaned up Node.js node_modules directory: {node_modules_dir}\")\n        except (OSError, shutil.Error, PermissionError) as e:\n            if verbose:\n                print(f\"Failed to clean up Node.js node_modules directory: {e}\")\n\n    results = dict(\n        testdir=str(testdir),\n        testcase=testdir.name,\n        model=main_model.name,\n        edit_format=edit_format,\n        tests_outcomes=test_outcomes,\n        cost=coder.total_cost,\n        duration=dur,\n        test_timeouts=timeouts,\n        commit_hash=commit_hash,\n        num_error_outputs=io.num_error_outputs,\n        num_user_asks=io.num_user_asks,\n        num_exhausted_context_windows=coder.num_exhausted_context_windows,\n        num_malformed_responses=coder.num_malformed_responses,\n        syntax_errors=syntax_errors,\n        indentation_errors=indentation_errors,\n        lazy_comments=lazy_comments,  # Add the count of pattern matches to the results\n        chat_hashes=list(\n            zip(\n                coder.chat_completion_call_hashes,\n                coder.chat_completion_response_hashes,\n            )\n        ),\n    )\n\n    if edit_format == \"architect\":\n        results[\"editor_model\"] = main_model.editor_model.name if main_model.editor_model else None\n        results[\"editor_edit_format\"] = main_model.editor_edit_format\n    dump(results)\n\n    results_fname.write_text(json.dumps(results, indent=4))\n\n    return results",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/benchmark.py::21",
    "metadata": {
      "file_path": "benchmark/benchmark.py",
      "file_name": "benchmark.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 537,
      "span_ids": [
        "cleanup_test_output",
        "run_unit_tests",
        "impl:8"
      ],
      "start_line": 916,
      "end_line": 992,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def run_unit_tests(original_dname, testdir, history_fname, test_files):\n    timeout = 60 * 3\n\n    # Map of file extensions to test commands\n    TEST_COMMANDS = {\n        \".py\": [\"pytest\"],\n        \".rs\": [\"cargo\", \"test\", \"--\", \"--include-ignored\"],\n        \".go\": [\"go\", \"test\", \"./...\"],\n        \".js\": [\"/aider/benchmark/npm-test.sh\"],\n        \".cpp\": [\"/aider/benchmark/cpp-test.sh\"],\n        \".java\": [\"./gradlew\", \"test\"],\n    }\n\n    # Get unique file extensions from test files\n    extensions = {Path(f).suffix for f in test_files}\n\n    # Find matching test command\n    command = None\n    for ext in extensions:\n        if ext in TEST_COMMANDS:\n            command = TEST_COMMANDS[ext]\n            break\n\n    if not command:\n        raise ValueError(f\"No test command found for files with extensions: {extensions}\")\n\n    # Copy test files from original directory\n    for file_path in test_files:\n        src = original_dname / testdir.name / file_path\n        dst = testdir / file_path\n        if src.exists():\n            os.makedirs(dst.parent, exist_ok=True)\n            shutil.copy(src, dst)\n\n    # Remove @Disabled annotations from Java test files\n    for file_path in test_files:\n        if file_path.endswith(\".java\"):\n            test_file = testdir / file_path\n            if test_file.exists():\n                content = test_file.read_text()\n                content = re.sub(r\"@Disabled\\([^)]*\\)\\s*\\n\", \"\", content)\n                test_file.write_text(content)\n\n    print(\" \".join(command))\n\n    result = subprocess.run(\n        command,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        text=True,\n        timeout=timeout,\n        cwd=testdir,\n    )\n\n    success = result.returncode == 0\n    res = result.stdout\n    res = cleanup_test_output(res, testdir)\n    dump(res)\n\n    with history_fname.open(\"a\") as fh:\n        fh.write(f\"```\\n{res}\\n```\")\n\n    if not success:\n        print(f\"Tests failed: {testdir}\")\n        return res\n\n\ndef cleanup_test_output(output, testdir):\n    # remove timing info, to avoid randomizing the response to GPT\n    res = re.sub(r\"\\bin \\d+\\.\\d+s\\b\", \"\", output)\n    res = res.replace(str(testdir), str(testdir.name))\n    return res\n\n\nif __name__ == \"__main__\":\n    app()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/over_time.py::1",
    "metadata": {
      "file_path": "benchmark/over_time.py",
      "file_name": "over_time.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 232,
      "span_ids": [
        "imports",
        "ModelData.color",
        "ModelData"
      ],
      "start_line": 1,
      "end_line": 38,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from dataclasses import dataclass\nfrom datetime import date\nfrom typing import Dict, List, Tuple\n\nimport matplotlib.pyplot as plt\nimport yaml\nfrom imgcat import imgcat\nfrom matplotlib import rc\n\n\n@dataclass\nclass ModelData:\n    name: str\n    release_date: date\n    pass_rate: float\n\n    @property\n    def color(self) -> str:\n        model = self.name.lower()\n        if \"gemini\" in model and \"pro\" in model:\n            return \"magenta\"\n        if \"qwen\" in model:\n            return \"darkblue\"\n        if \"mistral\" in model:\n            return \"cyan\"\n        if \"haiku\" in model:\n            return \"pink\"\n        if \"deepseek\" in model:\n            return \"brown\"\n        if \"sonnet\" in model:\n            return \"orange\"\n        if \"-4o\" in model:\n            return \"purple\"\n        if \"gpt-4\" in model:\n            return \"red\"\n        if \"gpt-3.5\" in model:\n            return \"green\"\n        return \"lightblue\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/over_time.py::2",
    "metadata": {
      "file_path": "benchmark/over_time.py",
      "file_name": "over_time.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 224,
      "span_ids": [
        "ModelData.legend_label"
      ],
      "start_line": 40,
      "end_line": 63,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "@dataclass\nclass ModelData:\n\n    @property\n    def legend_label(self) -> str:\n        model = self.name.lower()\n        if \"gemini\" in model and \"pro\" in model:\n            return \"Gemini 1.5 Pro\"\n        if \"claude-3-sonnet\" in model:\n            return \"Sonnet\"\n        if \"o1-preview\" in model:\n            return \"O1 Preview\"\n        if \"gpt-3.5\" in model:\n            return \"GPT-3.5 Turbo\"\n        if \"gpt-4-\" in model and \"-4o\" not in model:\n            return \"GPT-4\"\n        if \"qwen\" in model:\n            return \"Qwen\"\n        if \"-4o\" in model:\n            return \"GPT-4o\"\n        if \"haiku\" in model:\n            return \"Haiku\"\n        if \"deepseek\" in model:\n            return \"DeepSeek\"\n        if \"mistral\" in model:\n            return \"Mistral\"\n        return model",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/over_time.py::3",
    "metadata": {
      "file_path": "benchmark/over_time.py",
      "file_name": "over_time.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 297,
      "span_ids": [
        "BenchmarkPlotter.create_figure",
        "BenchmarkPlotter.load_data",
        "BenchmarkPlotter",
        "BenchmarkPlotter.__init__",
        "BenchmarkPlotter.setup_plot_style"
      ],
      "start_line": 66,
      "end_line": 99,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class BenchmarkPlotter:\n    LABEL_FONT_SIZE = 16\n\n    def __init__(self):\n        self.setup_plot_style()\n\n    def setup_plot_style(self):\n        plt.rcParams[\"hatch.linewidth\"] = 0.5\n        plt.rcParams[\"hatch.color\"] = \"#444444\"\n        rc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": [\"Helvetica\"], \"size\": 10})\n        plt.rcParams[\"text.color\"] = \"#444444\"\n\n    def load_data(self, yaml_file: str) -> List[ModelData]:\n        with open(yaml_file, \"r\") as file:\n            data = yaml.safe_load(file)\n\n        models = []\n        for entry in data:\n            if \"released\" in entry and \"pass_rate_2\" in entry:\n                model = ModelData(\n                    name=entry[\"model\"].split(\"(\")[0].strip(),\n                    release_date=entry[\"released\"],\n                    pass_rate=entry[\"pass_rate_2\"],\n                )\n                models.append(model)\n        return models\n\n    def create_figure(self) -> Tuple[plt.Figure, plt.Axes]:\n        fig, ax = plt.subplots(figsize=(12, 8))\n        ax.grid(axis=\"y\", zorder=0, lw=0.2)\n        for spine in ax.spines.values():\n            spine.set_edgecolor(\"#DDDDDD\")\n            spine.set_linewidth(0.5)\n        return fig, ax",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/over_time.py::4",
    "metadata": {
      "file_path": "benchmark/over_time.py",
      "file_name": "over_time.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 264,
      "span_ids": [
        "BenchmarkPlotter.plot_model_series"
      ],
      "start_line": 101,
      "end_line": 131,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class BenchmarkPlotter:\n\n    def plot_model_series(self, ax: plt.Axes, models: List[ModelData]):\n        # Group models by color\n        color_groups: Dict[str, List[ModelData]] = {}\n        for model in models:\n            if model.color not in color_groups:\n                color_groups[model.color] = []\n            color_groups[model.color].append(model)\n\n        # Plot each color group\n        for color, group in color_groups.items():\n            sorted_group = sorted(group, key=lambda x: x.release_date)\n            dates = [m.release_date for m in sorted_group]\n            rates = [m.pass_rate for m in sorted_group]\n\n            # Plot line\n            ax.plot(dates, rates, c=color, alpha=0.5, linewidth=1)\n\n            # Plot points\n            ax.scatter(dates, rates, c=color, alpha=0.5, s=120)\n\n            # Add label for first point\n            first_model = sorted_group[0]\n            ax.annotate(\n                first_model.legend_label,\n                (first_model.release_date, first_model.pass_rate),\n                xytext=(10, 5),\n                textcoords=\"offset points\",\n                color=color,\n                alpha=0.8,\n                fontsize=self.LABEL_FONT_SIZE,\n            )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/over_time.py::5",
    "metadata": {
      "file_path": "benchmark/over_time.py",
      "file_name": "over_time.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 311,
      "span_ids": [
        "BenchmarkPlotter.set_labels_and_style",
        "BenchmarkPlotter.save_and_display",
        "main",
        "BenchmarkPlotter.plot",
        "impl"
      ],
      "start_line": 133,
      "end_line": 169,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class BenchmarkPlotter:\n\n    def set_labels_and_style(self, ax: plt.Axes):\n        ax.set_xlabel(\"Model release date\", fontsize=18, color=\"#555\")\n        ax.set_ylabel(\n            \"Aider code editing benchmark,\\npercent completed correctly\", fontsize=18, color=\"#555\"\n        )\n        ax.set_title(\"LLM code editing skill by model release date\", fontsize=20)\n        ax.set_ylim(30, 90)\n        plt.xticks(fontsize=14, rotation=45, ha=\"right\")\n        plt.tight_layout(pad=1.0)\n\n    def save_and_display(self, fig: plt.Figure):\n        plt.savefig(\"aider/website/assets/models-over-time.png\")\n        plt.savefig(\"aider/website/assets/models-over-time.svg\")\n        imgcat(fig)\n\n    def plot(self, yaml_file: str):\n        models = self.load_data(yaml_file)\n        fig, ax = self.create_figure()\n        self.plot_model_series(ax, models)\n        self.set_labels_and_style(ax)\n        self.save_and_display(fig)\n\n\ndef main():\n    plotter = BenchmarkPlotter()\n    models = plotter.load_data(\"aider/website/_data/edit_leaderboard.yml\")\n\n    # Print release dates and model names\n    for model in sorted(models, key=lambda x: x.release_date):\n        print(f\"{model.release_date}: {model.name}\")\n\n    plotter.plot(\"aider/website/_data/edit_leaderboard.yml\")\n\n\nif __name__ == \"__main__\":\n    main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/plots.py::1",
    "metadata": {
      "file_path": "benchmark/plots.py",
      "file_name": "plots.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 495,
      "span_ids": [
        "imports",
        "plot_timing"
      ],
      "start_line": 1,
      "end_line": 59,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom imgcat import imgcat\n\nfrom aider.dump import dump  # noqa: F401\n\n\ndef plot_timing(df):\n    \"\"\"plot a graph showing the average duration of each (model, edit_format)\"\"\"\n    plt.rcParams[\"hatch.linewidth\"] = 0.5\n    plt.rcParams[\"hatch.color\"] = \"#444444\"\n\n    from matplotlib import rc\n\n    rc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": [\"Helvetica\"], \"size\": 10})\n\n    fig, ax = plt.subplots(figsize=(6, 4))\n    ax.grid(axis=\"y\", zorder=0, lw=0.2)\n\n    zorder = 1\n    grouped = df.groupby([\"model\", \"edit_format\"])[\"avg_duration\"].mean().unstack()\n    num_models, num_formats = grouped.shape\n\n    pos = np.array(range(num_models))\n    width = 0.8 / num_formats\n\n    formats = grouped.columns\n    models = grouped.index\n\n    for i, fmt in enumerate(formats):\n        edge = dict(edgecolor=\"#ffffff\", linewidth=1.5)\n        color = \"#b3e6a8\" if \"diff\" in fmt else \"#b3d1e6\"\n        hatch = \"////\" if \"func\" in fmt else \"\"\n        rects = ax.bar(\n            pos + i * width,\n            grouped[fmt],\n            width * 0.95,\n            label=fmt,\n            color=color,\n            hatch=hatch,\n            zorder=zorder + 1,\n            **edge,\n        )\n        ax.bar_label(rects, padding=4, labels=[f\"{v:.1f}s\" for v in grouped[fmt]], size=6)\n\n    ax.set_xticks([p + 0.5 * width for p in pos])\n    ax.set_xticklabels(models)\n\n    ax.set_ylabel(\"Average GPT response time\\nper exercise (sec)\")\n    ax.set_title(\"GPT Code Editing Speed\\n(time per coding task)\")\n    ax.legend(\n        title=\"Edit Format\",\n        loc=\"upper left\",\n    )\n    ax.set_ylim(top=max(grouped.max()) * 1.1)  # Set y-axis limit to 10% more than the max value\n\n    plt.tight_layout()\n    plt.savefig(\"tmp_timing.svg\")\n    imgcat(fig)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/plots.py::2",
    "metadata": {
      "file_path": "benchmark/plots.py",
      "file_name": "plots.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 818,
      "span_ids": [
        "plot_outcomes"
      ],
      "start_line": 62,
      "end_line": 167,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def plot_outcomes(df, repeats, repeat_hi, repeat_lo, repeat_avg):\n    tries = [df.groupby([\"model\", \"edit_format\"])[\"pass_rate_2\"].mean()]\n    if True:\n        tries += [df.groupby([\"model\", \"edit_format\"])[\"pass_rate_1\"].mean()]\n\n    plt.rcParams[\"hatch.linewidth\"] = 0.5\n    plt.rcParams[\"hatch.color\"] = \"#444444\"\n\n    from matplotlib import rc\n\n    rc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": [\"Helvetica\"], \"size\": 10})\n\n    fig, ax = plt.subplots(figsize=(6, 4))\n    ax.grid(axis=\"y\", zorder=0, lw=0.2)\n\n    zorder = 1\n    for grouped in tries:\n        zorder += 1\n        df = grouped.unstack()\n        num_models, num_formats = df.shape\n\n        pos = np.array(range(num_models))\n        width = 0.8 / num_formats\n\n        formats = df.columns\n        models = df.index\n\n        for i, fmt in enumerate(formats):\n            if zorder > 1:\n                edge = dict(\n                    edgecolor=\"#ffffff\",\n                    linewidth=1.5,\n                )\n            else:\n                edge = dict()\n            if zorder == 2:\n                edge[\"label\"] = fmt\n\n            color = \"#b3e6a8\" if \"diff\" in fmt else \"#b3d1e6\"\n            hatch = \"////\" if \"func\" in fmt else \"\"\n            rects = ax.bar(\n                pos + i * width,\n                df[fmt],\n                width * 0.95,\n                color=color,\n                hatch=hatch,\n                zorder=zorder,\n                **edge,\n            )\n            if zorder == 2:\n                ax.bar_label(rects, padding=4, labels=[f\"{v:.0f}%\" for v in df[fmt]], size=6)\n\n    if len(repeats):\n        ax.errorbar(\n            1.4,\n            repeat_avg,\n            yerr=[[repeat_lo], [repeat_hi]],\n            fmt=\"none\",\n            zorder=5,\n            capsize=2.5,\n            elinewidth=1,\n            markeredgewidth=1,\n        )\n\n    ax.set_xticks([p + 0.5 * width for p in pos])\n    model_labels = []\n    for model in models:\n        pieces = model.split(\"-\")\n        ml = \"-\".join(pieces[:2]) + \"-\\n\" + \"-\".join(pieces[2:])\n        model_labels.append(ml)\n\n    ax.set_xticklabels(model_labels)\n\n    top = 95\n    ax.annotate(\n        \"First attempt,\\nbased on\\nnatural language\\ninstructions\",\n        xy=(2.20, 41),\n        xytext=(2, top),\n        horizontalalignment=\"center\",\n        verticalalignment=\"top\",\n        arrowprops={\"arrowstyle\": \"->\", \"connectionstyle\": \"arc3,rad=0.3\"},\n    )\n    ax.annotate(\n        \"Second attempt,\\nincluding unit test\\nerror output\",\n        xy=(2.55, 56),\n        xytext=(3.5, top),\n        horizontalalignment=\"center\",\n        verticalalignment=\"top\",\n        arrowprops={\"arrowstyle\": \"->\", \"connectionstyle\": \"arc3,rad=0.3\"},\n    )\n\n    ax.set_ylabel(\"Percent of exercises completed successfully\")\n    # ax.set_xlabel(\"Model\")\n    ax.set_title(\"GPT Code Editing Skill\\n(percent coding tasks correct)\")\n    ax.legend(\n        title=\"Edit Format\",\n        loc=\"upper left\",\n        # bbox_to_anchor=(0.95, 0.95),\n    )\n    ax.set_ylim(top=100)\n\n    plt.tight_layout()\n    plt.savefig(\"tmp.svg\")\n    imgcat(fig)\n\n    # df.to_csv(\"tmp.benchmarks.csv\")\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/plots.py::3",
    "metadata": {
      "file_path": "benchmark/plots.py",
      "file_name": "plots.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 771,
      "span_ids": [
        "plot_outcomes_claude"
      ],
      "start_line": 170,
      "end_line": 275,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def plot_outcomes_claude(df):\n    print(df)\n\n    # Fix wrong column label\n    df[\"model\"] = df[\"model\"].replace(\"gpt-4-0314\", \"gpt-4-0613\")\n\n    tries = [\n        df[[\"model\", \"pass_rate_2\"]],\n        df[[\"model\", \"pass_rate_1\"]],\n    ]\n\n    plt.rcParams[\"hatch.linewidth\"] = 0.5\n    plt.rcParams[\"hatch.color\"] = \"#444444\"\n\n    from matplotlib import rc\n\n    rc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": [\"Helvetica\"], \"size\": 10})\n\n    fig, ax = plt.subplots(figsize=(6, 4))\n    ax.grid(axis=\"y\", zorder=0, lw=0.2)\n\n    zorder = 1\n    for df in tries:\n        zorder += 1\n        print(df)\n\n        num_models, _ = df.shape\n        num_formats = 1\n\n        pos = np.array(range(num_models))\n        width = 0.6 / num_formats\n\n        if zorder > 1:\n            edge = dict(\n                edgecolor=\"#ffffff\",\n                linewidth=1.5,\n            )\n        else:\n            edge = dict()\n        if zorder == 2:\n            edge[\"label\"] = \"??\"\n\n        color = [\n            \"#b3e6a8\",\n            \"#b3e6a8\",\n            \"#b3e6a8\",\n            \"#b3d1e6\",\n        ]\n        hatch = [  # noqa: F841\n            \"\",\n            \"\",\n            \"\",\n            \"\",\n            \"////\",\n            \"////\",\n            \"////\",\n            \"\",\n            \"////\",\n        ]\n        hatch = [  # noqa: F841\n            \"////\",\n            \"////\",\n            \"////\",\n            \"////\",\n            \"\",\n            \"\",\n            \"\",\n            \"////\",\n            \"\",\n        ]\n        rects = ax.bar(\n            pos + 0.5 * width,\n            df.iloc[:, 1],\n            width * 0.95,\n            color=color,\n            # hatch=hatch,\n            # zorder=zorder,\n            **edge,\n        )\n        if zorder == 2:\n            ax.bar_label(rects, padding=4, labels=[f\"{v:.0f}%\" for v in df.iloc[:, 1]], size=6)\n\n    ax.set_xticks([p + 0.5 * width for p in pos])\n\n    models = df.iloc[:, 0]\n    model_map = {\n        \"gpt-4-0613\": \"gpt-4-\\n0613\",\n        \"gpt-4-0125-preview\": \"gpt-4-\\n0125-preview\",\n        \"gpt-4-1106-preview\": \"gpt-4-\\n1106-preview\",\n        \"gpt-4-turbo-2024-04-09\": \"gpt-4-turbo-\\n2024-04-09\\n(GPT-4 Turbo with Vision)\",\n    }\n    model_labels = []\n    for model in models:\n        ml = model_map.get(model, model)\n        model_labels.append(ml)\n    ax.set_xticklabels(model_labels, rotation=0)\n\n    top = 95\n    ax.annotate(\n        \"First attempt,\\nbased on\\nnatural language\\ninstructions\",\n        xy=(1.0, 53),\n        xytext=(0.75, top),\n        horizontalalignment=\"center\",\n        verticalalignment=\"top\",\n        arrowprops={\"arrowstyle\": \"->\", \"connectionstyle\": \"arc3,rad=0.3\"},\n    )\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/plots.py::4",
    "metadata": {
      "file_path": "benchmark/plots.py",
      "file_name": "plots.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 179,
      "span_ids": [
        "plot_outcomes_claude"
      ],
      "start_line": 276,
      "end_line": 298,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def plot_outcomes_claude(df):\n    # ... other code\n    ax.annotate(\n        \"Second attempt,\\nincluding unit test\\nerror output\",\n        xy=(1.55, 65),\n        xytext=(1.9, top),\n        horizontalalignment=\"center\",\n        verticalalignment=\"top\",\n        arrowprops={\"arrowstyle\": \"->\", \"connectionstyle\": \"arc3,rad=0.3\"},\n    )\n\n    ax.set_ylabel(\"Percent of exercises completed successfully\")\n    # ax.set_xlabel(\"Model\")\n    ax.set_title(\"Code Editing Skill\")\n    # ax.legend(\n    #    title=\"Model family\",\n    #    loc=\"upper left\",\n    # )\n    ax.set_ylim(top=100)\n\n    plt.tight_layout()\n    plt.savefig(\"tmp.svg\")\n    imgcat(fig)\n\n    # df.to_csv(\"tmp.benchmarks.csv\")\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/plots.py::5",
    "metadata": {
      "file_path": "benchmark/plots.py",
      "file_name": "plots.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 900,
      "span_ids": [
        "plot_refactoring"
      ],
      "start_line": 301,
      "end_line": 418,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def plot_refactoring(df):\n    tries = [df.groupby([\"model\", \"edit_format\"])[\"pass_rate_1\"].mean()]\n\n    plt.rcParams[\"hatch.linewidth\"] = 0.5\n    plt.rcParams[\"hatch.color\"] = \"#444444\"\n\n    from matplotlib import rc\n\n    rc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": [\"Helvetica\"], \"size\": 10})\n\n    fig, ax = plt.subplots(figsize=(6, 4))\n    ax.grid(axis=\"y\", zorder=0, lw=0.2)\n\n    zorder = 1\n    for grouped in tries:\n        zorder += 1\n        df = grouped.unstack()\n\n        i, j = 0, 1\n        temp = df.iloc[i].copy()\n        df.iloc[i], df.iloc[j] = df.iloc[j], temp\n        dump(df)\n\n        # df.sort_values(by=[\"model\"], ascending=False, inplace=True)\n        num_models, num_formats = df.shape\n\n        pos = np.array(range(num_models))\n        width = 0.8 / num_formats\n\n        formats = df.columns\n        models = df.index\n\n        dump(df)\n        dump(models)\n        dump(formats)\n        for i, fmt in enumerate(formats):\n            hatch = \"\"\n\n            if fmt == \"diff\":\n                color = \"#b3e6a8\"\n                label = \"Search/replace blocks\"\n            elif fmt == \"udiff\":\n                color = \"#b3d1e6\"\n                label = \"Unified diffs\"\n            elif fmt == \"difffolk\":\n                label = \"Baseline + blind, no hands, $2k tip, etc\"\n                color = \"#b3e6a8\"\n                hatch = \"////\"\n            elif fmt == \"udifffolk\":\n                label = \"Unified diffs + blind, no hands, $2k tip, etc\"\n                color = \"#b3d1e6\"\n                hatch = \"////\"\n\n            if zorder > 1:\n                edge = dict(\n                    edgecolor=\"#ffffff\",\n                    linewidth=1.5,\n                )\n            else:\n                edge = dict()\n            if zorder == 2:\n                edge[\"label\"] = label\n\n            color = [\n                \"#b3e6a8\",\n                \"#b3e6a8\",\n                \"#b3d1e6\",\n            ]\n\n            rects = ax.bar(\n                pos + i * width,\n                df[fmt],\n                width * 0.95,\n                color=color,\n                hatch=hatch,\n                zorder=zorder,\n                **edge,\n            )\n\n            if zorder == 2:\n                ax.bar_label(rects, padding=4, labels=[f\"{v:.0f}%\" for v in df[fmt]], size=6)\n\n    ax.set_xticks([p + 0 * width for p in pos])\n\n    model_map = {\n        \"gpt-4-0125-preview\": \"gpt-4-\\n0125-preview\",\n        \"gpt-4-1106-preview\": \"gpt-4-\\n1106-preview\",\n        \"gpt-4-turbo-2024-04-09\": \"gpt-4-turbo-\\n2024-04-09\\n(GPT-4 Turbo with Vision)\",\n    }\n    model_labels = []\n\n    for model in models:\n        ml = model_map.get(model, model)\n        model_labels.append(ml)\n\n    model_labels = [\n        \"gpt-4-\\n1106-preview\",\n        \"gpt-4-\\n0125-preview\",\n        \"gpt-4-turbo-\\n2024-04-09\\n(GPT-4 Turbo with Vision)\",\n    ]\n    ax.set_xticklabels(model_labels, rotation=0)\n\n    ax.set_ylabel(\"Percent of exercises completed successfully\")\n    # ax.set_xlabel(\"Model\")\n    ax.set_title('Refactoring \"Laziness\" Benchmark')\n    # ax.legend(\n    # title=\"Edit Format\",\n    # loc=\"upper left\",\n    # bbox_to_anchor=(0.95, 0.95),\n    # )\n    ax.set_ylim(top=100)\n\n    plt.tight_layout()\n    plt.savefig(\"tmp.svg\")\n    imgcat(fig)\n\n    # df.to_csv(\"tmp.benchmarks.csv\")\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/problem_stats.py::1",
    "metadata": {
      "file_path": "benchmark/problem_stats.py",
      "file_name": "problem_stats.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 113,
      "span_ids": [
        "get_dirs_from_leaderboard",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 20,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "#!/usr/bin/env python\n\nimport argparse\nimport json\nimport shutil\nfrom collections import defaultdict\nfrom pathlib import Path\n\nimport yaml\n\nfrom aider.dump import dump  # noqa\n\nHARD_SET_NUM = 3  # Number of models that defines the hard set threshold\n\n\ndef get_dirs_from_leaderboard():\n    # Load the leaderboard data\n    with open(\"aider/website/_data/edit_leaderboard.yml\") as f:\n        leaderboard = yaml.safe_load(f)\n    return [(entry[\"dirname\"], entry[\"model\"]) for entry in leaderboard]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/problem_stats.py::2",
    "metadata": {
      "file_path": "benchmark/problem_stats.py",
      "file_name": "problem_stats.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 268,
      "span_ids": [
        "load_results"
      ],
      "start_line": 23,
      "end_line": 59,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def load_results(dirname):\n    \"\"\"Load all result files from a benchmark directory\"\"\"\n    dirname = Path(dirname)\n\n    benchmark_dir = dirname\n    if not benchmark_dir.exists():\n        benchmark_dir = Path(\"tmp.benchmarks\") / dirname\n        if not benchmark_dir.exists():\n            return None\n\n    all_results = []\n    parse_errors = []  # Track which exercises had parse errors for this model\n\n    # Look in language subdirectories under exercises/practice\n    for fname in benchmark_dir.glob(\"*/exercises/practice/*/.aider.results.json\"):\n        error = False\n        try:\n            results = json.loads(fname.read_text())\n            error = \"testcase\" not in results\n            if not error:\n                # Add language info to results\n                lang = fname.parts[-5]  # Get language from path\n                results[\"language\"] = lang\n                all_results.append(results)\n\n        except json.JSONDecodeError:\n            error = True\n\n        if error:\n            # Track the parse error for this exercise/model combination\n            lang = fname.parts[-5]\n            exercise = f\"{fname.parts[-2]}/{lang}\"  # Use directory name as testcase\n            parse_errors.append(exercise)\n            print(f\"Bad results file {fname}\")\n            continue\n\n    return all_results, parse_errors",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/problem_stats.py::3",
    "metadata": {
      "file_path": "benchmark/problem_stats.py",
      "file_name": "problem_stats.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 727,
      "span_ids": [
        "analyze_exercise_solutions"
      ],
      "start_line": 62,
      "end_line": 155,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):\n    PARSE_ERROR_M = 4  # Threshold for number of parse errors to DQ an exercise\n\n    if dirs is None:\n        # Use leaderboard data if no directories specified\n        dir_entries = get_dirs_from_leaderboard()\n    else:\n        # Use provided directories, with dirname as model name\n        dir_entries = [(d, d) for d in dirs]\n\n    # Filter out entries that don't load and sort by pass rate\n    valid_entries = []\n    parse_errors_by_model = {}  # Track which exercises had parse errors for each model\n\n    dump(dir_entries)\n\n    for dirname, model in dir_entries:\n        results_data = load_results(dirname)\n\n        if results_data:\n            results, model_parse_errors = results_data\n            parse_errors_by_model[model] = set(model_parse_errors)\n            # Calculate pass rate for sorting when using custom dirs\n            if dirs is not None:\n                pass_rate = sum(\n                    1 for r in results if r.get(\"tests_outcomes\", []) and r[\"tests_outcomes\"][-1]\n                ) / len(results)\n            else:\n                # Use existing pass rate from leaderboard\n                pass_rate = next(\n                    (\n                        entry[\"pass_rate_2\"]\n                        for entry in yaml.safe_load(\n                            open(\"aider/website/_data/edit_leaderboard.yml\")\n                        )\n                        if entry[\"dirname\"] == dirname\n                    ),\n                    0,\n                )\n            valid_entries.append(((dirname, model), results, float(pass_rate)))\n\n    # Sort by pass rate and take top N if specified\n    valid_entries.sort(key=lambda x: x[2], reverse=True)\n    if topn:\n        valid_entries = valid_entries[:topn]\n\n    # Get all exercise names from a complete run\n    all_exercises = set()\n    exercise_solutions = defaultdict(list)\n\n    # Get all unique exercise names from all results\n    all_exercises = set()\n    for (dirname, model), results, _ in valid_entries:\n        if results:\n            for result in results:\n                try:\n                    all_exercises.add(result[\"testcase\"] + \"/\" + result[\"language\"])\n                except KeyError:\n                    print(f\"Warning: Missing testcase in {dirname}\", json.dumps(result, indent=4))\n\n    for (dirname, model), results, _ in valid_entries:\n        if not results:\n            print(f\"Could not load results for {dirname}\")\n            continue\n\n        for result in results:\n            testcase = result.get(\"testcase\")\n            if not testcase:\n                continue\n            lang = result.get(\"language\")\n            if not lang:\n                continue\n\n            testcase = f\"{testcase}/{lang}\"\n            # Consider it solved if the last test attempt passed\n            tests_outcomes = result.get(\"tests_outcomes\", [])\n            if tests_outcomes and tests_outcomes[-1]:\n                exercise_solutions[testcase].append(model)\n\n    # Calculate never solved exercises\n    never_solved = len(all_exercises - set(exercise_solutions.keys()))\n\n    # Print per-exercise statistics\n    print(\"\\nExercise Solution Statistics:\")\n    print(\"-\" * 40)\n\n    # Add exercises that were never solved\n    for exercise in all_exercises:\n        if exercise not in exercise_solutions:\n            exercise_solutions[exercise] = []\n\n    # Create list of (language, exercise) pairs with solution stats\n    exercise_stats = []\n    total_models = len(valid_entries)\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/problem_stats.py::4",
    "metadata": {
      "file_path": "benchmark/problem_stats.py",
      "file_name": "problem_stats.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 805,
      "span_ids": [
        "analyze_exercise_solutions"
      ],
      "start_line": 157,
      "end_line": 222,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):\n    # ... other code\n\n    for testcase in all_exercises:\n        # Language is already in the testcase string\n        lang = testcase.split(\"/\")[0]  # First part is the language\n        models = exercise_solutions[testcase]\n        num_solved = len(models)\n        percent = (num_solved / total_models) * 100\n        testcase = testcase.replace(\"exercises/\", \"\")  # Remove the exercises/ prefix\n        # Remove duplicate language prefix (e.g. javascript/javascript/ -> javascript/)\n        if testcase.startswith(f\"{lang}/{lang}/\"):\n            testcase = testcase[len(lang) + 1 :]\n        exercise_stats.append((lang, testcase, num_solved, percent))\n\n    # Sort all exercises by solve rate, then by exercise name\n    exercise_stats.sort(\n        key=lambda x: (-x[2], x[1])\n    )  # -x[2] for descending solve rate, x[1] for ascending exercise name\n\n    # Calculate max lengths for alignment after cleaning up paths\n    max_name_len = max(len(f\"{lang}/{testcase}\") for lang, testcase, _, _ in exercise_stats)\n\n    # Print all exercises sorted by solve rate\n    print(\"\\nAll Exercises (sorted by solve rate):\")\n    for i, (lang, testcase, num_solved, percent) in enumerate(exercise_stats, 1):\n        print(f\"{i:>3}. {testcase:<{max_name_len}} : {num_solved:>3} solved ({percent:>5.1f}%)\")\n\n    print(\"\\nSummary:\")\n    solved_at_least_once = len([ex for ex, models in exercise_solutions.items() if models])\n    solved_by_none = never_solved\n    solved_by_all = len(\n        [ex for ex, models in exercise_solutions.items() if len(models) == total_models]\n    )\n\n    print(f\"Total exercises solved at least once: {solved_at_least_once}\")\n    print(f\"Never solved by any model: {solved_by_none}\")\n    if solved_by_none > 0:\n        print(\"\\nExercises never solved by any model:\")\n        unsolved = [ex for ex, models in exercise_solutions.items() if not models]\n        for ex in sorted(unsolved):\n            # Split into language and exercise parts\n            lang, exercise = ex.split(\"/\")\n            # Reconstruct path in desired format\n            formatted_path = f\"{lang}/exercises/practice/{exercise}\"\n            print(f\"  {formatted_path}\")\n    print(f\"\\nSolved by all models: {solved_by_all}\")\n    print(\n        f\"Total exercises: {len(all_exercises)} = {solved_by_none} (none) + {solved_by_all} (all) +\"\n        f\" {len(all_exercises) - solved_by_none - solved_by_all} (some)\"\n    )\n\n    # Distribution table of how many models solved each exercise\n    print(\"\\nDistribution of solutions:\")\n    print(\"Models  Exercises  Cumulative  RevCumulative\")\n    print(\"-\" * 50)\n    counts = [0] * (total_models + 1)\n    for ex, models in exercise_solutions.items():\n        counts[len(models)] += 1\n\n    cumsum = 0\n    revcumsum = sum(counts)  # Start with total number of exercises\n    for i, count in enumerate(counts):\n        cumsum += count\n        print(f\"{i:>6d}  {count:>9d}  {cumsum:>10d}  {revcumsum:>12d}\")\n        revcumsum -= count  # Decrement the reverse cumulative sum\n\n    # Count parse errors per exercise\n    parse_error_counts = defaultdict(int)\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/problem_stats.py::5",
    "metadata": {
      "file_path": "benchmark/problem_stats.py",
      "file_name": "problem_stats.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 821,
      "span_ids": [
        "analyze_exercise_solutions"
      ],
      "start_line": 223,
      "end_line": 305,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):\n    # ... other code\n    for model_errors in parse_errors_by_model.values():\n        for exercise in model_errors:\n            parse_error_counts[exercise] += 1\n\n    # Find exercises to disqualify based on parse error threshold\n    disqualified_exercises = {\n        exercise for exercise, count in parse_error_counts.items() if count >= PARSE_ERROR_M\n    }\n\n    if disqualified_exercises:\n        print(\n            f\"\\nDisqualified {len(disqualified_exercises)} exercises with {PARSE_ERROR_M}+ parse\"\n            \" errors:\"\n        )\n        for ex in sorted(disqualified_exercises):\n            print(f\"  {ex} ({parse_error_counts[ex]} parse errors)\")\n\n    # Collect the hard set (exercises solved by HARD_SET_NUM or fewer models)\n    print(f\"\\nHard Set Analysis (exercises solved by \u2264{HARD_SET_NUM} models):\")\n    print(\"-\" * 60)\n    hard_set = {\n        ex\n        for ex, models in exercise_solutions.items()\n        if len(models) <= HARD_SET_NUM and ex not in disqualified_exercises\n    }\n    print(f\"Total hard set exercises: {len(hard_set)}\")\n\n    # Count total problems, unsolved problems, and hard set problems by language\n    lang_totals = defaultdict(int)\n    lang_unsolved = defaultdict(int)\n    lang_hard_set = defaultdict(int)\n\n    for exercise in all_exercises:\n        lang = exercise.split(\"/\")[1]  # Get language from path\n        lang_totals[lang] += 1\n        if not exercise_solutions[exercise]:  # No models solved this exercise\n            lang_unsolved[lang] += 1\n        if exercise in hard_set:  # Exercise is in the hard set\n            lang_hard_set[lang] += 1\n\n    print(\"\\nUnsolved and hard set problems by language:\")\n    print(f\"{'Language':<12} {'Unsolved':>8} {'Hard Set':>9} {'Total':>7} {'%hardUnsolved':>8}\")\n    print(\"-\" * 47)\n    for lang in sorted(lang_totals.keys()):\n        count = lang_unsolved[lang]\n        hard = lang_hard_set[lang]\n        total = lang_totals[lang]\n        pct = (count / hard) * 100 if hard else -1\n        print(f\"{lang:<12} {count:>8} {hard:>9} {total:>7} {pct:>7.1f}%\")\n    print()\n\n    # For each model, compute performance on hard set\n    model_hard_stats = []\n    for (dirname, model), results, _ in valid_entries:\n        if not results:\n            continue\n\n        solved_hard = 0\n        for result in results:\n            testcase = result.get(\"testcase\")\n            if not testcase:\n                continue\n            lang = result.get(\"language\")\n            if not lang:\n                continue\n\n            testcase = f\"{testcase}/{lang}\"\n            if testcase in hard_set:\n                tests_outcomes = result.get(\"tests_outcomes\", [])\n                if tests_outcomes and tests_outcomes[-1]:\n                    solved_hard += 1\n\n        pct = (solved_hard / len(hard_set)) * 100\n        model_hard_stats.append((model, solved_hard, pct))\n\n    # Sort by number solved\n    model_hard_stats.sort(key=lambda x: x[1], reverse=True)\n\n    print(\"\\nModel performance on hard set:\")\n    print(f\"{'Model':<55} {'Solved':<8} {'Percent':>7}\")\n    print(\"-\" * 50)\n    for model, solved, pct in model_hard_stats:\n        print(f\"{model:<55} {solved:>6d}   {pct:>6.1f}%\")\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/problem_stats.py::6",
    "metadata": {
      "file_path": "benchmark/problem_stats.py",
      "file_name": "problem_stats.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 343,
      "span_ids": [
        "analyze_exercise_solutions"
      ],
      "start_line": 307,
      "end_line": 339,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def analyze_exercise_solutions(dirs=None, topn=None, copy_hard_set=False):\n    # ... other code\n\n    if copy_hard_set:\n        # Create hard set directory\n        src_dir = Path(\"tmp.benchmarks/exercism\")\n        dst_dir = Path(\"tmp.benchmarks/exercism-polyglot\")\n\n        if dst_dir.exists():\n            print(f\"\\nError: Destination directory {dst_dir} already exists\")\n            return\n\n        print(f\"\\nCopying hard set problems to {dst_dir}...\")\n\n        # Create a set of (exercise, language) pairs from hard_set\n        hard_set_pairs = {tuple(exercise.split(\"/\")) for exercise in hard_set}\n\n        # Copy each hard set problem's directory\n        copied_by_lang = defaultdict(int)\n        for lang_dir in src_dir.glob(\"*/exercises/practice\"):\n            if not lang_dir.is_dir():\n                continue\n\n            lang = lang_dir.parts[-3]  # Get language from path\n            for problem_dir in lang_dir.glob(\"*\"):\n                if (problem_dir.name, lang) in hard_set_pairs:\n                    rel_path = problem_dir.relative_to(src_dir)\n                    dst_path = dst_dir / rel_path\n                    dst_path.parent.mkdir(parents=True, exist_ok=True)\n                    shutil.copytree(problem_dir, dst_path)\n                    copied_by_lang[lang] += 1\n\n        total_copied = sum(copied_by_lang.values())\n        print(f\"\\nCopied {total_copied} hard set problems:\")\n        for lang in sorted(copied_by_lang):\n            print(f\"  {lang}: {copied_by_lang[lang]}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/problem_stats.py::7",
    "metadata": {
      "file_path": "benchmark/problem_stats.py",
      "file_name": "problem_stats.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 134,
      "span_ids": [
        "impl:3"
      ],
      "start_line": 342,
      "end_line": 356,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "if __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--topn\", type=int, help=\"Only consider top N models by pass rate\")\n    parser.add_argument(\n        \"dirs\", nargs=\"*\", help=\"Directories to analyze (optional, defaults to leaderboard entries)\"\n    )\n    parser.add_argument(\n        \"--copy-hard-set\",\n        action=\"store_true\",\n        help=\"Copy hard set problems to tmp.benchmarks/exercism-polygot\",\n    )\n    args = parser.parse_args()\n\n    analyze_exercise_solutions(args.dirs if args.dirs else None, args.topn, args.copy_hard_set)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/prompts.py::1",
    "metadata": {
      "file_path": "benchmark/prompts.py",
      "file_name": "prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 106,
      "span_ids": [
        "impl"
      ],
      "start_line": 1,
      "end_line": 17,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "instructions_addendum = \"\"\"\n####\n\nUse the above instructions to modify the supplied files: {file_list}\nDon't change the names of existing functions or classes, as they may be referenced from other code like unit tests, etc.\nOnly use standard libraries, don't suggest installing any packages.\n\"\"\"  # noqa: E501\n\n\ntest_failures = \"\"\"\n####\n\nSee the testing errors above.\nThe tests are correct, don't try and change them.\nFix the code in {file_list} to resolve the errors.\n\"\"\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/refactor_tools.py::1",
    "metadata": {
      "file_path": "benchmark/refactor_tools.py",
      "file_name": "refactor_tools.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 262,
      "span_ids": [
        "verify_full_func_at_top_level",
        "ParentNodeTransformer.generic_visit",
        "docstring",
        "ParentNodeTransformer"
      ],
      "start_line": 1,
      "end_line": 40,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "#!/usr/bin/env python\n\nimport ast\nimport os\nimport shutil\nimport sys\nfrom pathlib import Path\n\nfrom aider.dump import dump  # noqa: F401\n\n\nclass ParentNodeTransformer(ast.NodeTransformer):\n    \"\"\"\n    This transformer sets the 'parent' attribute on each node.\n    \"\"\"\n\n    def generic_visit(self, node):\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n        return super(ParentNodeTransformer, self).generic_visit(node)\n\n\ndef verify_full_func_at_top_level(tree, func, func_children):\n    func_nodes = [\n        item for item in ast.walk(tree) if isinstance(item, ast.FunctionDef) and item.name == func\n    ]\n    assert func_nodes, f\"Function {func} not found\"\n\n    for func_node in func_nodes:\n        if not isinstance(func_node.parent, ast.Module):\n            continue\n\n        num_children = sum(1 for _ in ast.walk(func_node))\n        pct_diff_children = abs(num_children - func_children) * 100 / func_children\n        assert (\n            pct_diff_children < 10\n        ), f\"Old method had {func_children} children, new method has {num_children}\"\n        return\n\n    assert False, f\"{func} is not a top level function\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/refactor_tools.py::2",
    "metadata": {
      "file_path": "benchmark/refactor_tools.py",
      "file_name": "refactor_tools.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 233,
      "span_ids": [
        "verify_old_class_children",
        "verify_refactor"
      ],
      "start_line": 43,
      "end_line": 70,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def verify_old_class_children(tree, old_class, old_class_children):\n    node = next(\n        (\n            item\n            for item in ast.walk(tree)\n            if isinstance(item, ast.ClassDef) and item.name == old_class\n        ),\n        None,\n    )\n    assert node is not None, f\"Old class {old_class} not found\"\n\n    num_children = sum(1 for _ in ast.walk(node))\n\n    pct_diff_children = abs(num_children - old_class_children) * 100 / old_class_children\n    assert (\n        pct_diff_children < 10\n    ), f\"Old class had {old_class_children} children, new class has {num_children}\"\n\n\ndef verify_refactor(fname, func, func_children, old_class, old_class_children):\n    with open(fname, \"r\") as file:\n        file_contents = file.read()\n    tree = ast.parse(file_contents)\n    ParentNodeTransformer().visit(tree)  # Set parent attribute for all nodes\n\n    verify_full_func_at_top_level(tree, func, func_children)\n\n    verify_old_class_children(tree, old_class, old_class_children - func_children)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/refactor_tools.py::3",
    "metadata": {
      "file_path": "benchmark/refactor_tools.py",
      "file_name": "refactor_tools.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 278,
      "span_ids": [
        "SelfUsageChecker.visit_FunctionDef",
        "verify_refactor",
        "SelfUsageChecker",
        "SelfUsageChecker.__init__",
        "SelfUsageChecker.visit_ClassDef"
      ],
      "start_line": 73,
      "end_line": 110,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "############################\n\n\nclass SelfUsageChecker(ast.NodeVisitor):\n    def __init__(self):\n        self.non_self_methods = []\n        self.parent_class_name = None\n        self.num_class_children = 0\n\n    def visit_FunctionDef(self, node):\n        # Check if the first argument is 'self' and if it's not used\n        if node.args.args and node.args.args[0].arg == \"self\":\n            self_used = any(\n                isinstance(expr, ast.Name) and expr.id == \"self\"\n                for stmt in node.body\n                for expr in ast.walk(stmt)\n            )\n            super_used = any(\n                isinstance(expr, ast.Name) and expr.id == \"super\"\n                for stmt in node.body\n                for expr in ast.walk(stmt)\n            )\n            if not self_used and not super_used:\n                # Calculate the number of child nodes in the function\n                num_child_nodes = sum(1 for _ in ast.walk(node))\n                res = (\n                    self.parent_class_name,\n                    node.name,\n                    self.num_class_children,\n                    num_child_nodes,\n                )\n                self.non_self_methods.append(res)\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node):\n        self.parent_class_name = node.name\n        self.num_class_children = sum(1 for _ in ast.walk(node))\n        self.generic_visit(node)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/refactor_tools.py::4",
    "metadata": {
      "file_path": "benchmark/refactor_tools.py",
      "file_name": "refactor_tools.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 189,
      "span_ids": [
        "find_python_files",
        "find_non_self_methods"
      ],
      "start_line": 113,
      "end_line": 142,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def find_python_files(path):\n    if os.path.isfile(path) and path.endswith(\".py\"):\n        return [path]\n    elif os.path.isdir(path):\n        py_files = []\n        for root, dirs, files in os.walk(path):\n            for file in files:\n                if file.endswith(\".py\"):\n                    full_path = os.path.join(root, file)\n                    py_files.append(full_path)\n        return py_files\n    else:\n        return []\n\n\ndef find_non_self_methods(path):\n    python_files = find_python_files(path)\n    non_self_methods = []\n    for filename in python_files:\n        with open(filename, \"r\") as file:\n            try:\n                node = ast.parse(file.read(), filename=filename)\n            except:\n                pass\n            checker = SelfUsageChecker()\n            checker.visit(node)\n            for method in checker.non_self_methods:\n                non_self_methods.append([filename] + list(method))\n\n    return non_self_methods",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/refactor_tools.py::5",
    "metadata": {
      "file_path": "benchmark/refactor_tools.py",
      "file_name": "refactor_tools.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 468,
      "span_ids": [
        "impl",
        "main",
        "process"
      ],
      "start_line": 145,
      "end_line": 210,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def process(entry):\n    fname, class_name, method_name, class_children, method_children = entry\n    if method_children > class_children / 2:\n        return\n    if method_children < 250:\n        return\n\n    fname = Path(fname)\n    if \"test\" in fname.stem:\n        return\n\n    print(f\"{fname} {class_name} {method_name} {class_children} {method_children}\")\n\n    dname = Path(\"tmp.benchmarks/refactor-benchmark-spyder\")\n    dname.mkdir(exist_ok=True)\n\n    dname = dname / f\"{fname.stem}_{class_name}_{method_name}\"\n    dname.mkdir(exist_ok=True)\n\n    shutil.copy(fname, dname / fname.name)\n\n    docs_dname = dname / \".docs\"\n    docs_dname.mkdir(exist_ok=True)\n\n    ins_fname = docs_dname / \"instructions.md\"\n    ins_fname.write_text(f\"\"\"# Refactor {class_name}.{method_name}\n\nRefactor the `{method_name}` method in the `{class_name}` class to be a stand alone, top level function.\nName the new function `{method_name}`, exactly the same name as the existing method.\nUpdate any existing `self.{method_name}` calls to work with the new `{method_name}` function.\n\"\"\")  # noqa: E501\n\n    test_fname = dname / f\"{fname.stem}_test.py\"\n    test_fname.write_text(f\"\"\"\nimport unittest\nfrom benchmark.refactor_tools import verify_refactor\nfrom pathlib import Path\n\nclass TheTest(unittest.TestCase):\n    def test_{method_name}(self):\n        fname = Path(__file__).parent / \"{fname.name}\"\n        method = \"{method_name}\"\n        method_children = {method_children}\n\n        class_name = \"{class_name}\"\n        class_children = {class_children}\n\n        verify_refactor(fname, method, method_children, class_name, class_children)\n\nif __name__ == \"__main__\":\n    unittest.main()\n\"\"\")\n\n\ndef main(paths):\n    for path in paths:\n        methods = find_non_self_methods(path)\n        # methods = sorted(methods, key=lambda x: x[4])\n\n        for method in methods:\n            process(method)\n\n\nif __name__ == \"__main__\":\n    main(sys.argv[1:])",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/rungrid.py::1",
    "metadata": {
      "file_path": "benchmark/rungrid.py",
      "file_name": "rungrid.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 382,
      "span_ids": [
        "impl",
        "run",
        "main",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 62,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "#!/usr/bin/env python\n\nimport subprocess\nimport sys\n\nfrom aider.dump import dump  # noqa: F401\n\n\ndef main():\n    models = [\n        \"gpt-3.5-turbo-0301\",\n        \"gpt-3.5-turbo-0613\",\n        # \"gpt-3.5-turbo-16k-0613\",\n        \"gpt-3.5-turbo-1106\",\n        # \"gpt-4-0314\",\n        # \"gpt-4-0613\",\n    ]\n    edit_formats = [\n        \"diff\",\n        # \"diff-func\",\n        # \"whole\",\n        # \"whole-func\",\n    ]\n\n    # for repeat in range(1, 2, 1):\n    for model in models:\n        for edit_format in edit_formats:\n            # dump(model, edit_format)\n\n            if \"-func\" in edit_format and \"-03\" in model:\n                continue\n\n            # if (model, edit_format) == (\"gpt-3.5-turbo-16k-0613\", \"whole-func\"):\n            #    # sublist reliably hangs the API?\n            #    continue\n\n            dirname = f\"rungrid-nov-{model}-{edit_format}\"\n            # dirname = f\"rungrid-{model}-{edit_format}-repeat-{repeat}\"\n            run(dirname, model, edit_format)\n\n\ndef run(dirname, model, edit_format):\n    cmd = [\n        \"./benchmark/benchmark.py\",\n        dirname,\n        \"--model\",\n        model,\n        \"--edit-format\",\n        edit_format,\n        \"--threads\",\n        \"10\",\n        \"--cont\",\n    ]\n    print(\" \".join(cmd))\n\n    subprocess.run(cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    status = main()\n    sys.exit(status)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/swe_bench.py::1",
    "metadata": {
      "file_path": "benchmark/swe_bench.py",
      "file_name": "swe_bench.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 967,
      "span_ids": [
        "imports",
        "plot_swe_bench",
        "impl"
      ],
      "start_line": 1,
      "end_line": 132,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import sys\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nfrom imgcat import imgcat\nfrom matplotlib import rc\n\nfrom aider.dump import dump  # noqa: F401\n\n\ndef plot_swe_bench(data_file, is_lite):\n    with open(data_file, \"r\") as file:\n        lines = file.readlines()\n\n    models = []\n    pass_rates = []\n    instances = []\n    for line in lines:\n        if line.strip():\n            pass_rate, model = line.split(\"%\")\n            model = model.strip()\n            if \"(\" in model:\n                pieces = model.split(\"(\")\n                model = pieces[0]\n                ins = pieces[1].strip(\")\")\n            else:\n                ins = None\n            instances.insert(0, ins)\n            model = model.replace(\"|\", \"\\n\")\n            models.insert(0, model.strip())\n            pass_rates.insert(0, float(pass_rate.strip()))\n\n    dump(instances)\n\n    plt.rcParams[\"hatch.linewidth\"] = 0.5\n    plt.rcParams[\"hatch.color\"] = \"#444444\"\n\n    font_color = \"#555\"\n    font_params = {\n        \"family\": \"sans-serif\",\n        \"sans-serif\": [\"Helvetica\"],\n        \"size\": 10,\n        \"weight\": \"bold\",\n    }\n    rc(\"font\", **font_params)\n    plt.rcParams[\"text.color\"] = font_color\n\n    fig, ax = plt.subplots(figsize=(10, 5.5))\n    ax.grid(axis=\"y\", zorder=0, lw=0.2)\n    for spine in ax.spines.values():\n        spine.set_edgecolor(\"#DDDDDD\")\n        spine.set_linewidth(0.5)\n\n    if is_lite:\n        colors = [\"#17965A\" if \"Aider\" in model else \"#b3d1e6\" for model in models]\n    else:\n        colors = [\"#1A75C2\" if \"Aider\" in model else \"#b3d1e6\" for model in models]\n\n    bars = []\n    for model, pass_rate, color in zip(models, pass_rates, colors):\n        alpha = 0.9 if \"Aider\" in model else 0.3\n        hatch = \"\"\n        # if is_lite:\n        #    hatch = \"///\" if \"(570)\" in model else \"\"\n        bar = ax.bar(model, pass_rate, color=color, alpha=alpha, zorder=3, hatch=hatch)\n        bars.append(bar[0])\n\n    for label in ax.get_xticklabels():\n        if \"Aider\" in str(label):\n            label.set_fontfamily(\"Helvetica Bold\")\n\n    for model, bar in zip(models, bars):\n        yval = bar.get_height()\n        y = yval - 1\n        va = \"top\"\n        color = \"#eee\" if \"Aider\" in model else \"#555\"\n        fontfamily = \"Helvetica Bold\" if \"Aider\" in model else \"Helvetica\"\n        ax.text(\n            bar.get_x() + bar.get_width() / 2,\n            y,\n            f\"{yval}%\",\n            ha=\"center\",\n            va=va,\n            fontsize=16,\n            color=color,\n            fontfamily=fontfamily,\n        )\n\n    for model, ins, bar in zip(models, instances, bars):\n        if not ins:\n            continue\n        yval = bar.get_height()\n        y = yval - 2.5\n        va = \"top\"\n        color = \"#eee\" if \"Aider\" in model else \"#555\"\n        ax.text(\n            bar.get_x() + bar.get_width() / 2,\n            y,\n            f\"of {ins}\",\n            ha=\"center\",\n            va=va,\n            fontsize=12,\n            color=color,\n        )\n\n    # ax.set_xlabel(\"Models\", fontsize=18)\n    ax.set_ylabel(\"Pass@1 (%)\", fontsize=18, color=font_color)\n    if is_lite:\n        title = \"SWE Bench Lite\"\n    else:\n        title = \"SWE Bench\"\n    ax.set_title(title, fontsize=20)\n    # ax.set_ylim(0, 29.9)\n    plt.xticks(\n        fontsize=16,\n        color=font_color,\n    )\n\n    plt.tight_layout(pad=3.0)\n\n    out_fname = Path(data_file.replace(\"-\", \"_\"))\n    plt.savefig(out_fname.with_suffix(\".jpg\").name)\n    plt.savefig(out_fname.with_suffix(\".svg\").name)\n    imgcat(fig)\n    ax.xaxis.label.set_color(font_color)\n\n\nfname = sys.argv[1]\nis_lite = \"lite\" in fname\n\nplot_swe_bench(fname, is_lite)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "benchmark/test_benchmark.py::1",
    "metadata": {
      "file_path": "benchmark/test_benchmark.py",
      "file_name": "test_benchmark.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 336,
      "span_ids": [
        "TestCleanupTestOutput.test_cleanup_test_output",
        "TestCleanupTestOutput",
        "TestCleanupTestOutput.test_cleanup_test_output_lines",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 48,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# flake8: noqa: E501\n\nimport unittest\n\nfrom benchmark import cleanup_test_output\n\n\nclass TestCleanupTestOutput(unittest.TestCase):\n    def test_cleanup_test_output(self):\n        # Test case with timing info\n        output = \"Ran 5 tests in 0.003s\\nOK\"\n        expected = \"\\nOK\"\n        self.assertEqual(cleanup_test_output(output), expected)\n\n        # Test case without timing info\n        output = \"OK\"\n        expected = \"OK\"\n        self.assertEqual(cleanup_test_output(output), expected)\n\n    def test_cleanup_test_output_lines(self):\n        # Test case with timing info\n        output = \"\"\"F\n======================================================================\nFAIL: test_cleanup_test_output (test_benchmark.TestCleanupTestOutput.test_cleanup_test_output)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/gauthier/Projects/aider/benchmark/test_benchmark.py\", line 14, in test_cleanup_test_output\n    self.assertEqual(cleanup_test_output(output), expected)\nAssertionError: 'OK' != 'OKx'\n- OK\n+ OKx\n?   +\n\"\"\"\n\n        expected = \"\"\"F\n====\nFAIL: test_cleanup_test_output (test_benchmark.TestCleanupTestOutput.test_cleanup_test_output)\n----\nTraceback (most recent call last):\n  File \"/Users/gauthier/Projects/aider/benchmark/test_benchmark.py\", line 14, in test_cleanup_test_output\n    self.assertEqual(cleanup_test_output(output), expected)\nAssertionError: 'OK' != 'OKx'\n- OK\n+ OKx\n?   +\n\"\"\"\n        self.assertEqual(cleanup_test_output(output), expected)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/blame.py::1",
    "metadata": {
      "file_path": "scripts/blame.py",
      "file_name": "blame.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 111,
      "span_ids": [
        "docstring"
      ],
      "start_line": 1,
      "end_line": 24,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "#!/usr/bin/env python3\n\nimport argparse\nimport os\nimport subprocess\nimport sys\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom operator import itemgetter\n\nimport semver\nimport yaml\nfrom tqdm import tqdm\n\nwebsite_files = [\n    \"aider/website/share/index.md\",\n    \"aider/website/_includes/head_custom.html\",\n    \"aider/website/docs/leaderboards/index.md\",\n]\n\nexclude_files = [\n    \"aider/website/install.ps1\",\n    \"aider/website/install.sh\",\n]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/blame.py::2",
    "metadata": {
      "file_path": "scripts/blame.py",
      "file_name": "blame.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 369,
      "span_ids": [
        "blame"
      ],
      "start_line": 27,
      "end_line": 63,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def blame(start_tag, end_tag=None):\n    commits = get_all_commit_hashes_between_tags(start_tag, end_tag)\n    commits = [commit[:hash_len] for commit in commits]\n\n    authors = get_commit_authors(commits)\n\n    revision = end_tag if end_tag else \"HEAD\"\n    files = run([\"git\", \"ls-tree\", \"-r\", \"--name-only\", revision]).strip().split(\"\\n\")\n    files = [\n        f\n        for f in files\n        if f.endswith((\".js\", \".py\", \".scm\", \".sh\", \"Dockerfile\", \"Gemfile\"))\n        or (f.startswith(\".github/workflows/\") and f.endswith(\".yml\"))\n        or f in website_files\n    ]\n    files = [f for f in files if not f.endswith(\"prompts.py\")]\n    files = [f for f in files if not f.startswith(\"tests/fixtures/watch\")]\n    files = [f for f in files if f not in exclude_files]\n\n    all_file_counts = {}\n    grand_total = defaultdict(int)\n    aider_total = 0\n    for file in files:\n        file_counts = get_counts_for_file(start_tag, end_tag, authors, file)\n        if file_counts:\n            all_file_counts[file] = file_counts\n            for author, count in file_counts.items():\n                grand_total[author] += count\n                if \"(aider)\" in author.lower():\n                    aider_total += count\n\n    total_lines = sum(grand_total.values())\n    aider_percentage = (aider_total / total_lines) * 100 if total_lines > 0 else 0\n\n    end_date = get_tag_date(end_tag if end_tag else \"HEAD\")\n\n    return all_file_counts, grand_total, total_lines, aider_total, aider_percentage, end_date",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/blame.py::3",
    "metadata": {
      "file_path": "scripts/blame.py",
      "file_name": "blame.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 234,
      "span_ids": [
        "run",
        "get_all_commit_hashes_between_tags",
        "impl:5",
        "get_commit_authors"
      ],
      "start_line": 66,
      "end_line": 94,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_all_commit_hashes_between_tags(start_tag, end_tag=None):\n    if end_tag:\n        res = run([\"git\", \"rev-list\", f\"{start_tag}..{end_tag}\"])\n    else:\n        res = run([\"git\", \"rev-list\", f\"{start_tag}..HEAD\"])\n\n    if res:\n        commit_hashes = res.strip().split(\"\\n\")\n        return commit_hashes\n\n\ndef run(cmd):\n    # Get all commit hashes since the specified tag\n    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n    return result.stdout\n\n\ndef get_commit_authors(commits):\n    commit_to_author = dict()\n    for commit in commits:\n        author = run([\"git\", \"show\", \"-s\", \"--format=%an\", commit]).strip()\n        commit_message = run([\"git\", \"show\", \"-s\", \"--format=%s\", commit]).strip()\n        if commit_message.lower().startswith(\"aider:\"):\n            author += \" (aider)\"\n        commit_to_author[commit] = author\n    return commit_to_author\n\n\nhash_len = len(\"44e6fefc2\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/blame.py::4",
    "metadata": {
      "file_path": "scripts/blame.py",
      "file_name": "blame.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 279,
      "span_ids": [
        "get_latest_version_tag",
        "process_all_tags_since"
      ],
      "start_line": 97,
      "end_line": 132,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def process_all_tags_since(start_tag):\n    tags = get_all_tags_since(start_tag)\n    # tags += ['HEAD']\n\n    results = []\n    for i in tqdm(range(len(tags) - 1), desc=\"Processing tags\"):\n        start_tag, end_tag = tags[i], tags[i + 1]\n        all_file_counts, grand_total, total_lines, aider_total, aider_percentage, end_date = blame(\n            start_tag, end_tag\n        )\n        results.append(\n            {\n                \"start_tag\": start_tag,\n                \"end_tag\": end_tag,\n                \"end_date\": end_date.strftime(\"%Y-%m-%d\"),\n                \"file_counts\": all_file_counts,\n                \"grand_total\": {\n                    author: count\n                    for author, count in sorted(\n                        grand_total.items(), key=itemgetter(1), reverse=True\n                    )\n                },\n                \"total_lines\": total_lines,\n                \"aider_total\": aider_total,\n                \"aider_percentage\": round(aider_percentage, 2),\n            }\n        )\n    return results\n\n\ndef get_latest_version_tag():\n    all_tags = run([\"git\", \"tag\", \"--sort=-v:refname\"]).strip().split(\"\\n\")\n    for tag in all_tags:\n        if semver.Version.is_valid(tag[1:]) and tag.endswith(\".0\"):\n            return tag\n    return None",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/blame.py::5",
    "metadata": {
      "file_path": "scripts/blame.py",
      "file_name": "blame.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 636,
      "span_ids": [
        "main"
      ],
      "start_line": 135,
      "end_line": 212,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main():\n    parser = argparse.ArgumentParser(description=\"Get aider/non-aider blame stats\")\n    parser.add_argument(\"start_tag\", nargs=\"?\", help=\"The tag to start from (optional)\")\n    parser.add_argument(\"--end-tag\", help=\"The tag to end at (default: HEAD)\", default=None)\n    parser.add_argument(\n        \"--all-since\",\n        action=\"store_true\",\n        help=(\n            \"Find all tags since the specified tag and print aider percentage between each pair of\"\n            \" successive tags\"\n        ),\n    )\n    parser.add_argument(\n        \"--output\", help=\"Output file to save the YAML results\", type=str, default=None\n    )\n    args = parser.parse_args()\n\n    if not args.start_tag:\n        args.start_tag = get_latest_version_tag()\n        if not args.start_tag:\n            print(\"Error: No valid vX.Y.0 tag found.\")\n            return\n\n    if args.all_since:\n        new_results = process_all_tags_since(args.start_tag)\n\n        # If output file exists, read and update it\n        existing_results = []\n        if args.output and os.path.exists(args.output):\n            with open(args.output, \"r\") as f:\n                existing_results = yaml.safe_load(f) or []\n\n        # Create a map of start_tag->end_tag to result for existing entries\n        existing_map = {(r[\"start_tag\"], r[\"end_tag\"]): i for i, r in enumerate(existing_results)}\n\n        # Update or append new results\n        for new_result in new_results:\n            key = (new_result[\"start_tag\"], new_result[\"end_tag\"])\n            if key in existing_map:\n                # Replace existing entry\n                existing_results[existing_map[key]] = new_result\n            else:\n                # Append new entry\n                existing_results.append(new_result)\n\n        # Sort results by start_tag\n        existing_results.sort(key=lambda x: semver.Version.parse(x[\"start_tag\"][1:]))\n\n        yaml_output = yaml.dump(existing_results, sort_keys=True)\n    else:\n        all_file_counts, grand_total, total_lines, aider_total, aider_percentage, end_date = blame(\n            args.start_tag, args.end_tag\n        )\n\n        result = {\n            \"start_tag\": args.start_tag,\n            \"end_tag\": args.end_tag or \"HEAD\",\n            \"end_date\": end_date.strftime(\"%Y-%m-%d\"),\n            \"file_counts\": all_file_counts,\n            \"grand_total\": {\n                author: count\n                for author, count in sorted(grand_total.items(), key=itemgetter(1), reverse=True)\n            },\n            \"total_lines\": total_lines,\n            \"aider_total\": aider_total,\n            \"aider_percentage\": round(aider_percentage, 2),\n        }\n\n        yaml_output = yaml.dump(result, sort_keys=True)\n\n    if args.output:\n        with open(args.output, \"w\") as f:\n            f.write(yaml_output)\n    else:\n        print(yaml_output)\n\n    if not args.all_since:\n        print(f\"- Aider wrote {round(aider_percentage)}% of the code in this release.\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/blame.py::6",
    "metadata": {
      "file_path": "scripts/blame.py",
      "file_name": "blame.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 274,
      "span_ids": [
        "get_counts_for_file"
      ],
      "start_line": 215,
      "end_line": 254,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_counts_for_file(start_tag, end_tag, authors, fname):\n    try:\n        if end_tag:\n            text = run(\n                [\n                    \"git\",\n                    \"blame\",\n                    \"-M\",\n                    \"-C\",\n                    \"-C\",\n                    \"--abbrev=9\",\n                    f\"{start_tag}..{end_tag}\",\n                    \"--\",\n                    fname,\n                ]\n            )\n        else:\n            text = run(\n                [\"git\", \"blame\", \"-M\", \"-C\", \"-C\", \"--abbrev=9\", f\"{start_tag}..HEAD\", \"--\", fname]\n            )\n        if not text:\n            return None\n        text = text.splitlines()\n        line_counts = defaultdict(int)\n        for line in text:\n            if line.startswith(\"^\"):\n                continue\n            hsh = line[:hash_len]\n            author = authors.get(hsh, \"Unknown\")\n            line_counts[author] += 1\n\n        return dict(line_counts)\n    except subprocess.CalledProcessError as e:\n        if \"no such path\" in str(e).lower():\n            # File doesn't exist in this revision range, which is okay\n            return None\n        else:\n            # Some other error occurred\n            print(f\"Warning: Unable to blame file {fname}. Error: {e}\", file=sys.stderr)\n            return None",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/blame.py::7",
    "metadata": {
      "file_path": "scripts/blame.py",
      "file_name": "blame.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 172,
      "span_ids": [
        "get_tag_date",
        "impl:7",
        "get_all_tags_since"
      ],
      "start_line": 257,
      "end_line": 275,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_all_tags_since(start_tag):\n    all_tags = run([\"git\", \"tag\", \"--sort=v:refname\"]).strip().split(\"\\n\")\n    start_version = semver.Version.parse(start_tag[1:])  # Remove 'v' prefix\n    filtered_tags = [\n        tag\n        for tag in all_tags\n        if semver.Version.is_valid(tag[1:]) and semver.Version.parse(tag[1:]) >= start_version\n    ]\n    return [tag for tag in filtered_tags if tag.endswith(\".0\")]\n\n\ndef get_tag_date(tag):\n    date_str = run([\"git\", \"log\", \"-1\", \"--format=%ai\", tag]).strip()\n    return datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S %z\")\n\n\nif __name__ == \"__main__\":\n    main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/history_prompts.py::1",
    "metadata": {
      "file_path": "scripts/history_prompts.py",
      "file_name": "history_prompts.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 171,
      "span_ids": [
        "impl"
      ],
      "start_line": 1,
      "end_line": 21,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "history_prompt = \"\"\"\nUpdate the history with changes shown in the diffs.\nDescribe actual user-facing changes, not every single commit that was made implementing them.\n\nOnly add new items not already listed.\nDo NOT edit or update existing history entries.\nDo NOT add duplicate entries for changes that have existing history entries.\n\nEnd each bullet with a period.\n\nIf the change was made by someone other than Paul Gauthier note it at the end of the bullet point as \", by XXX.\"\n\nBe sure to attribute changes to the proper .x version.\nChanges in the .x-dev version should be listed under a \"### main branch\" heading\n\nStart a new \"### main branch\" section at the top of the file if needed.\n\nAlso, add this as the last bullet under the \"### main branch\" section:\n{aider_line}\n\"\"\"  # noqa\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/issues.py::1",
    "metadata": {
      "file_path": "scripts/issues.py",
      "file_name": "issues.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 614,
      "span_ids": [
        "has_been_reopened",
        "impl",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 69,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "#!/usr/bin/env python3\n\nimport argparse\nimport os\nimport re\nfrom collections import defaultdict\nfrom datetime import datetime\n\nimport requests\nfrom dotenv import load_dotenv\nfrom tqdm import tqdm\n\n\ndef has_been_reopened(issue_number):\n    timeline_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue_number}/timeline\"\n    response = requests.get(timeline_url, headers=headers)\n    response.raise_for_status()\n    events = response.json()\n    return any(event[\"event\"] == \"reopened\" for event in events if \"event\" in event)\n\n\n# Load environment variables from .env file\nload_dotenv()\n\nBOT_SUFFIX = \"\"\"\n\nNote: [A bot script](https://github.com/Aider-AI/aider/blob/main/scripts/issues.py) made these updates to the issue.\n\"\"\"  # noqa\n\nDUPLICATE_COMMENT = (\n    \"\"\"Thanks for trying aider and filing this issue.\n\nThis looks like a duplicate of #{oldest_issue_number}. Please see the comments there for more information, and feel free to continue the discussion within that issue.\n\nI'm going to close this issue for now. But please let me know if you think this is actually a distinct issue and I will reopen this issue.\"\"\"  # noqa\n    + BOT_SUFFIX\n)\n\nSTALE_COMMENT = (\n    \"\"\"I'm labeling this issue as stale because it has been open for 2 weeks with no activity. If there are no additional comments, I will close it in 7 days.\"\"\"  # noqa\n    + BOT_SUFFIX\n)\n\nCLOSE_STALE_COMMENT = (\n    \"\"\"I'm closing this issue because it has been stalled for 3 weeks with no activity. Feel free to add a comment here and we can re-open it. Or feel free to file a new issue at any time.\"\"\"  # noqa\n    + BOT_SUFFIX\n)\n\nCLOSE_FIXED_ENHANCEMENT_COMMENT = (\n    \"\"\"I'm closing this enhancement request since it has been marked as 'fixed' for over \"\"\"\n    \"\"\"3 weeks. The requested feature should now be available in recent versions of aider.\\n\\n\"\"\"\n    \"\"\"If you find that this enhancement is still needed, please feel free to reopen this \"\"\"\n    \"\"\"issue or create a new one.\"\"\" + BOT_SUFFIX\n)\n\nCLOSE_FIXED_BUG_COMMENT = (\n    \"\"\"I'm closing this bug report since it has been marked as 'fixed' for over \"\"\"\n    \"\"\"3 weeks. This issue should be resolved in recent versions of aider.\\n\\n\"\"\"\n    \"\"\"If you find that this bug is still present, please feel free to reopen this \"\"\"\n    \"\"\"issue or create a new one with steps to reproduce.\"\"\" + BOT_SUFFIX\n)\n\n# GitHub API configuration\nGITHUB_API_URL = \"https://api.github.com\"\nREPO_OWNER = \"Aider-AI\"\nREPO_NAME = \"aider\"\nTOKEN = os.getenv(\"GITHUB_TOKEN\")\n\nheaders = {\"Authorization\": f\"token {TOKEN}\", \"Accept\": \"application/vnd.github.v3+json\"}",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/issues.py::2",
    "metadata": {
      "file_path": "scripts/issues.py",
      "file_name": "issues.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 250,
      "span_ids": [
        "get_issues"
      ],
      "start_line": 72,
      "end_line": 101,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def get_issues(state=\"open\"):\n    issues = []\n    page = 1\n    per_page = 100\n\n    # First, get the total count of issues\n    response = requests.get(\n        f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues\",\n        headers=headers,\n        params={\"state\": state, \"per_page\": 1},\n    )\n    response.raise_for_status()\n    total_count = int(response.headers.get(\"Link\", \"\").split(\"page=\")[-1].split(\">\")[0])\n    total_pages = (total_count + per_page - 1) // per_page\n\n    with tqdm(total=total_pages, desc=\"Collecting issues\", unit=\"page\") as pbar:\n        while True:\n            response = requests.get(\n                f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues\",\n                headers=headers,\n                params={\"state\": state, \"page\": page, \"per_page\": per_page},\n            )\n            response.raise_for_status()\n            page_issues = response.json()\n            if not page_issues:\n                break\n            issues.extend(page_issues)\n            page += 1\n            pbar.update(1)\n    return issues",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/issues.py::3",
    "metadata": {
      "file_path": "scripts/issues.py",
      "file_name": "issues.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 180,
      "span_ids": [
        "find_oldest_issue",
        "group_issues_by_subject"
      ],
      "start_line": 104,
      "end_line": 125,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def group_issues_by_subject(issues):\n    grouped_issues = defaultdict(list)\n    pattern = r\"Uncaught .+ in .+ line \\d+\"\n    for issue in issues:\n        if re.search(pattern, issue[\"title\"]) and not has_been_reopened(issue[\"number\"]):\n            subject = issue[\"title\"]\n            grouped_issues[subject].append(issue)\n    return grouped_issues\n\n\ndef find_oldest_issue(subject, all_issues):\n    oldest_issue = None\n    oldest_date = datetime.now()\n\n    for issue in all_issues:\n        if issue[\"title\"] == subject and not has_been_reopened(issue[\"number\"]):\n            created_at = datetime.strptime(issue[\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\")\n            if created_at < oldest_date:\n                oldest_date = created_at\n                oldest_issue = issue\n\n    return oldest_issue",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/issues.py::4",
    "metadata": {
      "file_path": "scripts/issues.py",
      "file_name": "issues.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 167,
      "span_ids": [
        "comment_and_close_duplicate"
      ],
      "start_line": 128,
      "end_line": 144,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def comment_and_close_duplicate(issue, oldest_issue):\n    comment_url = (\n        f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"\n    )\n    close_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n\n    comment_body = DUPLICATE_COMMENT.format(oldest_issue_number=oldest_issue[\"number\"])\n\n    # Post comment\n    response = requests.post(comment_url, headers=headers, json={\"body\": comment_body})\n    response.raise_for_status()\n\n    # Close issue\n    response = requests.patch(close_url, headers=headers, json={\"state\": \"closed\"})\n    response.raise_for_status()\n\n    print(f\"  - Commented and closed issue #{issue['number']}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/issues.py::5",
    "metadata": {
      "file_path": "scripts/issues.py",
      "file_name": "issues.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 169,
      "span_ids": [
        "find_unlabeled_with_paul_comments"
      ],
      "start_line": 147,
      "end_line": 166,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def find_unlabeled_with_paul_comments(issues):\n    unlabeled_issues = []\n    for issue in issues:\n        # Skip pull requests\n        if \"pull_request\" in issue:\n            continue\n\n        if not issue[\"labels\"] and issue[\"state\"] == \"open\":\n            # Get comments for this issue\n            comments_url = (\n                f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"\n            )\n            response = requests.get(comments_url, headers=headers)\n            response.raise_for_status()\n            comments = response.json()\n\n            # Check if paul-gauthier has commented\n            if any(comment[\"user\"][\"login\"] == \"paul-gauthier\" for comment in comments):\n                unlabeled_issues.append(issue)\n    return unlabeled_issues",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/issues.py::6",
    "metadata": {
      "file_path": "scripts/issues.py",
      "file_name": "issues.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 261,
      "span_ids": [
        "handle_unlabeled_issues"
      ],
      "start_line": 169,
      "end_line": 192,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def handle_unlabeled_issues(all_issues, auto_yes):\n    print(\"\\nFinding unlabeled issues with paul-gauthier comments...\")\n    unlabeled_issues = find_unlabeled_with_paul_comments(all_issues)\n\n    if not unlabeled_issues:\n        print(\"No unlabeled issues with paul-gauthier comments found.\")\n        return\n\n    print(f\"\\nFound {len(unlabeled_issues)} unlabeled issues with paul-gauthier comments:\")\n    for issue in unlabeled_issues:\n        print(f\"  - #{issue['number']}: {issue['title']} {issue['html_url']}\")\n\n    if not auto_yes:\n        confirm = input(\"\\nDo you want to add the 'question' label to these issues? (y/n): \")\n        if confirm.lower() != \"y\":\n            print(\"Skipping labeling.\")\n            return\n\n    print(\"\\nAdding 'question' label to issues...\")\n    for issue in unlabeled_issues:\n        url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n        response = requests.patch(url, headers=headers, json={\"labels\": [\"question\"]})\n        response.raise_for_status()\n        print(f\"  - Added 'question' label to #{issue['number']}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/issues.py::7",
    "metadata": {
      "file_path": "scripts/issues.py",
      "file_name": "issues.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 415,
      "span_ids": [
        "handle_stale_issues"
      ],
      "start_line": 195,
      "end_line": 235,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def handle_stale_issues(all_issues, auto_yes):\n    print(\"\\nChecking for stale question issues...\")\n\n    for issue in all_issues:\n        # Skip if not open, not a question, already stale, or has been reopened\n        if (\n            issue[\"state\"] != \"open\"\n            or \"question\" not in [label[\"name\"] for label in issue[\"labels\"]]\n            or \"stale\" in [label[\"name\"] for label in issue[\"labels\"]]\n            or has_been_reopened(issue[\"number\"])\n        ):\n            continue\n\n        # Get latest activity timestamp from issue or its comments\n        latest_activity = datetime.strptime(issue[\"updated_at\"], \"%Y-%m-%dT%H:%M:%SZ\")\n\n        # Check if issue is stale (no activity for 14 days)\n        days_inactive = (datetime.now() - latest_activity).days\n        if days_inactive >= 14:\n            print(f\"\\nStale issue found: #{issue['number']}: {issue['title']}\\n{issue['html_url']}\")\n            print(f\"  No activity for {days_inactive} days\")\n\n            if not auto_yes:\n                confirm = input(\"Add stale label and comment? (y/n): \")\n                if confirm.lower() != \"y\":\n                    print(\"Skipping this issue.\")\n                    continue\n\n            # Add comment\n            comment_url = (\n                f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"\n            )\n            response = requests.post(comment_url, headers=headers, json={\"body\": STALE_COMMENT})\n            response.raise_for_status()\n\n            # Add stale label\n            url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n            response = requests.patch(url, headers=headers, json={\"labels\": [\"question\", \"stale\"]})\n            response.raise_for_status()\n\n            print(f\"  Added stale label and comment to #{issue['number']}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/issues.py::8",
    "metadata": {
      "file_path": "scripts/issues.py",
      "file_name": "issues.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 780,
      "span_ids": [
        "handle_stale_closing"
      ],
      "start_line": 238,
      "end_line": 320,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def handle_stale_closing(all_issues, auto_yes):\n    print(\"\\nChecking for issues to close or unstale...\")\n\n    for issue in all_issues:\n        # Skip if not open or not stale\n        if issue[\"state\"] != \"open\" or \"stale\" not in [label[\"name\"] for label in issue[\"labels\"]]:\n            continue\n\n        # Get the timeline to find when the stale label was last added\n        timeline_url = (\n            f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/timeline\"\n        )\n        response = requests.get(timeline_url, headers=headers)\n        response.raise_for_status()\n        events = response.json()\n\n        # Find the most recent stale label addition\n        stale_events = [\n            event\n            for event in events\n            if event.get(\"event\") == \"labeled\" and event.get(\"label\", {}).get(\"name\") == \"stale\"\n        ]\n\n        if not stale_events:\n            continue\n\n        latest_stale = datetime.strptime(stale_events[-1][\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\")\n\n        # Get comments since the stale label\n        comments_url = (\n            f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"\n        )\n        response = requests.get(comments_url, headers=headers)\n        response.raise_for_status()\n        comments = response.json()\n\n        # Check for comments newer than the stale label\n        new_comments = [\n            comment\n            for comment in comments\n            if datetime.strptime(comment[\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\") > latest_stale\n        ]\n\n        if new_comments:\n            print(f\"\\nFound new activity on stale issue #{issue['number']}: {issue['title']}\")\n            print(f\"  {len(new_comments)} new comments since stale label\")\n\n            if not auto_yes:\n                confirm = input(\"Remove stale label? (y/n): \")\n                if confirm.lower() != \"y\":\n                    print(\"Skipping this issue.\")\n                    continue\n\n            # Remove stale label but keep question label\n            url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n            response = requests.patch(url, headers=headers, json={\"labels\": [\"question\"]})\n            response.raise_for_status()\n            print(f\"  Removed stale label from #{issue['number']}\")\n        else:\n            # Check if it's been 7 days since stale label\n            days_stale = (datetime.now() - latest_stale).days\n            if days_stale >= 7:\n                print(f\"\\nStale issue ready for closing #{issue['number']}: {issue['title']}\")\n                print(f\"  No activity for {days_stale} days since stale label\")\n\n                if not auto_yes:\n                    confirm = input(\"Close this issue? (y/n): \")\n                    if confirm.lower() != \"y\":\n                        print(\"Skipping this issue.\")\n                        continue\n\n                # Add closing comment\n                comment_url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"  # noqa\n                response = requests.post(\n                    comment_url, headers=headers, json={\"body\": CLOSE_STALE_COMMENT}\n                )\n                response.raise_for_status()\n\n                # Close the issue\n                url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n                response = requests.patch(url, headers=headers, json={\"state\": \"closed\"})\n                response.raise_for_status()\n                print(f\"  Closed issue #{issue['number']}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/issues.py::9",
    "metadata": {
      "file_path": "scripts/issues.py",
      "file_name": "issues.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 560,
      "span_ids": [
        "handle_fixed_issues"
      ],
      "start_line": 323,
      "end_line": 382,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def handle_fixed_issues(all_issues, auto_yes):\n    print(\"\\nChecking for fixed enhancement and bug issues to close...\")\n\n    for issue in all_issues:\n        # Skip if not open or doesn't have fixed label\n        labels = [label[\"name\"] for label in issue[\"labels\"]]\n        if issue[\"state\"] != \"open\" or \"fixed\" not in labels:\n            continue\n\n        # Check if it's an enhancement or bug\n        is_enhancement = \"enhancement\" in labels\n        is_bug = \"bug\" in labels\n        if not (is_enhancement or is_bug):\n            continue\n\n        # Find when the fixed label was added\n        timeline_url = (\n            f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/timeline\"\n        )\n        response = requests.get(timeline_url, headers=headers)\n        response.raise_for_status()\n        events = response.json()\n\n        # Find the most recent fixed label addition\n        fixed_events = [\n            event\n            for event in events\n            if event.get(\"event\") == \"labeled\" and event.get(\"label\", {}).get(\"name\") == \"fixed\"\n        ]\n\n        if not fixed_events:\n            continue\n\n        latest_fixed = datetime.strptime(fixed_events[-1][\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\")\n        days_fixed = (datetime.now() - latest_fixed).days\n\n        if days_fixed >= 21:\n            issue_type = \"enhancement\" if is_enhancement else \"bug\"\n            print(f\"\\nFixed {issue_type} ready for closing #{issue['number']}: {issue['title']}\")\n            print(f\"  Has been marked fixed for {days_fixed} days\")\n\n            if not auto_yes:\n                confirm = input(\"Close this issue? (y/n): \")\n                if confirm.lower() != \"y\":\n                    print(\"Skipping this issue.\")\n                    continue\n\n            # Add closing comment\n            comment_url = (\n                f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}/comments\"\n            )\n            comment = CLOSE_FIXED_ENHANCEMENT_COMMENT if is_enhancement else CLOSE_FIXED_BUG_COMMENT\n            response = requests.post(comment_url, headers=headers, json={\"body\": comment})\n            response.raise_for_status()\n\n            # Close the issue\n            url = f\"{GITHUB_API_URL}/repos/{REPO_OWNER}/{REPO_NAME}/issues/{issue['number']}\"\n            response = requests.patch(url, headers=headers, json={\"state\": \"closed\"})\n            response.raise_for_status()\n            print(f\"  Closed issue #{issue['number']}\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/issues.py::10",
    "metadata": {
      "file_path": "scripts/issues.py",
      "file_name": "issues.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 355,
      "span_ids": [
        "handle_duplicate_issues"
      ],
      "start_line": 385,
      "end_line": 422,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def handle_duplicate_issues(all_issues, auto_yes):\n    open_issues = [issue for issue in all_issues if issue[\"state\"] == \"open\"]\n    grouped_open_issues = group_issues_by_subject(open_issues)\n\n    print(\"Looking for duplicate issues (skipping reopened issues)...\")\n    for subject, issues in grouped_open_issues.items():\n        oldest_issue = find_oldest_issue(subject, all_issues)\n        if not oldest_issue:\n            continue\n\n        related_issues = set(issue[\"number\"] for issue in issues)\n        related_issues.add(oldest_issue[\"number\"])\n        if len(related_issues) <= 1:\n            continue\n\n        print(f\"\\nIssue: {subject}\")\n        print(f\"Open issues: {len(issues)}\")\n        sorted_issues = sorted(issues, key=lambda x: x[\"number\"], reverse=True)\n        for issue in sorted_issues:\n            print(f\"  - #{issue['number']}: {issue['comments']} comments {issue['html_url']}\")\n\n        print(\n            f\"Oldest issue: #{oldest_issue['number']}: {oldest_issue['comments']} comments\"\n            f\" {oldest_issue['html_url']} ({oldest_issue['state']})\"\n        )\n\n        if not auto_yes:\n            confirm = input(\"Do you want to comment and close duplicate issues? (y/n): \")\n            if confirm.lower() != \"y\":\n                print(\"Skipping this group of issues.\")\n                continue\n\n        for issue in issues:\n            if issue[\"number\"] != oldest_issue[\"number\"]:\n                comment_and_close_duplicate(issue, oldest_issue)\n\n        if oldest_issue[\"state\"] == \"open\":\n            print(f\"Oldest issue #{oldest_issue['number']} left open\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/issues.py::11",
    "metadata": {
      "file_path": "scripts/issues.py",
      "file_name": "issues.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 154,
      "span_ids": [
        "main",
        "impl:24"
      ],
      "start_line": 425,
      "end_line": 447,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main():\n    parser = argparse.ArgumentParser(description=\"Handle duplicate GitHub issues\")\n    parser.add_argument(\n        \"--yes\", action=\"store_true\", help=\"Automatically close duplicates without prompting\"\n    )\n    args = parser.parse_args()\n\n    if not TOKEN:\n        print(\"Error: Missing GITHUB_TOKEN environment variable. Please check your .env file.\")\n        return\n\n    all_issues = get_issues(\"all\")\n\n    handle_unlabeled_issues(all_issues, args.yes)\n    handle_stale_issues(all_issues, args.yes)\n    handle_stale_closing(all_issues, args.yes)\n    handle_duplicate_issues(all_issues, args.yes)\n    handle_fixed_issues(all_issues, args.yes)\n\n\nif __name__ == \"__main__\":\n    main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/my_models.py::1",
    "metadata": {
      "file_path": "scripts/my_models.py",
      "file_name": "my_models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 200,
      "span_ids": [
        "collect_model_stats",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 30,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "#!/usr/bin/env python3\n\nimport json\nfrom collections import defaultdict, deque\nfrom pathlib import Path\n\n\ndef collect_model_stats(n_lines=1000):\n    \"\"\"Collect model usage statistics from the analytics file.\"\"\"\n    analytics_path = Path.home() / \".aider\" / \"analytics.jsonl\"\n    model_stats = defaultdict(int)\n\n    with open(analytics_path) as f:\n        lines = deque(f, n_lines)\n        for line in lines:\n            try:\n                event = json.loads(line)\n                if event[\"event\"] == \"message_send\":\n                    properties = event[\"properties\"]\n                    main_model = properties.get(\"main_model\")\n\n                    total_tokens = properties.get(\"total_tokens\", 0)\n                    if main_model == \"deepseek/deepseek-coder\":\n                        main_model = \"deepseek/deepseek-chat\"\n                    if main_model:\n                        model_stats[main_model] += total_tokens\n            except json.JSONDecodeError:\n                continue\n\n    return model_stats",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/my_models.py::2",
    "metadata": {
      "file_path": "scripts/my_models.py",
      "file_name": "my_models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 195,
      "span_ids": [
        "format_text_table"
      ],
      "start_line": 33,
      "end_line": 50,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def format_text_table(model_stats):\n    \"\"\"Format model statistics as a text table.\"\"\"\n    total_tokens = sum(model_stats.values())\n    lines = []\n\n    lines.append(\"\\nModel Token Usage Summary:\")\n    lines.append(\"-\" * 80)\n    lines.append(f\"{'Model Name':<40} {'Total Tokens':>15} {'Percent':>10}\")\n    lines.append(\"-\" * 80)\n\n    for model, tokens in sorted(model_stats.items(), key=lambda x: x[1], reverse=True):\n        percentage = (tokens / total_tokens) * 100 if total_tokens > 0 else 0\n        lines.append(f\"{model:<40} {tokens:>15,} {percentage:>9.1f}%\")\n\n    lines.append(\"-\" * 80)\n    lines.append(f\"{'TOTAL':<40} {total_tokens:>15,} {100:>9.1f}%\")\n\n    return \"\\n\".join(lines)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/my_models.py::3",
    "metadata": {
      "file_path": "scripts/my_models.py",
      "file_name": "my_models.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 395,
      "span_ids": [
        "format_html_table",
        "impl"
      ],
      "start_line": 53,
      "end_line": 99,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def format_html_table(model_stats):\n    \"\"\"Format model statistics as an HTML table.\"\"\"\n    total_tokens = sum(model_stats.values())\n\n    html = [\n        \"<style>\",\n        \"table { border-collapse: collapse; width: 100%; }\",\n        \"th, td { padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }\",\n        \"th { background-color: #f2f2f2; }\",\n        \"tr:hover { background-color: #f5f5f5; }\",\n        \".right { text-align: right; }\",\n        \"</style>\",\n        \"<table>\",\n        (\n            \"<tr><th>Model Name</th><th class='right'>Total Tokens</th><th\"\n            \" class='right'>Percent</th></tr>\"\n        ),\n    ]\n\n    for model, tokens in sorted(model_stats.items(), key=lambda x: x[1], reverse=True):\n        percentage = (tokens / total_tokens) * 100 if total_tokens > 0 else 0\n        html.append(\n            f\"<tr><td>{model}</td>\"\n            f\"<td class='right'>{tokens:,}</td>\"\n            f\"<td class='right'>{percentage:.1f}%</td></tr>\"\n        )\n\n    html.append(\"</table>\")\n\n    # Add note about redacted models if any are present\n    if any(\"REDACTED\" in model for model in model_stats.keys()):\n        html.extend(\n            [\n                \"\",\n                \"{: .note :}\",\n                \"Some models show as REDACTED, because they are new or unpopular models.\",\n                'Aider\\'s analytics only records the names of \"well known\" LLMs.',\n            ]\n        )\n\n    return \"\\n\".join(html)\n\n\nif __name__ == \"__main__\":\n    stats = collect_model_stats()\n    print(format_text_table(stats))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/update-history.py::1",
    "metadata": {
      "file_path": "scripts/update-history.py",
      "file_name": "update-history.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 204,
      "span_ids": [
        "run_git_log",
        "docstring",
        "get_base_version"
      ],
      "start_line": 1,
      "end_line": 36,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "#!/usr/bin/env python3\n\nimport os\nimport re\nimport subprocess\nimport tempfile\n\nfrom history_prompts import history_prompt\n\nfrom aider import __version__\n\n\ndef get_base_version():\n    # Parse current version like \"0.64.2.dev\" to get major.minor\n    match = re.match(r\"(\\d+\\.\\d+)\", __version__)\n    if not match:\n        raise ValueError(f\"Could not parse version: {__version__}\")\n    return match.group(1) + \".0\"\n\n\ndef run_git_log():\n    base_ver = get_base_version()\n    cmd = [\n        \"git\",\n        \"log\",\n        \"-p\",\n        \"--pretty=full\",\n        f\"v{base_ver}..HEAD\",\n        \"--\",\n        \"aider/\",\n        \":!aider/website/\",\n        \":!scripts/\",\n        \":!HISTORY.md\",\n    ]\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    return result.stdout",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/update-history.py::2",
    "metadata": {
      "file_path": "scripts/update-history.py",
      "file_name": "update-history.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 673,
      "span_ids": [
        "impl",
        "main"
      ],
      "start_line": 39,
      "end_line": 120,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main():\n    # Get the git log output\n    diff_content = run_git_log()\n\n    # Extract relevant portion of HISTORY.md\n    base_ver = get_base_version()\n    with open(\"HISTORY.md\", \"r\") as f:\n        history_content = f.read()\n\n    # Find the section for this version\n    version_header = f\"### Aider v{base_ver}\"\n    start_idx = history_content.find(\"# Release history\")\n    if start_idx == -1:\n        raise ValueError(\"Could not find start of release history\")\n\n    # Find where this version's section ends\n    version_idx = history_content.find(version_header, start_idx)\n    if version_idx == -1:\n        raise ValueError(f\"Could not find version header: {version_header}\")\n\n    # Find the next version header after this one\n    next_version_idx = history_content.find(\"\\n### Aider v\", version_idx + len(version_header))\n    if next_version_idx == -1:\n        # No next version found, use the rest of the file\n        relevant_history = history_content[start_idx:]\n    else:\n        # Extract just up to the next version\n        relevant_history = history_content[start_idx:next_version_idx]\n\n    # Save relevant portions to temporary files\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".diff\") as tmp_diff:\n        tmp_diff.write(diff_content)\n        diff_path = tmp_diff.name\n\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".md\") as tmp_hist:\n        tmp_hist.write(relevant_history)\n        hist_path = tmp_hist.name\n\n    # Run blame to get aider percentage\n    blame_result = subprocess.run([\"python3\", \"scripts/blame.py\"], capture_output=True, text=True)\n    aider_line = blame_result.stdout.strip().split(\"\\n\")[-1]  # Get last line with percentage\n\n    # Construct and run the aider command\n    message = history_prompt.format(aider_line=aider_line)\n\n    cmd = [\"aider\", hist_path, \"--read\", diff_path, \"--msg\", message, \"--no-auto-commit\"]\n    subprocess.run(cmd)\n\n    # Read back the updated history\n    with open(hist_path, \"r\") as f:\n        updated_history = f.read()\n\n    # Find where the next version section would start\n    if next_version_idx == -1:\n        # No next version found, use the rest of the file\n        full_history = history_content[:start_idx] + updated_history\n    else:\n        # Splice the updated portion back in between the unchanged parts\n        full_history = (\n            history_content[:start_idx]\n            + updated_history  # Keep unchanged header\n            + history_content[next_version_idx:]  # Add updated portion  # Keep older entries\n        )\n\n    # Write back the full history\n    with open(\"HISTORY.md\", \"w\") as f:\n        f.write(full_history)\n\n    # Run update-docs.sh after aider\n    subprocess.run([\"scripts/update-docs.sh\"])\n\n    # Cleanup\n    os.unlink(diff_path)\n    os.unlink(hist_path)\n\n    # Show git diff of HISTORY.md\n    subprocess.run([\"git\", \"diff\", \"HISTORY.md\"])\n\n\nif __name__ == \"__main__\":\n    main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/versionbump.py::1",
    "metadata": {
      "file_path": "scripts/versionbump.py",
      "file_name": "versionbump.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 255,
      "span_ids": [
        "main",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 38,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "#!/usr/bin/env python\n\nimport argparse\nimport datetime\nimport os\nimport re\nimport subprocess\nimport sys\n\nfrom packaging import version\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Bump version\")\n    parser.add_argument(\"new_version\", help=\"New version in x.y.z format\")\n    parser.add_argument(\n        \"--dry-run\", action=\"store_true\", help=\"Print each step without actually executing them\"\n    )\n\n    # Function to check if we are on the main branch\n    def check_branch():\n        branch = subprocess.run(\n            [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"], capture_output=True, text=True\n        ).stdout.strip()\n        if branch != \"main\":\n            print(\"Error: Not on the main branch.\")\n            sys.exit(1)\n\n    # Function to check if the working directory is clean\n    def check_working_directory_clean():\n        status = subprocess.run(\n            [\"git\", \"status\", \"--porcelain\"], capture_output=True, text=True\n        ).stdout\n        if status:\n            print(\"Error: Working directory is not clean.\")\n            sys.exit(1)\n\n    # Function to fetch the latest changes and check if the main branch is up to date\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/versionbump.py::2",
    "metadata": {
      "file_path": "scripts/versionbump.py",
      "file_name": "versionbump.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 353,
      "span_ids": [
        "main"
      ],
      "start_line": 39,
      "end_line": 70,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main():\n    # ... other code\n    def check_main_branch_up_to_date():\n        subprocess.run([\"git\", \"fetch\", \"origin\"], check=True)\n        local_main = subprocess.run(\n            [\"git\", \"rev-parse\", \"main\"], capture_output=True, text=True\n        ).stdout.strip()\n        print(f\"Local main commit hash: {local_main}\")\n        origin_main = subprocess.run(\n            [\"git\", \"rev-parse\", \"origin/main\"], capture_output=True, text=True\n        ).stdout.strip()\n        print(f\"Origin main commit hash: {origin_main}\")\n        if local_main != origin_main:\n            local_date = subprocess.run(\n                [\"git\", \"show\", \"-s\", \"--format=%ci\", \"main\"], capture_output=True, text=True\n            ).stdout.strip()\n            origin_date = subprocess.run(\n                [\"git\", \"show\", \"-s\", \"--format=%ci\", \"origin/main\"], capture_output=True, text=True\n            ).stdout.strip()\n            local_date = datetime.datetime.strptime(local_date, \"%Y-%m-%d %H:%M:%S %z\")\n            origin_date = datetime.datetime.strptime(origin_date, \"%Y-%m-%d %H:%M:%S %z\")\n            if local_date < origin_date:\n                print(\n                    \"Error: The local main branch is behind origin/main. Please pull the latest\"\n                    \" changes.\"\n                )\n            elif local_date > origin_date:\n                print(\n                    \"Error: The origin/main branch is behind the local main branch. Please push\"\n                    \" your changes.\"\n                )\n            else:\n                print(\"Error: The main branch and origin/main have diverged.\")\n            sys.exit(1)\n    # ... other code",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/versionbump.py::3",
    "metadata": {
      "file_path": "scripts/versionbump.py",
      "file_name": "versionbump.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 708,
      "span_ids": [
        "impl",
        "main"
      ],
      "start_line": 72,
      "end_line": 157,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main():\n    # ... other code\n\n    args = parser.parse_args()\n    dry_run = args.dry_run\n\n    # Perform checks before proceeding\n    check_branch()\n    check_working_directory_clean()\n    check_main_branch_up_to_date()\n\n    new_version_str = args.new_version\n    if not re.match(r\"^\\d+\\.\\d+\\.\\d+$\", new_version_str):\n        raise ValueError(f\"Invalid version format, must be x.y.z: {new_version_str}\")\n\n    new_version = version.parse(new_version_str)\n    incremented_version = version.Version(\n        f\"{new_version.major}.{new_version.minor}.{new_version.micro + 1}\"\n    )\n\n    from aider import __version__ as current_version\n\n    if new_version <= version.parse(current_version):\n        raise ValueError(\n            f\"New version {new_version} must be greater than the current version {current_version}\"\n        )\n\n    with open(\"aider/__init__.py\", \"r\") as f:\n        content = f.read()\n    updated_content = re.sub(r'__version__ = \".+?\"', f'__version__ = \"{new_version}\"', content)\n\n    print(\"Updating aider/__init__.py with new version:\")\n    print(updated_content)\n    if not dry_run:\n        with open(\"aider/__init__.py\", \"w\") as f:\n            f.write(updated_content)\n\n    git_commands = [\n        [\"git\", \"add\", \"aider/__init__.py\"],\n        [\"git\", \"commit\", \"-m\", f\"version bump to {new_version}\"],\n        [\"git\", \"tag\", f\"v{new_version}\"],\n        [\"git\", \"push\", \"origin\"],\n        [\"git\", \"push\", \"origin\", f\"v{new_version}\", \"--no-verify\"],\n    ]\n\n    for cmd in git_commands:\n        print(f\"Running: {' '.join(cmd)}\")\n        if not dry_run:\n            subprocess.run(\n                cmd,\n                check=True,\n            )\n\n    new_dev_version = f\"{incremented_version}.dev\"\n    updated_dev_content = re.sub(\n        r'__version__ = \".+?\"', f'__version__ = \"{new_dev_version}\"', content\n    )\n\n    print()\n    print(\"Updating aider/__init__.py with new dev version:\")\n    print(updated_dev_content)\n    if not dry_run:\n        with open(\"aider/__init__.py\", \"w\") as f:\n            f.write(updated_dev_content)\n\n    git_commands_dev = [\n        [\"git\", \"add\", \"aider/__init__.py\"],\n        [\"git\", \"commit\", \"-m\", f\"set version to {new_dev_version}\"],\n        [\"git\", \"tag\", f\"v{new_dev_version}\"],\n        [\"git\", \"push\", \"origin\", \"--no-verify\"],\n        [\"git\", \"push\", \"origin\", f\"v{new_dev_version}\", \"--no-verify\"],\n    ]\n\n    for cmd in git_commands_dev:\n        print(f\"Running: {' '.join(cmd)}\")\n        if not dry_run:\n            subprocess.run(cmd, check=True)\n\n    # Remove aider/_version.py if it exists\n    version_file = \"aider/_version.py\"\n    if os.path.exists(version_file):\n        print(f\"Removing {version_file}\")\n        if not dry_run:\n            os.remove(version_file)\n\n\nif __name__ == \"__main__\":\n    main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scripts/yank-old-versions.py::1",
    "metadata": {
      "file_path": "scripts/yank-old-versions.py",
      "file_name": "yank-old-versions.py",
      "file_type": "text/x-python",
      "category": "implementation",
      "tokens": 353,
      "span_ids": [
        "imports",
        "main",
        "get_versions_supporting_python38_or_lower",
        "impl"
      ],
      "start_line": 1,
      "end_line": 52,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import requests\nfrom packaging import version\nfrom packaging.specifiers import SpecifierSet\n\n\ndef get_versions_supporting_python38_or_lower(package_name):\n    url = f\"https://pypi.org/pypi/{package_name}/json\"\n    response = requests.get(url)\n    if response.status_code != 200:\n        print(f\"Failed to fetch data for {package_name}\")\n        return {}\n\n    data = response.json()\n    compatible_versions = {}\n\n    for release, release_data in data[\"releases\"].items():\n        if not release_data:  # Skip empty releases\n            continue\n\n        requires_python = release_data[0].get(\"requires_python\")\n\n        if requires_python is None:\n            compatible_versions[release] = (\n                \"Unspecified (assumed compatible with Python 3.8 and lower)\"\n            )\n        else:\n            try:\n                spec = SpecifierSet(requires_python)\n                if version.parse(\"3.8\") in spec:\n                    compatible_versions[release] = (\n                        f\"Compatible with Python 3.8 (spec: {requires_python})\"\n                    )\n            except ValueError:\n                print(f\"Invalid requires_python specifier for version {release}: {requires_python}\")\n\n    return compatible_versions\n\n\ndef main():\n    package_name = \"aider-chat\"  # Replace with your package name\n    compatible_versions = get_versions_supporting_python38_or_lower(package_name)\n\n    print(f\"Versions of {package_name} compatible with Python 3.8 or lower:\")\n    for release, support in sorted(\n        compatible_versions.items(), key=lambda x: version.parse(x[0]), reverse=True\n    ):\n        print(f\"{release}: {support}\")\n\n\nif __name__ == \"__main__\":\n    main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_analytics.py::1",
    "metadata": {
      "file_path": "tests/basic/test_analytics.py",
      "file_name": "test_analytics.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 301,
      "span_ids": [
        "temp_analytics_file",
        "test_analytics_enable_disable",
        "test_analytics_data_persistence",
        "test_analytics_initialization",
        "imports",
        "temp_data_dir"
      ],
      "start_line": 1,
      "end_line": 57,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import json\nimport os\nimport tempfile\nfrom pathlib import Path\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom aider.analytics import Analytics\n\n\n@pytest.fixture\ndef temp_analytics_file():\n    with tempfile.NamedTemporaryFile(delete=False) as f:\n        yield f.name\n    os.unlink(f.name)\n\n\n@pytest.fixture\ndef temp_data_dir(monkeypatch):\n    with tempfile.TemporaryDirectory() as tmpdir:\n        temp_dir = Path(tmpdir)\n        monkeypatch.setattr(Path, \"home\", lambda: temp_dir)\n        yield temp_dir\n\n\ndef test_analytics_initialization(temp_data_dir):\n    analytics = Analytics(permanently_disable=True)\n    assert analytics.mp is None\n    assert analytics.ph is None\n    assert analytics.permanently_disable is True\n    assert analytics.user_id is not None\n\n\ndef test_analytics_enable_disable(temp_data_dir):\n    analytics = Analytics()\n    analytics.asked_opt_in = True\n\n    analytics.enable()\n    # assert analytics.mp is not None\n    assert analytics.ph is not None\n\n    analytics.disable(permanently=False)\n    assert analytics.mp is None\n    assert analytics.ph is None\n    assert analytics.permanently_disable is not True\n\n    analytics.disable(permanently=True)\n    assert analytics.permanently_disable is True\n\n\ndef test_analytics_data_persistence(temp_data_dir):\n    analytics1 = Analytics()\n    user_id = analytics1.user_id\n\n    analytics2 = Analytics()\n    assert analytics2.user_id == user_id",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_analytics.py::2",
    "metadata": {
      "file_path": "tests/basic/test_analytics.py",
      "file_name": "test_analytics.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 169,
      "span_ids": [
        "test_analytics_event_logging"
      ],
      "start_line": 60,
      "end_line": 79,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def test_analytics_event_logging(temp_analytics_file, temp_data_dir):\n    analytics = Analytics(logfile=temp_analytics_file)\n    analytics.asked_opt_in = True\n    analytics.enable()\n\n    test_event = \"test_event\"\n    test_properties = {\"test_key\": \"test_value\"}\n\n    # with patch.object(analytics.mp, \"track\") as mock_mp_track:\n    with patch.object(analytics.ph, \"capture\") as mock_ph_capture:\n        analytics.event(test_event, **test_properties)\n\n        # mock_mp_track.assert_called_once()\n        mock_ph_capture.assert_called_once()\n\n        # Verify logfile\n        with open(temp_analytics_file) as f:\n            log_entry = json.loads(f.read().strip())\n            assert log_entry[\"event\"] == test_event\n            assert \"test_key\" in log_entry[\"properties\"]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_analytics.py::3",
    "metadata": {
      "file_path": "tests/basic/test_analytics.py",
      "file_name": "test_analytics.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 160,
      "span_ids": [
        "test_system_info",
        "test_need_to_ask"
      ],
      "start_line": 82,
      "end_line": 104,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def test_system_info(temp_data_dir):\n    analytics = Analytics()\n    sys_info = analytics.get_system_info()\n\n    assert \"python_version\" in sys_info\n    assert \"os_platform\" in sys_info\n    assert \"os_release\" in sys_info\n    assert \"machine\" in sys_info\n\n\ndef test_need_to_ask(temp_data_dir):\n    analytics = Analytics()\n    assert analytics.need_to_ask(True) is True\n    assert analytics.need_to_ask(False) is False\n\n    analytics.user_id = \"000\"\n    assert analytics.need_to_ask(None) is True\n\n    analytics.asked_opt_in = True\n    assert analytics.need_to_ask(True) is False\n\n    analytics.permanently_disable = True\n    assert analytics.need_to_ask(True) is False",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_analytics.py::4",
    "metadata": {
      "file_path": "tests/basic/test_analytics.py",
      "file_name": "test_analytics.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 443,
      "span_ids": [
        "test_is_uuid_in_percentage"
      ],
      "start_line": 107,
      "end_line": 137,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def test_is_uuid_in_percentage():\n    from aider.analytics import is_uuid_in_percentage\n\n    # Test basic percentage thresholds\n    assert is_uuid_in_percentage(\"00000000000000000000000000000000\", 1) is True\n    assert is_uuid_in_percentage(\"01999000000000000000000000000000\", 1) is True\n    assert is_uuid_in_percentage(\"02000000000000000000000000000000\", 1) is True\n    assert is_uuid_in_percentage(\"02910000000000000000000000000001\", 1) is False\n    assert is_uuid_in_percentage(\"03000000000000000000000000000000\", 1) is False\n    assert is_uuid_in_percentage(\"ff000000000000000000000000000000\", 1) is False\n\n    assert is_uuid_in_percentage(\"00000000000000000000000000000000\", 10) is True\n    assert is_uuid_in_percentage(\"19000000000000000000000000000000\", 10) is True\n    assert is_uuid_in_percentage(\"1a000000000000000000000000000000\", 10) is False\n    assert is_uuid_in_percentage(\"ff000000000000000000000000000000\", 10) is False\n\n    # Test edge cases\n    assert is_uuid_in_percentage(\"00000000000000000000000000000000\", 0) is False\n    assert is_uuid_in_percentage(\"00000000000000000000000000000000\", 100) is True\n    assert is_uuid_in_percentage(\"ffffffffffffffffffffffffffffffff\", 100) is True\n\n    # Test invalid inputs\n    with pytest.raises(ValueError):\n        is_uuid_in_percentage(\"00000000000000000000000000000000\", -1)\n    with pytest.raises(ValueError):\n        is_uuid_in_percentage(\"00000000000000000000000000000000\", 101)\n\n    # Test empty/None UUID\n    assert is_uuid_in_percentage(\"\", 50) is False\n    assert is_uuid_in_percentage(None, 50) is False",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::1",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 316,
      "span_ids": [
        "TestCoder",
        "imports",
        "TestCoder.test_allowed_to_edit",
        "TestCoder.setUp"
      ],
      "start_line": 1,
      "end_line": 49,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import os\nimport tempfile\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import MagicMock, patch\n\nimport git\n\nfrom aider.coders import Coder\nfrom aider.coders.base_coder import UnknownEditFormat\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import InputOutput\nfrom aider.models import Model\nfrom aider.repo import GitRepo\nfrom aider.utils import GitTemporaryDirectory\n\n\nclass TestCoder(unittest.TestCase):\n    def setUp(self):\n        self.GPT35 = Model(\"gpt-3.5-turbo\")\n        self.webbrowser_patcher = patch(\"aider.io.webbrowser.open\")\n        self.mock_webbrowser = self.webbrowser_patcher.start()\n\n    def test_allowed_to_edit(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n\n            fname = Path(\"added.txt\")\n            fname.touch()\n            repo.git.add(str(fname))\n\n            fname = Path(\"repo.txt\")\n            fname.touch()\n            repo.git.add(str(fname))\n\n            repo.git.commit(\"-m\", \"init\")\n\n            # YES!\n            io = InputOutput(yes=True)\n            coder = Coder.create(self.GPT35, None, io, fnames=[\"added.txt\"])\n\n            self.assertTrue(coder.allowed_to_edit(\"added.txt\"))\n            self.assertTrue(coder.allowed_to_edit(\"repo.txt\"))\n            self.assertTrue(coder.allowed_to_edit(\"new.txt\"))\n\n            self.assertIn(\"repo.txt\", str(coder.abs_fnames))\n            self.assertIn(\"new.txt\", str(coder.abs_fnames))\n\n            self.assertFalse(coder.need_commit_before_edits)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::2",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 183,
      "span_ids": [
        "TestCoder.test_allowed_to_edit_no"
      ],
      "start_line": 51,
      "end_line": 77,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_allowed_to_edit_no(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n\n            fname = Path(\"added.txt\")\n            fname.touch()\n            repo.git.add(str(fname))\n\n            fname = Path(\"repo.txt\")\n            fname.touch()\n            repo.git.add(str(fname))\n\n            repo.git.commit(\"-m\", \"init\")\n\n            # say NO\n            io = InputOutput(yes=False)\n\n            coder = Coder.create(self.GPT35, None, io, fnames=[\"added.txt\"])\n\n            self.assertTrue(coder.allowed_to_edit(\"added.txt\"))\n            self.assertFalse(coder.allowed_to_edit(\"repo.txt\"))\n            self.assertFalse(coder.allowed_to_edit(\"new.txt\"))\n\n            self.assertNotIn(\"repo.txt\", str(coder.abs_fnames))\n            self.assertNotIn(\"new.txt\", str(coder.abs_fnames))\n\n            self.assertFalse(coder.need_commit_before_edits)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::3",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 143,
      "span_ids": [
        "TestCoder.test_allowed_to_edit_dirty"
      ],
      "start_line": 79,
      "end_line": 99,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_allowed_to_edit_dirty(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n\n            fname = Path(\"added.txt\")\n            fname.touch()\n            repo.git.add(str(fname))\n\n            repo.git.commit(\"-m\", \"init\")\n\n            # say NO\n            io = InputOutput(yes=False)\n\n            coder = Coder.create(self.GPT35, None, io, fnames=[\"added.txt\"])\n\n            self.assertTrue(coder.allowed_to_edit(\"added.txt\"))\n            self.assertFalse(coder.need_commit_before_edits)\n\n            fname.write_text(\"dirty!\")\n            self.assertTrue(coder.allowed_to_edit(\"added.txt\"))\n            self.assertTrue(coder.need_commit_before_edits)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::4",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 138,
      "span_ids": [
        "TestCoder.test_get_files_content"
      ],
      "start_line": 101,
      "end_line": 117,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_get_files_content(self):\n        tempdir = Path(tempfile.mkdtemp())\n\n        file1 = tempdir / \"file1.txt\"\n        file2 = tempdir / \"file2.txt\"\n\n        file1.touch()\n        file2.touch()\n\n        files = [file1, file2]\n\n        # Initialize the Coder object with the mocked IO and mocked repo\n        coder = Coder.create(self.GPT35, None, io=InputOutput(), fnames=files)\n\n        content = coder.get_files_content().splitlines()\n        self.assertIn(\"file1.txt\", content)\n        self.assertIn(\"file2.txt\", content)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::5",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 211,
      "span_ids": [
        "TestCoder.test_check_for_filename_mentions"
      ],
      "start_line": 119,
      "end_line": 149,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_check_for_filename_mentions(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n\n            mock_io = MagicMock()\n\n            fname1 = Path(\"file1.txt\")\n            fname2 = Path(\"file2.py\")\n\n            fname1.write_text(\"one\\n\")\n            fname2.write_text(\"two\\n\")\n\n            repo.git.add(str(fname1))\n            repo.git.add(str(fname2))\n            repo.git.commit(\"-m\", \"new\")\n\n            # Initialize the Coder object with the mocked IO and mocked repo\n            coder = Coder.create(self.GPT35, None, mock_io)\n\n            # Call the check_for_file_mentions method\n            coder.check_for_file_mentions(\"Please check file1.txt and file2.py\")\n\n            # Check if coder.abs_fnames contains both files\n            expected_files = set(\n                [\n                    str(Path(coder.root) / fname1),\n                    str(Path(coder.root) / fname2),\n                ]\n            )\n\n            self.assertEqual(coder.abs_fnames, expected_files)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::6",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 167,
      "span_ids": [
        "TestCoder.test_check_for_ambiguous_filename_mentions_of_longer_paths"
      ],
      "start_line": 151,
      "end_line": 170,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_check_for_ambiguous_filename_mentions_of_longer_paths(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            fname = Path(\"file1.txt\")\n            fname.touch()\n\n            other_fname = Path(\"other\") / \"file1.txt\"\n            other_fname.parent.mkdir(parents=True, exist_ok=True)\n            other_fname.touch()\n\n            mock = MagicMock()\n            mock.return_value = set([str(fname), str(other_fname)])\n            coder.repo.get_tracked_files = mock\n\n            # Call the check_for_file_mentions method\n            coder.check_for_file_mentions(f\"Please check {fname}!\")\n\n            self.assertEqual(coder.abs_fnames, set([str(fname.resolve())]))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::7",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 292,
      "span_ids": [
        "TestCoder.test_skip_duplicate_basename_mentions"
      ],
      "start_line": 172,
      "end_line": 201,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_skip_duplicate_basename_mentions(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Create files with same basename in different directories\n            fname1 = Path(\"dir1\") / \"file.txt\"\n            fname2 = Path(\"dir2\") / \"file.txt\"\n            fname3 = Path(\"dir3\") / \"unique.txt\"\n\n            for fname in [fname1, fname2, fname3]:\n                fname.parent.mkdir(parents=True, exist_ok=True)\n                fname.touch()\n\n            # Add one file to chat\n            coder.add_rel_fname(str(fname1))\n\n            # Mock get_tracked_files to return all files\n            mock = MagicMock()\n            mock.return_value = set([str(fname1), str(fname2), str(fname3)])\n            coder.repo.get_tracked_files = mock\n\n            # Check that file mentions skip files with duplicate basenames\n            mentioned = coder.get_file_mentions(f\"Check {fname2} and {fname3}\")\n            self.assertEqual(mentioned, {str(fname3)})\n\n            # Add a read-only file with same basename\n            coder.abs_read_only_fnames.add(str(fname2.resolve()))\n            mentioned = coder.get_file_mentions(f\"Check {fname1} and {fname3}\")\n            self.assertEqual(mentioned, {str(fname3)})",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::8",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 188,
      "span_ids": [
        "TestCoder.test_check_for_file_mentions_read_only"
      ],
      "start_line": 203,
      "end_line": 228,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_check_for_file_mentions_read_only(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(\n                pretty=False,\n                yes=True,\n            )\n            coder = Coder.create(self.GPT35, None, io)\n\n            fname = Path(\"readonly_file.txt\")\n            fname.touch()\n\n            coder.abs_read_only_fnames.add(str(fname.resolve()))\n\n            # Mock the get_tracked_files method\n            mock = MagicMock()\n            mock.return_value = set([str(fname)])\n            coder.repo.get_tracked_files = mock\n\n            # Call the check_for_file_mentions method\n            result = coder.check_for_file_mentions(f\"Please check {fname}!\")\n\n            # Assert that the method returns None (user not asked to add the file)\n            self.assertIsNone(result)\n\n            # Assert that abs_fnames is still empty (file not added)\n            self.assertEqual(coder.abs_fnames, set())",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::9",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 337,
      "span_ids": [
        "TestCoder.test_check_for_file_mentions_with_mocked_confirm"
      ],
      "start_line": 230,
      "end_line": 265,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_check_for_file_mentions_with_mocked_confirm(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Mock get_file_mentions to return two file names\n            coder.get_file_mentions = MagicMock(return_value=set([\"file1.txt\", \"file2.txt\"]))\n\n            # Mock confirm_ask to return False for the first call and True for the second\n            io.confirm_ask = MagicMock(side_effect=[False, True, True])\n\n            # First call to check_for_file_mentions\n            coder.check_for_file_mentions(\"Please check file1.txt for the info\")\n\n            # Assert that confirm_ask was called twice\n            self.assertEqual(io.confirm_ask.call_count, 2)\n\n            # Assert that only file2.txt was added to abs_fnames\n            self.assertEqual(len(coder.abs_fnames), 1)\n            self.assertIn(\"file2.txt\", str(coder.abs_fnames))\n\n            # Reset the mock\n            io.confirm_ask.reset_mock()\n\n            # Second call to check_for_file_mentions\n            coder.check_for_file_mentions(\"Please check file1.txt and file2.txt again\")\n\n            # Assert that confirm_ask was called only once (for file1.txt)\n            self.assertEqual(io.confirm_ask.call_count, 1)\n\n            # Assert that abs_fnames still contains only file2.txt\n            self.assertEqual(len(coder.abs_fnames), 1)\n            self.assertIn(\"file2.txt\", str(coder.abs_fnames))\n\n            # Assert that file1.txt is in ignore_mentions\n            self.assertIn(\"file1.txt\", coder.ignore_mentions)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::10",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 143,
      "span_ids": [
        "TestCoder.test_check_for_subdir_mention"
      ],
      "start_line": 267,
      "end_line": 283,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_check_for_subdir_mention(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            fname = Path(\"other\") / \"file1.txt\"\n            fname.parent.mkdir(parents=True, exist_ok=True)\n            fname.touch()\n\n            mock = MagicMock()\n            mock.return_value = set([str(fname)])\n            coder.repo.get_tracked_files = mock\n\n            # Call the check_for_file_mentions method\n            coder.check_for_file_mentions(f\"Please check `{fname}`\")\n\n            self.assertEqual(coder.abs_fnames, set([str(fname.resolve())]))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::11",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 441,
      "span_ids": [
        "TestCoder.test_get_file_mentions_path_formats"
      ],
      "start_line": 285,
      "end_line": 321,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_get_file_mentions_path_formats(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n\n            # Test cases with different path formats\n            test_cases = [\n                # Unix paths in content, Unix paths in get_addable_relative_files\n                (\"Check file1.txt and dir/file2.txt\", [\"file1.txt\", \"dir/file2.txt\"]),\n                # Windows paths in content, Windows paths in get_addable_relative_files\n                (\"Check file1.txt and dir\\\\file2.txt\", [\"file1.txt\", \"dir\\\\file2.txt\"]),\n                # Unix paths in content, Windows paths in get_addable_relative_files\n                (\"Check file1.txt and dir/file2.txt\", [\"file1.txt\", \"dir\\\\file2.txt\"]),\n                # Windows paths in content, Unix paths in get_addable_relative_files\n                (\"Check file1.txt and dir\\\\file2.txt\", [\"file1.txt\", \"dir/file2.txt\"]),\n                # Mixed paths in content, Unix paths in get_addable_relative_files\n                (\n                    \"Check file1.txt, dir/file2.txt, and other\\\\file3.txt\",\n                    [\"file1.txt\", \"dir/file2.txt\", \"other/file3.txt\"],\n                ),\n                # Mixed paths in content, Windows paths in get_addable_relative_files\n                (\n                    \"Check file1.txt, dir/file2.txt, and other\\\\file3.txt\",\n                    [\"file1.txt\", \"dir\\\\file2.txt\", \"other\\\\file3.txt\"],\n                ),\n            ]\n\n            for content, addable_files in test_cases:\n                with self.subTest(content=content, addable_files=addable_files):\n                    coder.get_addable_relative_files = MagicMock(return_value=set(addable_files))\n                    mentioned_files = coder.get_file_mentions(content)\n                    expected_files = set(addable_files)\n                    self.assertEqual(\n                        mentioned_files,\n                        expected_files,\n                        f\"Failed for content: {content}, addable_files: {addable_files}\",\n                    )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::12",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 221,
      "span_ids": [
        "TestCoder.test_run_with_file_deletion"
      ],
      "start_line": 323,
      "end_line": 354,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_run_with_file_deletion(self):\n        # Create a few temporary files\n\n        tempdir = Path(tempfile.mkdtemp())\n\n        file1 = tempdir / \"file1.txt\"\n        file2 = tempdir / \"file2.txt\"\n\n        file1.touch()\n        file2.touch()\n\n        files = [file1, file2]\n\n        # Initialize the Coder object with the mocked IO and mocked repo\n        coder = Coder.create(self.GPT35, None, io=InputOutput(), fnames=files)\n\n        def mock_send(*args, **kwargs):\n            coder.partial_response_content = \"ok\"\n            coder.partial_response_function_call = dict()\n            return []\n\n        coder.send = mock_send\n\n        # Call the run method with a message\n        coder.run(with_message=\"hi\")\n        self.assertEqual(len(coder.abs_fnames), 2)\n\n        file1.unlink()\n\n        # Call the run method again with a message\n        coder.run(with_message=\"hi\")\n        self.assertEqual(len(coder.abs_fnames), 1)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::13",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 225,
      "span_ids": [
        "TestCoder.test_run_with_file_unicode_error"
      ],
      "start_line": 356,
      "end_line": 383,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_run_with_file_unicode_error(self):\n        # Create a few temporary files\n        _, file1 = tempfile.mkstemp()\n        _, file2 = tempfile.mkstemp()\n\n        files = [file1, file2]\n\n        # Initialize the Coder object with the mocked IO and mocked repo\n        coder = Coder.create(self.GPT35, None, io=InputOutput(), fnames=files)\n\n        def mock_send(*args, **kwargs):\n            coder.partial_response_content = \"ok\"\n            coder.partial_response_function_call = dict()\n            return []\n\n        coder.send = mock_send\n\n        # Call the run method with a message\n        coder.run(with_message=\"hi\")\n        self.assertEqual(len(coder.abs_fnames), 2)\n\n        # Write some non-UTF8 text into the file\n        with open(file1, \"wb\") as f:\n            f.write(b\"\\x80abc\")\n\n        # Call the run method again with a message\n        coder.run(with_message=\"hi\")\n        self.assertEqual(len(coder.abs_fnames), 1)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::14",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 171,
      "span_ids": [
        "TestCoder.test_choose_fence"
      ],
      "start_line": 385,
      "end_line": 407,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_choose_fence(self):\n        # Create a few temporary files\n        _, file1 = tempfile.mkstemp()\n\n        with open(file1, \"wb\") as f:\n            f.write(b\"this contains\\n```\\nbackticks\")\n\n        files = [file1]\n\n        # Initialize the Coder object with the mocked IO and mocked repo\n        coder = Coder.create(self.GPT35, None, io=InputOutput(), fnames=files)\n\n        def mock_send(*args, **kwargs):\n            coder.partial_response_content = \"ok\"\n            coder.partial_response_function_call = dict()\n            return []\n\n        coder.send = mock_send\n\n        # Call the run method with a message\n        coder.run(with_message=\"hi\")\n\n        self.assertNotEqual(coder.fence[0], \"```\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::15",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 284,
      "span_ids": [
        "TestCoder.test_run_with_file_utf_unicode_error"
      ],
      "start_line": 409,
      "end_line": 445,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_run_with_file_utf_unicode_error(self):\n        \"make sure that we honor InputOutput(encoding) and don't just assume utf-8\"\n        # Create a few temporary files\n        _, file1 = tempfile.mkstemp()\n        _, file2 = tempfile.mkstemp()\n\n        files = [file1, file2]\n\n        encoding = \"utf-16\"\n\n        # Initialize the Coder object with the mocked IO and mocked repo\n        coder = Coder.create(\n            self.GPT35,\n            None,\n            io=InputOutput(encoding=encoding),\n            fnames=files,\n        )\n\n        def mock_send(*args, **kwargs):\n            coder.partial_response_content = \"ok\"\n            coder.partial_response_function_call = dict()\n            return []\n\n        coder.send = mock_send\n\n        # Call the run method with a message\n        coder.run(with_message=\"hi\")\n        self.assertEqual(len(coder.abs_fnames), 2)\n\n        some_content_which_will_error_if_read_with_encoding_utf8 = \"\u00c5\u00cd\u00ce\u00cf\".encode(encoding)\n        with open(file1, \"wb\") as f:\n            f.write(some_content_which_will_error_if_read_with_encoding_utf8)\n\n        coder.run(with_message=\"hi\")\n\n        # both files should still be here\n        self.assertEqual(len(coder.abs_fnames), 2)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::16",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 252,
      "span_ids": [
        "TestCoder.test_new_file_edit_one_commit"
      ],
      "start_line": 447,
      "end_line": 487,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_new_file_edit_one_commit(self):\n        \"\"\"A new file should get pre-committed before the GPT edit commit\"\"\"\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n\n            fname = Path(\"file.txt\")\n\n            io = InputOutput(yes=True)\n            coder = Coder.create(self.GPT35, \"diff\", io=io, fnames=[str(fname)])\n\n            self.assertTrue(fname.exists())\n\n            # make sure it was not committed\n            with self.assertRaises(git.exc.GitCommandError):\n                list(repo.iter_commits(repo.active_branch.name))\n\n            def mock_send(*args, **kwargs):\n                coder.partial_response_content = f\"\"\"\nDo this:\n\n{str(fname)}\n<<<<<<< SEARCH\n=======\nnew\n>>>>>>> REPLACE\n\n\"\"\"\n                coder.partial_response_function_call = dict()\n                return []\n\n            coder.send = mock_send\n            coder.repo.get_commit_message = MagicMock()\n            coder.repo.get_commit_message.return_value = \"commit message\"\n\n            coder.run(with_message=\"hi\")\n\n            content = fname.read_text()\n            self.assertEqual(content, \"new\\n\")\n\n            num_commits = len(list(repo.iter_commits(repo.active_branch.name)))\n            self.assertEqual(num_commits, 2)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::17",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 328,
      "span_ids": [
        "TestCoder.test_only_commit_gpt_edited_file"
      ],
      "start_line": 489,
      "end_line": 542,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_only_commit_gpt_edited_file(self):\n        \"\"\"\n        Only commit file that gpt edits, not other dirty files.\n        Also ensure commit msg only depends on diffs from the GPT edited file.\n        \"\"\"\n\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n\n            fname1 = Path(\"file1.txt\")\n            fname2 = Path(\"file2.txt\")\n\n            fname1.write_text(\"one\\n\")\n            fname2.write_text(\"two\\n\")\n\n            repo.git.add(str(fname1))\n            repo.git.add(str(fname2))\n            repo.git.commit(\"-m\", \"new\")\n\n            # DIRTY!\n            fname1.write_text(\"ONE\\n\")\n\n            io = InputOutput(yes=True)\n            coder = Coder.create(self.GPT35, \"diff\", io=io, fnames=[str(fname1), str(fname2)])\n\n            def mock_send(*args, **kwargs):\n                coder.partial_response_content = f\"\"\"\nDo this:\n\n{str(fname2)}\n<<<<<<< SEARCH\ntwo\n=======\nTWO\n>>>>>>> REPLACE\n\n\"\"\"\n                coder.partial_response_function_call = dict()\n                return []\n\n            def mock_get_commit_message(diffs, context):\n                self.assertNotIn(\"one\", diffs)\n                self.assertNotIn(\"ONE\", diffs)\n                return \"commit message\"\n\n            coder.send = mock_send\n            coder.repo.get_commit_message = MagicMock(side_effect=mock_get_commit_message)\n\n            coder.run(with_message=\"hi\")\n\n            content = fname2.read_text()\n            self.assertEqual(content, \"TWO\\n\")\n\n            self.assertTrue(repo.is_dirty(path=str(fname1)))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::18",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 544,
      "span_ids": [
        "TestCoder.test_gpt_edit_to_dirty_file"
      ],
      "start_line": 544,
      "end_line": 627,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_gpt_edit_to_dirty_file(self):\n        \"\"\"A dirty file should be committed before the GPT edits are committed\"\"\"\n\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n\n            fname = Path(\"file.txt\")\n            fname.write_text(\"one\\n\")\n            repo.git.add(str(fname))\n\n            fname2 = Path(\"other.txt\")\n            fname2.write_text(\"other\\n\")\n            repo.git.add(str(fname2))\n\n            repo.git.commit(\"-m\", \"new\")\n\n            # dirty\n            fname.write_text(\"two\\n\")\n            fname2.write_text(\"OTHER\\n\")\n\n            io = InputOutput(yes=True)\n            coder = Coder.create(self.GPT35, \"diff\", io=io, fnames=[str(fname)])\n\n            def mock_send(*args, **kwargs):\n                coder.partial_response_content = f\"\"\"\nDo this:\n\n{str(fname)}\n<<<<<<< SEARCH\ntwo\n=======\nthree\n>>>>>>> REPLACE\n\n\"\"\"\n                coder.partial_response_function_call = dict()\n                return []\n\n            saved_diffs = []\n\n            def mock_get_commit_message(diffs, context):\n                saved_diffs.append(diffs)\n                return \"commit message\"\n\n            coder.repo.get_commit_message = MagicMock(side_effect=mock_get_commit_message)\n            coder.send = mock_send\n\n            coder.run(with_message=\"hi\")\n\n            content = fname.read_text()\n            self.assertEqual(content, \"three\\n\")\n\n            num_commits = len(list(repo.iter_commits(repo.active_branch.name)))\n            self.assertEqual(num_commits, 3)\n\n            diff = repo.git.diff([\"HEAD~2\", \"HEAD~1\"])\n            self.assertIn(\"one\", diff)\n            self.assertIn(\"two\", diff)\n            self.assertNotIn(\"three\", diff)\n            self.assertNotIn(\"other\", diff)\n            self.assertNotIn(\"OTHER\", diff)\n\n            diff = saved_diffs[0]\n            self.assertIn(\"one\", diff)\n            self.assertIn(\"two\", diff)\n            self.assertNotIn(\"three\", diff)\n            self.assertNotIn(\"other\", diff)\n            self.assertNotIn(\"OTHER\", diff)\n\n            diff = repo.git.diff([\"HEAD~1\", \"HEAD\"])\n            self.assertNotIn(\"one\", diff)\n            self.assertIn(\"two\", diff)\n            self.assertIn(\"three\", diff)\n            self.assertNotIn(\"other\", diff)\n            self.assertNotIn(\"OTHER\", diff)\n\n            diff = saved_diffs[1]\n            self.assertNotIn(\"one\", diff)\n            self.assertIn(\"two\", diff)\n            self.assertIn(\"three\", diff)\n            self.assertNotIn(\"other\", diff)\n            self.assertNotIn(\"OTHER\", diff)\n\n            self.assertEqual(len(saved_diffs), 2)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::19",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 264,
      "span_ids": [
        "TestCoder.test_gpt_edit_to_existing_file_not_in_repo"
      ],
      "start_line": 629,
      "end_line": 675,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_gpt_edit_to_existing_file_not_in_repo(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n\n            fname = Path(\"file.txt\")\n            fname.write_text(\"one\\n\")\n\n            fname2 = Path(\"other.txt\")\n            fname2.write_text(\"other\\n\")\n            repo.git.add(str(fname2))\n\n            repo.git.commit(\"-m\", \"initial\")\n\n            io = InputOutput(yes=True)\n            coder = Coder.create(self.GPT35, \"diff\", io=io, fnames=[str(fname)])\n\n            def mock_send(*args, **kwargs):\n                coder.partial_response_content = f\"\"\"\nDo this:\n\n{str(fname)}\n<<<<<<< SEARCH\none\n=======\ntwo\n>>>>>>> REPLACE\n\n\"\"\"\n                coder.partial_response_function_call = dict()\n                return []\n\n            saved_diffs = []\n\n            def mock_get_commit_message(diffs, context):\n                saved_diffs.append(diffs)\n                return \"commit message\"\n\n            coder.repo.get_commit_message = MagicMock(side_effect=mock_get_commit_message)\n            coder.send = mock_send\n\n            coder.run(with_message=\"hi\")\n\n            content = fname.read_text()\n            self.assertEqual(content, \"two\\n\")\n\n            diff = saved_diffs[0]\n            self.assertIn(\"file.txt\", diff)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::20",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 236,
      "span_ids": [
        "TestCoder.test_skip_aiderignored_files"
      ],
      "start_line": 677,
      "end_line": 712,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_skip_aiderignored_files(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n\n            fname1 = \"ignoreme1.txt\"\n            fname2 = \"ignoreme2.txt\"\n            fname3 = \"dir/ignoreme3.txt\"\n\n            Path(fname2).touch()\n            repo.git.add(str(fname2))\n            repo.git.commit(\"-m\", \"initial\")\n\n            io = InputOutput(yes=True)\n\n            fnames = [fname1, fname2, fname3]\n\n            aignore = Path(\".aiderignore\")\n            aignore.write_text(f\"{fname1}\\n{fname2}\\ndir\\n\")\n            repo = GitRepo(\n                io,\n                fnames,\n                None,\n                aider_ignore_file=str(aignore),\n            )\n\n            coder = Coder.create(\n                self.GPT35,\n                None,\n                io,\n                fnames=fnames,\n                repo=repo,\n            )\n\n            self.assertNotIn(fname1, str(coder.abs_fnames))\n            self.assertNotIn(fname2, str(coder.abs_fnames))\n            self.assertNotIn(fname3, str(coder.abs_fnames))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::21",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 712,
      "span_ids": [
        "TestCoder.test_check_for_urls"
      ],
      "start_line": 714,
      "end_line": 784,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_check_for_urls(self):\n        io = InputOutput(yes=True)\n        coder = Coder.create(self.GPT35, None, io=io)\n        coder.commands.scraper = MagicMock()\n        coder.commands.scraper.scrape = MagicMock(return_value=\"some content\")\n\n        # Test various URL formats\n        test_cases = [\n            (\"Check http://example.com, it's cool\", \"http://example.com\"),\n            (\"Visit https://www.example.com/page and see stuff\", \"https://www.example.com/page\"),\n            (\n                \"Go to http://subdomain.example.com:8080/path?query=value, or not\",\n                \"http://subdomain.example.com:8080/path?query=value\",\n            ),\n            (\n                \"See https://example.com/path#fragment for example\",\n                \"https://example.com/path#fragment\",\n            ),\n            (\"Look at http://localhost:3000\", \"http://localhost:3000\"),\n            (\"View https://example.com/setup#whatever\", \"https://example.com/setup#whatever\"),\n            (\"Open http://127.0.0.1:8000/api/v1/\", \"http://127.0.0.1:8000/api/v1/\"),\n            (\n                \"Try https://example.com/path/to/page.html?param1=value1&param2=value2\",\n                \"https://example.com/path/to/page.html?param1=value1&param2=value2\",\n            ),\n            (\"Access http://user:password@example.com\", \"http://user:password@example.com\"),\n            (\n                \"Use https://example.com/path_(with_parentheses)\",\n                \"https://example.com/path_(with_parentheses)\",\n            ),\n        ]\n\n        for input_text, expected_url in test_cases:\n            with self.subTest(input_text=input_text):\n                result = coder.check_for_urls(input_text)\n                self.assertIn(expected_url, result)\n\n        # Test cases from the GitHub issue\n        issue_cases = [\n            (\"check http://localhost:3002, there is an error\", \"http://localhost:3002\"),\n            (\n                \"can you check out https://example.com/setup#whatever\",\n                \"https://example.com/setup#whatever\",\n            ),\n        ]\n\n        for input_text, expected_url in issue_cases:\n            with self.subTest(input_text=input_text):\n                result = coder.check_for_urls(input_text)\n                self.assertIn(expected_url, result)\n\n        # Test case with multiple URLs\n        multi_url_input = \"Check http://example1.com and https://example2.com/page\"\n        result = coder.check_for_urls(multi_url_input)\n        self.assertIn(\"http://example1.com\", result)\n        self.assertIn(\"https://example2.com/page\", result)\n\n        # Test case with no URL\n        no_url_input = \"This text contains no URL\"\n        result = coder.check_for_urls(no_url_input)\n        self.assertEqual(result, no_url_input)\n\n        # Test case with the same URL appearing multiple times\n        repeated_url_input = (\n            \"Check https://example.com, then https://example.com again, and https://example.com one\"\n            \" more time\"\n        )\n        result = coder.check_for_urls(repeated_url_input)\n        # the original 3 in the input text, plus 1 more for the scraped text\n        self.assertEqual(result.count(\"https://example.com\"), 4)\n        self.assertIn(\"https://example.com\", result)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::22",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 330,
      "span_ids": [
        "TestCoder.test_coder_from_coder_with_subdir"
      ],
      "start_line": 786,
      "end_line": 820,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_coder_from_coder_with_subdir(self):\n        with GitTemporaryDirectory() as root:\n            repo = git.Repo.init(root)\n\n            # Create a file in a subdirectory\n            subdir = Path(root) / \"subdir\"\n            subdir.mkdir()\n            test_file = subdir / \"test_file.txt\"\n            test_file.write_text(\"Test content\")\n\n            repo.git.add(str(test_file))\n            repo.git.commit(\"-m\", \"Add test file\")\n\n            # Change directory to the subdirectory\n            os.chdir(subdir.resolve())\n\n            # Create the first coder\n            io = InputOutput(yes=True)\n            coder1 = Coder.create(self.GPT35, None, io=io, fnames=[test_file.name])\n\n            # Create a new coder from the first coder\n            coder2 = Coder.create(from_coder=coder1)\n\n            # Check if both coders have the same set of abs_fnames\n            self.assertEqual(coder1.abs_fnames, coder2.abs_fnames)\n\n            # Ensure the abs_fnames contain the correct absolute path\n            expected_abs_path = os.path.realpath(str(test_file))\n            coder1_abs_fnames = set(os.path.realpath(path) for path in coder1.abs_fnames)\n            self.assertIn(expected_abs_path, coder1_abs_fnames)\n            self.assertIn(expected_abs_path, coder2.abs_fnames)\n\n            # Check that the abs_fnames do not contain duplicate or incorrect paths\n            self.assertEqual(len(coder1.abs_fnames), 1)\n            self.assertEqual(len(coder2.abs_fnames), 1)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::23",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 227,
      "span_ids": [
        "TestCoder.test_suggest_shell_commands"
      ],
      "start_line": 822,
      "end_line": 851,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_suggest_shell_commands(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            coder = Coder.create(self.GPT35, \"diff\", io=io)\n\n            def mock_send(*args, **kwargs):\n                coder.partial_response_content = \"\"\"Here's a shell command to run:\n\n```bash\necho \"Hello, World!\"\n```\n\nThis command will print 'Hello, World!' to the console.\"\"\"\n                coder.partial_response_function_call = dict()\n                return []\n\n            coder.send = mock_send\n\n            # Mock the handle_shell_commands method to check if it's called\n            coder.handle_shell_commands = MagicMock()\n\n            # Run the coder with a message\n            coder.run(with_message=\"Suggest a shell command\")\n\n            # Check if the shell command was added to the list\n            self.assertEqual(len(coder.shell_commands), 1)\n            self.assertEqual(coder.shell_commands[0].strip(), 'echo \"Hello, World!\"')\n\n            # Check if handle_shell_commands was called with the correct argument\n            coder.handle_shell_commands.assert_called_once()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::24",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 176,
      "span_ids": [
        "TestCoder.test_detect_urls_enabled",
        "TestCoder.test_no_suggest_shell_commands"
      ],
      "start_line": 853,
      "end_line": 869,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_no_suggest_shell_commands(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            coder = Coder.create(self.GPT35, \"diff\", io=io, suggest_shell_commands=False)\n            self.assertFalse(coder.suggest_shell_commands)\n\n    def test_detect_urls_enabled(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            coder = Coder.create(self.GPT35, \"diff\", io=io, detect_urls=True)\n            coder.commands.scraper = MagicMock()\n            coder.commands.scraper.scrape = MagicMock(return_value=\"some content\")\n\n            # Test with a message containing a URL\n            message = \"Check out https://example.com\"\n            coder.check_for_urls(message)\n            coder.commands.scraper.scrape.assert_called_once_with(\"https://example.com\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::25",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 122,
      "span_ids": [
        "TestCoder.test_detect_urls_disabled"
      ],
      "start_line": 871,
      "end_line": 882,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_detect_urls_disabled(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            coder = Coder.create(self.GPT35, \"diff\", io=io, detect_urls=False)\n            coder.commands.scraper = MagicMock()\n            coder.commands.scraper.scrape = MagicMock(return_value=\"some content\")\n\n            # Test with a message containing a URL\n            message = \"Check out https://example.com\"\n            result = coder.check_for_urls(message)\n            self.assertEqual(result, message)\n            coder.commands.scraper.scrape.assert_not_called()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::26",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 201,
      "span_ids": [
        "TestCoder.test_unknown_edit_format_creation",
        "TestCoder.test_unknown_edit_format_exception"
      ],
      "start_line": 884,
      "end_line": 905,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_unknown_edit_format_exception(self):\n        # Test the exception message format\n        invalid_format = \"invalid_format\"\n        valid_formats = [\"diff\", \"whole\", \"map\"]\n        exc = UnknownEditFormat(invalid_format, valid_formats)\n        expected_msg = (\n            f\"Unknown edit format {invalid_format}. Valid formats are: {', '.join(valid_formats)}\"\n        )\n        self.assertEqual(str(exc), expected_msg)\n\n    def test_unknown_edit_format_creation(self):\n        # Test that creating a Coder with invalid edit format raises the exception\n        io = InputOutput(yes=True)\n        invalid_format = \"invalid_format\"\n\n        with self.assertRaises(UnknownEditFormat) as cm:\n            Coder.create(self.GPT35, invalid_format, io=io)\n\n        exc = cm.exception\n        self.assertEqual(exc.edit_format, invalid_format)\n        self.assertIsInstance(exc.valid_formats, list)\n        self.assertTrue(len(exc.valid_formats) > 0)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::27",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 159,
      "span_ids": [
        "TestCoder.test_coder_create_with_new_file_oserror"
      ],
      "start_line": 907,
      "end_line": 921,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_coder_create_with_new_file_oserror(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            new_file = \"new_file.txt\"\n\n            # Mock Path.touch() to raise OSError\n            with patch(\"pathlib.Path.touch\", side_effect=OSError(\"Permission denied\")):\n                # Create the coder with a new file\n                coder = Coder.create(self.GPT35, \"diff\", io=io, fnames=[new_file])\n\n            # Check if the coder was created successfully\n            self.assertIsInstance(coder, Coder)\n\n            # Check if the new file is not in abs_fnames\n            self.assertNotIn(new_file, [os.path.basename(f) for f in coder.abs_fnames])",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_coder.py::28",
    "metadata": {
      "file_path": "tests/basic/test_coder.py",
      "file_name": "test_coder.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 434,
      "span_ids": [
        "impl",
        "TestCoder.test_show_exhausted_error"
      ],
      "start_line": 923,
      "end_line": 980,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCoder(unittest.TestCase):\n\n    def test_show_exhausted_error(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(yes=True)\n            coder = Coder.create(self.GPT35, \"diff\", io=io)\n\n            # Set up some real done_messages and cur_messages\n            coder.done_messages = [\n                {\"role\": \"user\", \"content\": \"Hello, can you help me with a Python problem?\"},\n                {\n                    \"role\": \"assistant\",\n                    \"content\": \"Of course! I'd be happy to help. What's the problem you're facing?\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": (\n                        \"I need to write a function that calculates the factorial of a number.\"\n                    ),\n                },\n                {\n                    \"role\": \"assistant\",\n                    \"content\": (\n                        \"Sure, I can help you with that. Here's a simple Python function to\"\n                        \" calculate the factorial of a number:\"\n                    ),\n                },\n            ]\n\n            coder.cur_messages = [\n                {\"role\": \"user\", \"content\": \"Can you optimize this function for large numbers?\"},\n            ]\n\n            # Set up real values for the main model\n            coder.main_model.info = {\n                \"max_input_tokens\": 4000,\n                \"max_output_tokens\": 1000,\n            }\n            coder.partial_response_content = (\n                \"Here's an optimized version of the factorial function:\"\n            )\n            coder.io.tool_error = MagicMock()\n\n            # Call the method\n            coder.show_exhausted_error()\n\n            # Check if tool_error was called with the expected message\n            coder.io.tool_error.assert_called()\n            error_message = coder.io.tool_error.call_args[0][0]\n\n            # Assert that the error message contains the expected information\n            self.assertIn(\"Model gpt-3.5-turbo has hit a token limit!\", error_message)\n            self.assertIn(\"Input tokens:\", error_message)\n            self.assertIn(\"Output tokens:\", error_message)\n            self.assertIn(\"Total tokens:\", error_message)\n\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::1",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 317,
      "span_ids": [
        "TestCommands.setUp",
        "TestCommands.tearDown",
        "TestCommands.test_cmd_add",
        "TestCommands",
        "imports"
      ],
      "start_line": 1,
      "end_line": 48,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import codecs\nimport os\nimport re\nimport shutil\nimport sys\nimport tempfile\nfrom io import StringIO\nfrom pathlib import Path\nfrom unittest import TestCase, mock\n\nimport git\nimport pyperclip\n\nfrom aider.coders import Coder\nfrom aider.commands import Commands, SwitchCoder\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import InputOutput\nfrom aider.models import Model\nfrom aider.repo import GitRepo\nfrom aider.utils import ChdirTemporaryDirectory, GitTemporaryDirectory, make_repo\n\n\nclass TestCommands(TestCase):\n    def setUp(self):\n        self.original_cwd = os.getcwd()\n        self.tempdir = tempfile.mkdtemp()\n        os.chdir(self.tempdir)\n\n        self.GPT35 = Model(\"gpt-3.5-turbo\")\n\n    def tearDown(self):\n        os.chdir(self.original_cwd)\n        shutil.rmtree(self.tempdir, ignore_errors=True)\n\n    def test_cmd_add(self):\n        # Initialize the Commands and InputOutput objects\n        io = InputOutput(pretty=False, fancy_input=False, yes=True)\n        from aider.coders import Coder\n\n        coder = Coder.create(self.GPT35, None, io)\n        commands = Commands(io, coder)\n\n        # Call the cmd_add method with 'foo.txt' and 'bar.txt' as a single string\n        commands.cmd_add(\"foo.txt bar.txt\")\n\n        # Check if both files have been created in the temporary directory\n        self.assertTrue(os.path.exists(\"foo.txt\"))\n        self.assertTrue(os.path.exists(\"bar.txt\"))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::2",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 257,
      "span_ids": [
        "TestCommands.test_cmd_copy"
      ],
      "start_line": 50,
      "end_line": 78,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_copy(self):\n        # Initialize InputOutput and Coder instances\n        io = InputOutput(pretty=False, fancy_input=False, yes=True)\n        coder = Coder.create(self.GPT35, None, io)\n        commands = Commands(io, coder)\n\n        # Add some assistant messages to the chat history\n        coder.done_messages = [\n            {\"role\": \"assistant\", \"content\": \"First assistant message\"},\n            {\"role\": \"user\", \"content\": \"User message\"},\n            {\"role\": \"assistant\", \"content\": \"Second assistant message\"},\n        ]\n\n        # Mock pyperclip.copy and io.tool_output\n        with (\n            mock.patch(\"pyperclip.copy\") as mock_copy,\n            mock.patch.object(io, \"tool_output\") as mock_tool_output,\n        ):\n            # Invoke the /copy command\n            commands.cmd_copy(\"\")\n\n            # Assert pyperclip.copy was called with the last assistant message\n            mock_copy.assert_called_once_with(\"Second assistant message\")\n\n            # Assert that tool_output was called with the expected preview\n            expected_preview = (\n                \"Copied last assistant message to clipboard. Preview: Second assistant message\"\n            )\n            mock_tool_output.assert_any_call(expected_preview)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::3",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 413,
      "span_ids": [
        "TestCommands.test_cmd_copy_with_cur_messages"
      ],
      "start_line": 80,
      "end_line": 125,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_copy_with_cur_messages(self):\n        # Initialize InputOutput and Coder instances\n        io = InputOutput(pretty=False, fancy_input=False, yes=True)\n        coder = Coder.create(self.GPT35, None, io)\n        commands = Commands(io, coder)\n\n        # Add messages to done_messages and cur_messages\n        coder.done_messages = [\n            {\"role\": \"assistant\", \"content\": \"First assistant message in done_messages\"},\n            {\"role\": \"user\", \"content\": \"User message in done_messages\"},\n        ]\n        coder.cur_messages = [\n            {\"role\": \"assistant\", \"content\": \"Latest assistant message in cur_messages\"},\n        ]\n\n        # Mock pyperclip.copy and io.tool_output\n        with (\n            mock.patch(\"pyperclip.copy\") as mock_copy,\n            mock.patch.object(io, \"tool_output\") as mock_tool_output,\n        ):\n            # Invoke the /copy command\n            commands.cmd_copy(\"\")\n\n            # Assert pyperclip.copy was called with the last assistant message in cur_messages\n            mock_copy.assert_called_once_with(\"Latest assistant message in cur_messages\")\n\n            # Assert that tool_output was called with the expected preview\n            expected_preview = (\n                \"Copied last assistant message to clipboard. Preview: Latest assistant message in\"\n                \" cur_messages\"\n            )\n            mock_tool_output.assert_any_call(expected_preview)\n        io = InputOutput(pretty=False, fancy_input=False, yes=True)\n        coder = Coder.create(self.GPT35, None, io)\n        commands = Commands(io, coder)\n\n        # Add only user messages\n        coder.done_messages = [\n            {\"role\": \"user\", \"content\": \"User message\"},\n        ]\n\n        # Mock io.tool_error\n        with mock.patch.object(io, \"tool_error\") as mock_tool_error:\n            commands.cmd_copy(\"\")\n            # Assert tool_error was called indicating no assistant messages\n            mock_tool_error.assert_called_once_with(\"No assistant messages found to copy.\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::4",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 255,
      "span_ids": [
        "TestCommands.test_cmd_add_bad_glob",
        "TestCommands.test_cmd_copy_pyperclip_exception"
      ],
      "start_line": 127,
      "end_line": 157,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_copy_pyperclip_exception(self):\n        io = InputOutput(pretty=False, fancy_input=False, yes=True)\n        coder = Coder.create(self.GPT35, None, io)\n        commands = Commands(io, coder)\n\n        coder.done_messages = [\n            {\"role\": \"assistant\", \"content\": \"Assistant message\"},\n        ]\n\n        # Mock pyperclip.copy to raise an exception\n        with (\n            mock.patch(\n                \"pyperclip.copy\", side_effect=pyperclip.PyperclipException(\"Clipboard error\")\n            ),\n            mock.patch.object(io, \"tool_error\") as mock_tool_error,\n        ):\n            commands.cmd_copy(\"\")\n\n            # Assert that tool_error was called with the clipboard error message\n            mock_tool_error.assert_called_once_with(\"Failed to copy to clipboard: Clipboard error\")\n\n    def test_cmd_add_bad_glob(self):\n        # https://github.com/Aider-AI/aider/issues/293\n\n        io = InputOutput(pretty=False, fancy_input=False, yes=False)\n        from aider.coders import Coder\n\n        coder = Coder.create(self.GPT35, None, io)\n        commands = Commands(io, coder)\n\n        commands.cmd_add(\"**.txt\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::5",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 247,
      "span_ids": [
        "TestCommands.test_cmd_add_with_glob_patterns"
      ],
      "start_line": 159,
      "end_line": 183,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_with_glob_patterns(self):\n        # Initialize the Commands and InputOutput objects\n        io = InputOutput(pretty=False, fancy_input=False, yes=True)\n        from aider.coders import Coder\n\n        coder = Coder.create(self.GPT35, None, io)\n        commands = Commands(io, coder)\n\n        # Create some test files\n        with open(\"test1.py\", \"w\") as f:\n            f.write(\"print('test1')\")\n        with open(\"test2.py\", \"w\") as f:\n            f.write(\"print('test2')\")\n        with open(\"test.txt\", \"w\") as f:\n            f.write(\"test\")\n\n        # Call the cmd_add method with a glob pattern\n        commands.cmd_add(\"*.py\")\n\n        # Check if the Python files have been added to the chat session\n        self.assertIn(str(Path(\"test1.py\").resolve()), coder.abs_fnames)\n        self.assertIn(str(Path(\"test2.py\").resolve()), coder.abs_fnames)\n\n        # Check if the text file has not been added to the chat session\n        self.assertNotIn(str(Path(\"test.txt\").resolve()), coder.abs_fnames)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::6",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 130,
      "span_ids": [
        "TestCommands.test_cmd_add_no_match"
      ],
      "start_line": 185,
      "end_line": 197,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_no_match(self):\n        # yes=False means we will *not* create the file when it is not found\n        io = InputOutput(pretty=False, fancy_input=False, yes=False)\n        from aider.coders import Coder\n\n        coder = Coder.create(self.GPT35, None, io)\n        commands = Commands(io, coder)\n\n        # Call the cmd_add method with a non-existent file pattern\n        commands.cmd_add(\"*.nonexistent\")\n\n        # Check if no files have been added to the chat session\n        self.assertEqual(len(coder.abs_fnames), 0)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::7",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 146,
      "span_ids": [
        "TestCommands.test_cmd_add_no_match_but_make_it"
      ],
      "start_line": 199,
      "end_line": 214,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_no_match_but_make_it(self):\n        # yes=True means we *will* create the file when it is not found\n        io = InputOutput(pretty=False, fancy_input=False, yes=True)\n        from aider.coders import Coder\n\n        coder = Coder.create(self.GPT35, None, io)\n        commands = Commands(io, coder)\n\n        fname = Path(\"[abc].nonexistent\")\n\n        # Call the cmd_add method with a non-existent file pattern\n        commands.cmd_add(str(fname))\n\n        # Check if no files have been added to the chat session\n        self.assertEqual(len(coder.abs_fnames), 1)\n        self.assertTrue(fname.exists())",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::8",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 522,
      "span_ids": [
        "TestCommands.test_cmd_add_drop_directory"
      ],
      "start_line": 216,
      "end_line": 265,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_drop_directory(self):\n        # Initialize the Commands and InputOutput objects\n        io = InputOutput(pretty=False, fancy_input=False, yes=False)\n        from aider.coders import Coder\n\n        coder = Coder.create(self.GPT35, None, io)\n        commands = Commands(io, coder)\n\n        # Create a directory and add files to it using pathlib\n        Path(\"test_dir\").mkdir()\n        Path(\"test_dir/another_dir\").mkdir()\n        Path(\"test_dir/test_file1.txt\").write_text(\"Test file 1\")\n        Path(\"test_dir/test_file2.txt\").write_text(\"Test file 2\")\n        Path(\"test_dir/another_dir/test_file.txt\").write_text(\"Test file 3\")\n\n        # Call the cmd_add method with a directory\n        commands.cmd_add(\"test_dir test_dir/test_file2.txt\")\n\n        # Check if the files have been added to the chat session\n        self.assertIn(str(Path(\"test_dir/test_file1.txt\").resolve()), coder.abs_fnames)\n        self.assertIn(str(Path(\"test_dir/test_file2.txt\").resolve()), coder.abs_fnames)\n        self.assertIn(str(Path(\"test_dir/another_dir/test_file.txt\").resolve()), coder.abs_fnames)\n\n        commands.cmd_drop(str(Path(\"test_dir/another_dir\")))\n        self.assertIn(str(Path(\"test_dir/test_file1.txt\").resolve()), coder.abs_fnames)\n        self.assertIn(str(Path(\"test_dir/test_file2.txt\").resolve()), coder.abs_fnames)\n        self.assertNotIn(\n            str(Path(\"test_dir/another_dir/test_file.txt\").resolve()), coder.abs_fnames\n        )\n\n        # Issue #139 /add problems when cwd != git_root\n\n        # remember the proper abs path to this file\n        abs_fname = str(Path(\"test_dir/another_dir/test_file.txt\").resolve())\n\n        # chdir to someplace other than git_root\n        Path(\"side_dir\").mkdir()\n        os.chdir(\"side_dir\")\n\n        # add it via it's git_root referenced name\n        commands.cmd_add(\"test_dir/another_dir/test_file.txt\")\n\n        # it should be there, but was not in v0.10.0\n        self.assertIn(abs_fname, coder.abs_fnames)\n\n        # drop it via it's git_root referenced name\n        commands.cmd_drop(\"test_dir/another_dir/test_file.txt\")\n\n        # it should be there, but was not in v0.10.0\n        self.assertNotIn(abs_fname, coder.abs_fnames)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::9",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 264,
      "span_ids": [
        "TestCommands.test_cmd_drop_with_glob_patterns"
      ],
      "start_line": 267,
      "end_line": 294,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_drop_with_glob_patterns(self):\n        # Initialize the Commands and InputOutput objects\n        io = InputOutput(pretty=False, fancy_input=False, yes=True)\n        from aider.coders import Coder\n\n        coder = Coder.create(self.GPT35, None, io)\n        commands = Commands(io, coder)\n\n        # Create test files in root and subdirectory\n        subdir = Path(\"subdir\")\n        subdir.mkdir()\n        (subdir / \"subtest1.py\").touch()\n        (subdir / \"subtest2.py\").touch()\n\n        Path(\"test1.py\").touch()\n        Path(\"test2.py\").touch()\n        Path(\"test3.txt\").touch()\n\n        # Add all Python files to the chat session\n        commands.cmd_add(\"*.py\")\n        initial_count = len(coder.abs_fnames)\n        self.assertEqual(initial_count, 2)  # Only root .py files should be added\n\n        # Test dropping with glob pattern\n        commands.cmd_drop(\"*2.py\")\n        self.assertIn(str(Path(\"test1.py\").resolve()), coder.abs_fnames)\n        self.assertNotIn(str(Path(\"test2.py\").resolve()), coder.abs_fnames)\n        self.assertEqual(len(coder.abs_fnames), initial_count - 1)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::10",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 289,
      "span_ids": [
        "TestCommands.test_cmd_drop_without_glob"
      ],
      "start_line": 296,
      "end_line": 326,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_drop_without_glob(self):\n        # Initialize the Commands and InputOutput objects\n        io = InputOutput(pretty=False, fancy_input=False, yes=True)\n        from aider.coders import Coder\n\n        coder = Coder.create(self.GPT35, None, io)\n        commands = Commands(io, coder)\n\n        # Create test files\n        test_files = [\"file1.txt\", \"file2.txt\", \"file3.py\"]\n        for fname in test_files:\n            Path(fname).touch()\n\n        # Add all files to the chat session\n        for fname in test_files:\n            commands.cmd_add(fname)\n\n        initial_count = len(coder.abs_fnames)\n        self.assertEqual(initial_count, 3)\n\n        # Test dropping individual files without glob\n        commands.cmd_drop(\"file1.txt\")\n        self.assertNotIn(str(Path(\"file1.txt\").resolve()), coder.abs_fnames)\n        self.assertIn(str(Path(\"file2.txt\").resolve()), coder.abs_fnames)\n        self.assertEqual(len(coder.abs_fnames), initial_count - 1)\n\n        # Test dropping multiple files without glob\n        commands.cmd_drop(\"file2.txt file3.py\")\n        self.assertNotIn(str(Path(\"file2.txt\").resolve()), coder.abs_fnames)\n        self.assertNotIn(str(Path(\"file3.py\").resolve()), coder.abs_fnames)\n        self.assertEqual(len(coder.abs_fnames), 0)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::11",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 152,
      "span_ids": [
        "TestCommands.test_cmd_add_bad_encoding"
      ],
      "start_line": 328,
      "end_line": 342,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_bad_encoding(self):\n        # Initialize the Commands and InputOutput objects\n        io = InputOutput(pretty=False, fancy_input=False, yes=True)\n        from aider.coders import Coder\n\n        coder = Coder.create(self.GPT35, None, io)\n        commands = Commands(io, coder)\n\n        # Create a new file foo.bad which will fail to decode as utf-8\n        with codecs.open(\"foo.bad\", \"w\", encoding=\"iso-8859-15\") as f:\n            f.write(\"\u00c6\u00d8\u00c5\")  # Characters not present in utf-8\n\n        commands.cmd_add(\"foo.bad\")\n\n        self.assertEqual(coder.abs_fnames, set())",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::12",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 186,
      "span_ids": [
        "TestCommands.test_cmd_git"
      ],
      "start_line": 344,
      "end_line": 363,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_git(self):\n        # Initialize the Commands and InputOutput objects\n        io = InputOutput(pretty=False, fancy_input=False, yes=True)\n\n        with GitTemporaryDirectory() as tempdir:\n            # Create a file in the temporary directory\n            with open(f\"{tempdir}/test.txt\", \"w\") as f:\n                f.write(\"test\")\n\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Run the cmd_git method with the arguments \"commit -a -m msg\"\n            commands.cmd_git(\"add test.txt\")\n            commands.cmd_git(\"commit -a -m msg\")\n\n            # Check if the file has been committed to the repository\n            repo = git.Repo(tempdir)\n            files_in_repo = repo.git.ls_files()\n            self.assertIn(\"test.txt\", files_in_repo)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::13",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 151,
      "span_ids": [
        "TestCommands.test_cmd_tokens"
      ],
      "start_line": 365,
      "end_line": 387,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_tokens(self):\n        # Initialize the Commands and InputOutput objects\n        io = InputOutput(pretty=False, fancy_input=False, yes=True)\n\n        coder = Coder.create(self.GPT35, None, io)\n        commands = Commands(io, coder)\n\n        commands.cmd_add(\"foo.txt bar.txt\")\n\n        # Redirect the standard output to an instance of io.StringIO\n        stdout = StringIO()\n        sys.stdout = stdout\n\n        commands.cmd_tokens(\"\")\n\n        # Reset the standard output\n        sys.stdout = sys.__stdout__\n\n        # Get the console output\n        console_output = stdout.getvalue()\n\n        self.assertIn(\"foo.txt\", console_output)\n        self.assertIn(\"bar.txt\", console_output)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::14",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 295,
      "span_ids": [
        "TestCommands.test_cmd_add_from_subdir"
      ],
      "start_line": 389,
      "end_line": 421,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_from_subdir(self):\n        repo = git.Repo.init()\n        repo.config_writer().set_value(\"user\", \"name\", \"Test User\").release()\n        repo.config_writer().set_value(\"user\", \"email\", \"testuser@example.com\").release()\n\n        # Create three empty files and add them to the git repository\n        filenames = [\"one.py\", Path(\"subdir\") / \"two.py\", Path(\"anotherdir\") / \"three.py\"]\n        for filename in filenames:\n            file_path = Path(filename)\n            file_path.parent.mkdir(parents=True, exist_ok=True)\n            file_path.touch()\n            repo.git.add(str(file_path))\n        repo.git.commit(\"-m\", \"added\")\n\n        filenames = [str(Path(fn).resolve()) for fn in filenames]\n\n        ###\n\n        os.chdir(\"subdir\")\n\n        io = InputOutput(pretty=False, fancy_input=False, yes=True)\n        coder = Coder.create(self.GPT35, None, io)\n        commands = Commands(io, coder)\n\n        # this should get added\n        commands.cmd_add(str(Path(\"anotherdir\") / \"three.py\"))\n\n        # this should add one.py\n        commands.cmd_add(\"*.py\")\n\n        self.assertIn(filenames[0], coder.abs_fnames)\n        self.assertNotIn(filenames[1], coder.abs_fnames)\n        self.assertIn(filenames[2], coder.abs_fnames)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::15",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 147,
      "span_ids": [
        "TestCommands.test_cmd_add_from_subdir_again"
      ],
      "start_line": 423,
      "end_line": 440,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_from_subdir_again(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            from aider.coders import Coder\n\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            Path(\"side_dir\").mkdir()\n            os.chdir(\"side_dir\")\n\n            # add a file that is in the side_dir\n            with open(\"temp.txt\", \"w\"):\n                pass\n\n            # this was blowing up with GitCommandError, per:\n            # https://github.com/Aider-AI/aider/issues/201\n            commands.cmd_add(\"temp.txt\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::16",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 158,
      "span_ids": [
        "TestCommands.test_cmd_commit"
      ],
      "start_line": 442,
      "end_line": 462,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_commit(self):\n        with GitTemporaryDirectory():\n            fname = \"test.txt\"\n            with open(fname, \"w\") as f:\n                f.write(\"test\")\n            repo = git.Repo()\n            repo.git.add(fname)\n            repo.git.commit(\"-m\", \"initial\")\n\n            io = InputOutput(pretty=False, fancy_input=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            self.assertFalse(repo.is_dirty())\n            with open(fname, \"w\") as f:\n                f.write(\"new\")\n            self.assertTrue(repo.is_dirty())\n\n            commit_message = \"Test commit message\"\n            commands.cmd_commit(commit_message)\n            self.assertFalse(repo.is_dirty())",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::17",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 156,
      "span_ids": [
        "TestCommands.test_cmd_add_from_outside_root"
      ],
      "start_line": 464,
      "end_line": 483,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_from_outside_root(self):\n        with ChdirTemporaryDirectory() as tmp_dname:\n            root = Path(\"root\")\n            root.mkdir()\n            os.chdir(str(root))\n\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            from aider.coders import Coder\n\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            outside_file = Path(tmp_dname) / \"outside.txt\"\n            outside_file.touch()\n\n            # This should not be allowed!\n            # https://github.com/Aider-AI/aider/issues/178\n            commands.cmd_add(\"../outside.txt\")\n\n            self.assertEqual(len(coder.abs_fnames), 0)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::18",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 274,
      "span_ids": [
        "TestCommands.test_cmd_add_filename_with_special_chars",
        "TestCommands.test_cmd_add_from_outside_git"
      ],
      "start_line": 485,
      "end_line": 522,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_from_outside_git(self):\n        with ChdirTemporaryDirectory() as tmp_dname:\n            root = Path(\"root\")\n            root.mkdir()\n            os.chdir(str(root))\n\n            make_repo()\n\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            from aider.coders import Coder\n\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            outside_file = Path(tmp_dname) / \"outside.txt\"\n            outside_file.touch()\n\n            # This should not be allowed!\n            # It was blowing up with GitCommandError, per:\n            # https://github.com/Aider-AI/aider/issues/178\n            commands.cmd_add(\"../outside.txt\")\n\n            self.assertEqual(len(coder.abs_fnames), 0)\n\n    def test_cmd_add_filename_with_special_chars(self):\n        with ChdirTemporaryDirectory():\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            from aider.coders import Coder\n\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            fname = Path(\"with[brackets].txt\")\n            fname.touch()\n\n            commands.cmd_add(str(fname))\n\n            self.assertIn(str(fname.resolve()), coder.abs_fnames)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::19",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 435,
      "span_ids": [
        "TestCommands.test_cmd_tokens_output"
      ],
      "start_line": 524,
      "end_line": 572,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_tokens_output(self):\n        with GitTemporaryDirectory() as repo_dir:\n            # Create a small repository with a few files\n            (Path(repo_dir) / \"file1.txt\").write_text(\"Content of file 1\")\n            (Path(repo_dir) / \"file2.py\").write_text(\"print('Content of file 2')\")\n            (Path(repo_dir) / \"subdir\").mkdir()\n            (Path(repo_dir) / \"subdir\" / \"file3.md\").write_text(\"# Content of file 3\")\n\n            repo = git.Repo.init(repo_dir)\n            repo.git.add(A=True)\n            repo.git.commit(\"-m\", \"Initial commit\")\n\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            from aider.coders import Coder\n\n            coder = Coder.create(Model(\"claude-3-5-sonnet-20240620\"), None, io)\n            print(coder.get_announcements())\n            commands = Commands(io, coder)\n\n            commands.cmd_add(\"*.txt\")\n\n            # Capture the output of cmd_tokens\n            original_tool_output = io.tool_output\n            output_lines = []\n\n            def capture_output(*args, **kwargs):\n                output_lines.extend(args)\n                original_tool_output(*args, **kwargs)\n\n            io.tool_output = capture_output\n\n            # Run cmd_tokens\n            commands.cmd_tokens(\"\")\n\n            # Restore original tool_output\n            io.tool_output = original_tool_output\n\n            # Check if the output includes repository map information\n            repo_map_line = next((line for line in output_lines if \"repository map\" in line), None)\n            self.assertIsNotNone(\n                repo_map_line, \"Repository map information not found in the output\"\n            )\n\n            # Check if the output includes information about all added files\n            self.assertTrue(any(\"file1.txt\" in line for line in output_lines))\n\n            # Check if the total tokens and remaining tokens are reported\n            self.assertTrue(any(\"tokens total\" in line for line in output_lines))\n            self.assertTrue(any(\"tokens remaining\" in line for line in output_lines))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::20",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 129,
      "span_ids": [
        "TestCommands.test_cmd_add_dirname_with_special_chars"
      ],
      "start_line": 574,
      "end_line": 590,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_dirname_with_special_chars(self):\n        with ChdirTemporaryDirectory():\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            from aider.coders import Coder\n\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            dname = Path(\"with[brackets]\")\n            dname.mkdir()\n            fname = dname / \"filename.txt\"\n            fname.touch()\n\n            commands.cmd_add(str(dname))\n\n            dump(coder.abs_fnames)\n            self.assertIn(str(fname.resolve()), coder.abs_fnames)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::21",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 153,
      "span_ids": [
        "TestCommands.test_cmd_add_dirname_with_special_chars_git"
      ],
      "start_line": 592,
      "end_line": 612,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_dirname_with_special_chars_git(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            from aider.coders import Coder\n\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            dname = Path(\"with[brackets]\")\n            dname.mkdir()\n            fname = dname / \"filename.txt\"\n            fname.touch()\n\n            repo = git.Repo()\n            repo.git.add(str(fname))\n            repo.git.commit(\"-m\", \"init\")\n\n            commands.cmd_add(str(dname))\n\n            dump(coder.abs_fnames)\n            self.assertIn(str(fname.resolve()), coder.abs_fnames)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::22",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 202,
      "span_ids": [
        "TestCommands.test_cmd_add_abs_filename",
        "TestCommands.test_cmd_add_quoted_filename"
      ],
      "start_line": 614,
      "end_line": 642,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_abs_filename(self):\n        with ChdirTemporaryDirectory():\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            from aider.coders import Coder\n\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            fname = Path(\"file.txt\")\n            fname.touch()\n\n            commands.cmd_add(str(fname.resolve()))\n\n            self.assertIn(str(fname.resolve()), coder.abs_fnames)\n\n    def test_cmd_add_quoted_filename(self):\n        with ChdirTemporaryDirectory():\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            from aider.coders import Coder\n\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            fname = Path(\"file with spaces.txt\")\n            fname.touch()\n\n            commands.cmd_add(f'\"{fname}\"')\n\n            self.assertIn(str(fname.resolve()), coder.abs_fnames)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::23",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 231,
      "span_ids": [
        "TestCommands.test_cmd_add_existing_with_dirty_repo"
      ],
      "start_line": 644,
      "end_line": 678,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_existing_with_dirty_repo(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n\n            files = [\"one.txt\", \"two.txt\"]\n            for fname in files:\n                Path(fname).touch()\n                repo.git.add(fname)\n            repo.git.commit(\"-m\", \"initial\")\n\n            commit = repo.head.commit.hexsha\n\n            # leave a dirty `git rm`\n            repo.git.rm(\"one.txt\")\n\n            io = InputOutput(pretty=False, fancy_input=False, yes=True)\n            from aider.coders import Coder\n\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # There's no reason this /add should trigger a commit\n            commands.cmd_add(\"two.txt\")\n\n            self.assertEqual(commit, repo.head.commit.hexsha)\n\n            # Windows is throwing:\n            # PermissionError: [WinError 32] The process cannot access\n            # the file because it is being used by another process\n\n            repo.git.commit(\"-m\", \"cleanup\")\n\n            del coder\n            del commands\n            del repo",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::24",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 668,
      "span_ids": [
        "TestCommands.test_cmd_save_and_load"
      ],
      "start_line": 680,
      "end_line": 752,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_save_and_load(self):\n        with GitTemporaryDirectory() as repo_dir:\n            io = InputOutput(pretty=False, fancy_input=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Create some test files\n            test_files = {\n                \"file1.txt\": \"Content of file 1\",\n                \"file2.py\": \"print('Content of file 2')\",\n                \"subdir/file3.md\": \"# Content of file 3\",\n            }\n\n            for file_path, content in test_files.items():\n                full_path = Path(repo_dir) / file_path\n                full_path.parent.mkdir(parents=True, exist_ok=True)\n                full_path.write_text(content)\n\n            # Add some files as editable and some as read-only\n            commands.cmd_add(\"file1.txt file2.py\")\n            commands.cmd_read_only(\"subdir/file3.md\")\n\n            # Save the session to a file\n            session_file = \"test_session.txt\"\n            commands.cmd_save(session_file)\n\n            # Verify the session file was created and contains the expected commands\n            self.assertTrue(Path(session_file).exists())\n            with open(session_file, encoding=io.encoding) as f:\n                commands_text = f.read().splitlines()\n\n                # Convert paths to absolute for comparison\n                abs_file1 = str(Path(\"file1.txt\").resolve())\n                abs_file2 = str(Path(\"file2.py\").resolve())\n                abs_file3 = str(Path(\"subdir/file3.md\").resolve())\n\n                # Check each line for matching paths using os.path.samefile\n                found_file1 = found_file2 = found_file3 = False\n                for line in commands_text:\n                    if line.startswith(\"/add \"):\n                        path = Path(line[5:].strip()).resolve()\n                        if os.path.samefile(str(path), abs_file1):\n                            found_file1 = True\n                        elif os.path.samefile(str(path), abs_file2):\n                            found_file2 = True\n                    elif line.startswith(\"/read-only \"):\n                        path = Path(line[11:]).resolve()\n                        if os.path.samefile(str(path), abs_file3):\n                            found_file3 = True\n\n                self.assertTrue(found_file1, \"file1.txt not found in commands\")\n                self.assertTrue(found_file2, \"file2.py not found in commands\")\n                self.assertTrue(found_file3, \"file3.md not found in commands\")\n\n            # Clear the current session\n            commands.cmd_reset(\"\")\n            self.assertEqual(len(coder.abs_fnames), 0)\n            self.assertEqual(len(coder.abs_read_only_fnames), 0)\n\n            # Load the session back\n            commands.cmd_load(session_file)\n\n            # Verify files were restored correctly\n            added_files = {Path(coder.get_rel_fname(f)).as_posix() for f in coder.abs_fnames}\n            read_only_files = {\n                Path(coder.get_rel_fname(f)).as_posix() for f in coder.abs_read_only_fnames\n            }\n\n            self.assertEqual(added_files, {\"file1.txt\", \"file2.py\"})\n            self.assertEqual(read_only_files, {\"subdir/file3.md\"})\n\n            # Clean up\n            Path(session_file).unlink()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::25",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 579,
      "span_ids": [
        "TestCommands.test_cmd_save_and_load_with_external_file"
      ],
      "start_line": 754,
      "end_line": 822,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_save_and_load_with_external_file(self):\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as external_file:\n            external_file.write(\"External file content\")\n            external_file_path = external_file.name\n\n        try:\n            with GitTemporaryDirectory() as repo_dir:\n                io = InputOutput(pretty=False, fancy_input=False, yes=True)\n                coder = Coder.create(self.GPT35, None, io)\n                commands = Commands(io, coder)\n\n                # Create some test files in the repo\n                test_files = {\n                    \"file1.txt\": \"Content of file 1\",\n                    \"file2.py\": \"print('Content of file 2')\",\n                }\n\n                for file_path, content in test_files.items():\n                    full_path = Path(repo_dir) / file_path\n                    full_path.parent.mkdir(parents=True, exist_ok=True)\n                    full_path.write_text(content)\n\n                # Add some files as editable and some as read-only\n                commands.cmd_add(str(Path(\"file1.txt\")))\n                commands.cmd_read_only(external_file_path)\n\n                # Save the session to a file\n                session_file = str(Path(\"test_session.txt\"))\n                commands.cmd_save(session_file)\n\n                # Verify the session file was created and contains the expected commands\n                self.assertTrue(Path(session_file).exists())\n                with open(session_file, encoding=io.encoding) as f:\n                    commands_text = f.read()\n                    commands_text = re.sub(\n                        r\"/add +\", \"/add \", commands_text\n                    )  # Normalize add command spaces\n                    self.assertIn(\"/add file1.txt\", commands_text)\n                    # Split commands and check each one\n                    for line in commands_text.splitlines():\n                        if line.startswith(\"/read-only \"):\n                            saved_path = line.split(\" \", 1)[1]\n                            if os.path.samefile(saved_path, external_file_path):\n                                break\n                    else:\n                        self.fail(f\"No matching read-only command found for {external_file_path}\")\n\n                # Clear the current session\n                commands.cmd_reset(\"\")\n                self.assertEqual(len(coder.abs_fnames), 0)\n                self.assertEqual(len(coder.abs_read_only_fnames), 0)\n\n                # Load the session back\n                commands.cmd_load(session_file)\n\n                # Verify files were restored correctly\n                added_files = {coder.get_rel_fname(f) for f in coder.abs_fnames}\n                read_only_files = {coder.get_rel_fname(f) for f in coder.abs_read_only_fnames}\n\n                self.assertEqual(added_files, {str(Path(\"file1.txt\"))})\n                self.assertTrue(\n                    any(os.path.samefile(external_file_path, f) for f in read_only_files)\n                )\n\n                # Clean up\n                Path(session_file).unlink()\n\n        finally:\n            os.unlink(external_file_path)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::26",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 747,
      "span_ids": [
        "TestCommands.test_cmd_save_and_load_with_multiple_external_files"
      ],
      "start_line": 824,
      "end_line": 910,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_save_and_load_with_multiple_external_files(self):\n        with (\n            tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as external_file1,\n            tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as external_file2,\n        ):\n            external_file1.write(\"External file 1 content\")\n            external_file2.write(\"External file 2 content\")\n            external_file1_path = external_file1.name\n            external_file2_path = external_file2.name\n\n        try:\n            with GitTemporaryDirectory() as repo_dir:\n                io = InputOutput(pretty=False, fancy_input=False, yes=True)\n                coder = Coder.create(self.GPT35, None, io)\n                commands = Commands(io, coder)\n\n                # Create some test files in the repo\n                test_files = {\n                    \"internal1.txt\": \"Content of internal file 1\",\n                    \"internal2.txt\": \"Content of internal file 2\",\n                }\n\n                for file_path, content in test_files.items():\n                    full_path = Path(repo_dir) / file_path\n                    full_path.parent.mkdir(parents=True, exist_ok=True)\n                    full_path.write_text(content)\n\n                # Add files as editable and read-only\n                commands.cmd_add(str(Path(\"internal1.txt\")))\n                commands.cmd_read_only(external_file1_path)\n                commands.cmd_read_only(external_file2_path)\n\n                # Save the session to a file\n                session_file = str(Path(\"test_session.txt\"))\n                commands.cmd_save(session_file)\n\n                # Verify the session file was created and contains the expected commands\n                self.assertTrue(Path(session_file).exists())\n                with open(session_file, encoding=io.encoding) as f:\n                    commands_text = f.read()\n                    commands_text = re.sub(\n                        r\"/add +\", \"/add \", commands_text\n                    )  # Normalize add command spaces\n                    self.assertIn(\"/add internal1.txt\", commands_text)\n                    # Split commands and check each one\n                    for line in commands_text.splitlines():\n                        if line.startswith(\"/read-only \"):\n                            saved_path = line.split(\" \", 1)[1]\n                            if os.path.samefile(saved_path, external_file1_path):\n                                break\n                    else:\n                        self.fail(f\"No matching read-only command found for {external_file1_path}\")\n                    # Split commands and check each one\n                    for line in commands_text.splitlines():\n                        if line.startswith(\"/read-only \"):\n                            saved_path = line.split(\" \", 1)[1]\n                            if os.path.samefile(saved_path, external_file2_path):\n                                break\n                    else:\n                        self.fail(f\"No matching read-only command found for {external_file2_path}\")\n\n                # Clear the current session\n                commands.cmd_reset(\"\")\n                self.assertEqual(len(coder.abs_fnames), 0)\n                self.assertEqual(len(coder.abs_read_only_fnames), 0)\n\n                # Load the session back\n                commands.cmd_load(session_file)\n\n                # Verify files were restored correctly\n                added_files = {coder.get_rel_fname(f) for f in coder.abs_fnames}\n                read_only_files = {coder.get_rel_fname(f) for f in coder.abs_read_only_fnames}\n\n                self.assertEqual(added_files, {str(Path(\"internal1.txt\"))})\n                self.assertTrue(\n                    all(\n                        any(os.path.samefile(external_path, fname) for fname in read_only_files)\n                        for external_path in [external_file1_path, external_file2_path]\n                    )\n                )\n\n                # Clean up\n                Path(session_file).unlink()\n\n        finally:\n            os.unlink(external_file1_path)\n            os.unlink(external_file2_path)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::27",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 406,
      "span_ids": [
        "TestCommands.test_cmd_read_only_with_image_file"
      ],
      "start_line": 912,
      "end_line": 955,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_read_only_with_image_file(self):\n        with GitTemporaryDirectory() as repo_dir:\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Create a test image file\n            test_file = Path(repo_dir) / \"test_image.jpg\"\n            test_file.write_text(\"Mock image content\")\n\n            # Test with non-vision model\n            commands.cmd_read_only(str(test_file))\n            self.assertEqual(len(coder.abs_read_only_fnames), 0)\n\n            # Test with vision model\n            vision_model = Model(\"gpt-4-vision-preview\")\n            vision_coder = Coder.create(vision_model, None, io)\n            vision_commands = Commands(io, vision_coder)\n\n            vision_commands.cmd_read_only(str(test_file))\n            self.assertEqual(len(vision_coder.abs_read_only_fnames), 1)\n            self.assertTrue(\n                any(\n                    os.path.samefile(str(test_file), fname)\n                    for fname in vision_coder.abs_read_only_fnames\n                )\n            )\n\n            # Add a dummy message to ensure format_messages() works\n            vision_coder.cur_messages = [{\"role\": \"user\", \"content\": \"Check the image\"}]\n\n            # Check that the image file appears in the messages\n            messages = vision_coder.format_messages().all_messages()\n            found_image = False\n            for msg in messages:\n                if msg.get(\"role\") == \"user\" and \"content\" in msg:\n                    content = msg[\"content\"]\n                    if isinstance(content, list):\n                        for item in content:\n                            if isinstance(item, dict) and item.get(\"type\") == \"text\":\n                                if \"test_image.jpg\" in item.get(\"text\", \"\"):\n                                    found_image = True\n                                    break\n            self.assertTrue(found_image, \"Image file not found in messages to LLM\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::28",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 303,
      "span_ids": [
        "TestCommands.test_cmd_read_only_with_glob_pattern"
      ],
      "start_line": 957,
      "end_line": 990,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_read_only_with_glob_pattern(self):\n        with GitTemporaryDirectory() as repo_dir:\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Create multiple test files\n            test_files = [\"test_file1.txt\", \"test_file2.txt\", \"other_file.txt\"]\n            for file_name in test_files:\n                file_path = Path(repo_dir) / file_name\n                file_path.write_text(f\"Content of {file_name}\")\n\n            # Test the /read-only command with a glob pattern\n            commands.cmd_read_only(\"test_*.txt\")\n\n            # Check if only the matching files were added to abs_read_only_fnames\n            self.assertEqual(len(coder.abs_read_only_fnames), 2)\n            for file_name in [\"test_file1.txt\", \"test_file2.txt\"]:\n                file_path = Path(repo_dir) / file_name\n                self.assertTrue(\n                    any(\n                        os.path.samefile(str(file_path), fname)\n                        for fname in coder.abs_read_only_fnames\n                    )\n                )\n\n            # Check that other_file.txt was not added\n            other_file_path = Path(repo_dir) / \"other_file.txt\"\n            self.assertFalse(\n                any(\n                    os.path.samefile(str(other_file_path), fname)\n                    for fname in coder.abs_read_only_fnames\n                )\n            )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::29",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 253,
      "span_ids": [
        "TestCommands.test_cmd_read_only_with_recursive_glob"
      ],
      "start_line": 992,
      "end_line": 1017,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_read_only_with_recursive_glob(self):\n        with GitTemporaryDirectory() as repo_dir:\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Create a directory structure with files\n            (Path(repo_dir) / \"subdir\").mkdir()\n            test_files = [\"test_file1.txt\", \"subdir/test_file2.txt\", \"subdir/other_file.txt\"]\n            for file_name in test_files:\n                file_path = Path(repo_dir) / file_name\n                file_path.write_text(f\"Content of {file_name}\")\n\n            # Test the /read-only command with a recursive glob pattern\n            commands.cmd_read_only(\"**/*.txt\")\n\n            # Check if all .txt files were added to abs_read_only_fnames\n            self.assertEqual(len(coder.abs_read_only_fnames), 3)\n            for file_name in test_files:\n                file_path = Path(repo_dir) / file_name\n                self.assertTrue(\n                    any(\n                        os.path.samefile(str(file_path), fname)\n                        for fname in coder.abs_read_only_fnames\n                    )\n                )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::30",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 184,
      "span_ids": [
        "TestCommands.test_cmd_read_only_with_nonexistent_glob"
      ],
      "start_line": 1019,
      "end_line": 1035,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_read_only_with_nonexistent_glob(self):\n        with GitTemporaryDirectory() as repo_dir:\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Test the /read-only command with a non-existent glob pattern\n            with mock.patch.object(io, \"tool_error\") as mock_tool_error:\n                commands.cmd_read_only(str(Path(repo_dir) / \"nonexistent*.txt\"))\n\n            # Check if the appropriate error message was displayed\n            mock_tool_error.assert_called_once_with(\n                f\"No matches found for: {Path(repo_dir) / 'nonexistent*.txt'}\"\n            )\n\n            # Ensure no files were added to abs_read_only_fnames\n            self.assertEqual(len(coder.abs_read_only_fnames), 0)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::31",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 160,
      "span_ids": [
        "TestCommands.test_cmd_add_unicode_error"
      ],
      "start_line": 1037,
      "end_line": 1052,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_unicode_error(self):\n        # Initialize the Commands and InputOutput objects\n        io = InputOutput(pretty=False, fancy_input=False, yes=True)\n        from aider.coders import Coder\n\n        coder = Coder.create(self.GPT35, None, io)\n        commands = Commands(io, coder)\n\n        fname = \"file.txt\"\n        encoding = \"utf-16\"\n        some_content_which_will_error_if_read_with_encoding_utf8 = \"\u00c5\u00cd\u00ce\u00cf\".encode(encoding)\n        with open(fname, \"wb\") as f:\n            f.write(some_content_which_will_error_if_read_with_encoding_utf8)\n\n        commands.cmd_add(\"file.txt\")\n        self.assertEqual(coder.abs_fnames, set())",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::32",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 386,
      "span_ids": [
        "TestCommands.test_cmd_add_read_only_file"
      ],
      "start_line": 1054,
      "end_line": 1108,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_read_only_file(self):\n        with GitTemporaryDirectory():\n            # Initialize the Commands and InputOutput objects\n            io = InputOutput(pretty=False, fancy_input=False, yes=True)\n            from aider.coders import Coder\n\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Create a test file\n            test_file = Path(\"test_read_only.txt\")\n            test_file.write_text(\"Test content\")\n\n            # Add the file as read-only\n            commands.cmd_read_only(str(test_file))\n\n            # Verify it's in abs_read_only_fnames\n            self.assertTrue(\n                any(\n                    os.path.samefile(str(test_file.resolve()), fname)\n                    for fname in coder.abs_read_only_fnames\n                )\n            )\n\n            # Try to add the read-only file\n            commands.cmd_add(str(test_file))\n\n            # It's not in the repo, should not do anything\n            self.assertFalse(\n                any(os.path.samefile(str(test_file.resolve()), fname) for fname in coder.abs_fnames)\n            )\n            self.assertTrue(\n                any(\n                    os.path.samefile(str(test_file.resolve()), fname)\n                    for fname in coder.abs_read_only_fnames\n                )\n            )\n\n            repo = git.Repo()\n            repo.git.add(str(test_file))\n            repo.git.commit(\"-m\", \"initial\")\n\n            # Try to add the read-only file\n            commands.cmd_add(str(test_file))\n\n            # Verify it's now in abs_fnames and not in abs_read_only_fnames\n            self.assertTrue(\n                any(os.path.samefile(str(test_file.resolve()), fname) for fname in coder.abs_fnames)\n            )\n            self.assertFalse(\n                any(\n                    os.path.samefile(str(test_file.resolve()), fname)\n                    for fname in coder.abs_read_only_fnames\n                )\n            )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::33",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 165,
      "span_ids": [
        "TestCommands.test_cmd_test_unbound_local_error"
      ],
      "start_line": 1110,
      "end_line": 1125,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_test_unbound_local_error(self):\n        with ChdirTemporaryDirectory():\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            from aider.coders import Coder\n\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Mock the io.prompt_ask method to simulate user input\n            io.prompt_ask = lambda *args, **kwargs: \"y\"\n\n            # Test the cmd_run method with a command that should not raise an error\n            commands.cmd_run(\"exit 1\", add_on_nonzero_exit=True)\n\n            # Check that the output was added to cur_messages\n            self.assertTrue(any(\"exit 1\" in msg[\"content\"] for msg in coder.cur_messages))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::34",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 160,
      "span_ids": [
        "TestCommands.test_cmd_add_drop_untracked_files"
      ],
      "start_line": 1127,
      "end_line": 1151,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_drop_untracked_files(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            from aider.coders import Coder\n\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            fname = Path(\"test.txt\")\n            fname.touch()\n\n            self.assertEqual(len(coder.abs_fnames), 0)\n\n            commands.cmd_add(str(fname))\n\n            files_in_repo = repo.git.ls_files()\n            self.assertNotIn(str(fname), files_in_repo)\n\n            self.assertEqual(len(coder.abs_fnames), 1)\n\n            commands.cmd_drop(str(fname))\n\n            self.assertEqual(len(coder.abs_fnames), 0)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::35",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 357,
      "span_ids": [
        "TestCommands.test_cmd_undo_with_dirty_files_not_in_last_commit"
      ],
      "start_line": 1153,
      "end_line": 1199,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_undo_with_dirty_files_not_in_last_commit(self):\n        with GitTemporaryDirectory() as repo_dir:\n            repo = git.Repo(repo_dir)\n            io = InputOutput(pretty=False, fancy_input=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            other_path = Path(repo_dir) / \"other_file.txt\"\n            other_path.write_text(\"other content\")\n            repo.git.add(str(other_path))\n\n            # Create and commit a file\n            filename = \"test_file.txt\"\n            file_path = Path(repo_dir) / filename\n            file_path.write_text(\"first content\")\n            repo.git.add(filename)\n            repo.git.commit(\"-m\", \"first commit\")\n\n            file_path.write_text(\"second content\")\n            repo.git.add(filename)\n            repo.git.commit(\"-m\", \"second commit\")\n\n            # Store the commit hash\n            last_commit_hash = repo.head.commit.hexsha[:7]\n            coder.aider_commit_hashes.add(last_commit_hash)\n\n            file_path.write_text(\"dirty content\")\n\n            # Attempt to undo the last commit\n            commands.cmd_undo(\"\")\n\n            # Check that the last commit is still present\n            self.assertEqual(last_commit_hash, repo.head.commit.hexsha[:7])\n\n            # Put back the initial content (so it's not dirty now)\n            file_path.write_text(\"second content\")\n            other_path.write_text(\"dirty content\")\n\n            commands.cmd_undo(\"\")\n            self.assertNotEqual(last_commit_hash, repo.head.commit.hexsha[:7])\n\n            self.assertEqual(file_path.read_text(), \"first content\")\n            self.assertEqual(other_path.read_text(), \"dirty content\")\n\n            del coder\n            del commands\n            del repo",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::36",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 281,
      "span_ids": [
        "TestCommands.test_cmd_undo_with_newly_committed_file"
      ],
      "start_line": 1201,
      "end_line": 1235,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_undo_with_newly_committed_file(self):\n        with GitTemporaryDirectory() as repo_dir:\n            repo = git.Repo(repo_dir)\n            io = InputOutput(pretty=False, fancy_input=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Put in a random first commit\n            filename = \"first_file.txt\"\n            file_path = Path(repo_dir) / filename\n            file_path.write_text(\"new file content\")\n            repo.git.add(filename)\n            repo.git.commit(\"-m\", \"Add new file\")\n\n            # Create and commit a new file\n            filename = \"new_file.txt\"\n            file_path = Path(repo_dir) / filename\n            file_path.write_text(\"new file content\")\n            repo.git.add(filename)\n            repo.git.commit(\"-m\", \"Add new file\")\n\n            # Store the commit hash\n            last_commit_hash = repo.head.commit.hexsha[:7]\n            coder.aider_commit_hashes.add(last_commit_hash)\n\n            # Attempt to undo the last commit, should refuse\n            commands.cmd_undo(\"\")\n\n            # Check that the last commit was not undone\n            self.assertEqual(last_commit_hash, repo.head.commit.hexsha[:7])\n            self.assertTrue(file_path.exists())\n\n            del coder\n            del commands\n            del repo",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::37",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 218,
      "span_ids": [
        "TestCommands.test_cmd_undo_on_first_commit"
      ],
      "start_line": 1237,
      "end_line": 1264,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_undo_on_first_commit(self):\n        with GitTemporaryDirectory() as repo_dir:\n            repo = git.Repo(repo_dir)\n            io = InputOutput(pretty=False, fancy_input=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Create and commit a new file\n            filename = \"new_file.txt\"\n            file_path = Path(repo_dir) / filename\n            file_path.write_text(\"new file content\")\n            repo.git.add(filename)\n            repo.git.commit(\"-m\", \"Add new file\")\n\n            # Store the commit hash\n            last_commit_hash = repo.head.commit.hexsha[:7]\n            coder.aider_commit_hashes.add(last_commit_hash)\n\n            # Attempt to undo the last commit\n            commands.cmd_undo(\"\")\n\n            # Check that the commit is still present\n            self.assertEqual(last_commit_hash, repo.head.commit.hexsha[:7])\n            self.assertTrue(file_path.exists())\n\n            del coder\n            del commands\n            del repo",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::38",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 160,
      "span_ids": [
        "TestCommands.test_cmd_add_gitignored_file"
      ],
      "start_line": 1266,
      "end_line": 1284,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_gitignored_file(self):\n        with GitTemporaryDirectory():\n            # Create a .gitignore file\n            gitignore = Path(\".gitignore\")\n            gitignore.write_text(\"*.ignored\\n\")\n\n            # Create a file that matches the gitignore pattern\n            ignored_file = Path(\"test.ignored\")\n            ignored_file.write_text(\"This should be ignored\")\n\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Try to add the ignored file\n            commands.cmd_add(str(ignored_file))\n\n            # Verify the file was not added\n            self.assertEqual(len(coder.abs_fnames), 0)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::39",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 258,
      "span_ids": [
        "TestCommands.test_cmd_add_aiderignored_file"
      ],
      "start_line": 1286,
      "end_line": 1324,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_add_aiderignored_file(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n\n            fname1 = \"ignoreme1.txt\"\n            fname2 = \"ignoreme2.txt\"\n            fname3 = \"dir/ignoreme3.txt\"\n\n            Path(fname2).touch()\n            repo.git.add(str(fname2))\n            repo.git.commit(\"-m\", \"initial\")\n\n            aignore = Path(\".aiderignore\")\n            aignore.write_text(f\"{fname1}\\n{fname2}\\ndir\\n\")\n\n            io = InputOutput(yes=True)\n\n            fnames = [fname1, fname2]\n            repo = GitRepo(\n                io,\n                fnames,\n                None,\n                aider_ignore_file=str(aignore),\n            )\n\n            coder = Coder.create(\n                self.GPT35,\n                None,\n                io,\n                fnames=fnames,\n                repo=repo,\n            )\n            commands = Commands(io, coder)\n\n            commands.cmd_add(f\"{fname1} {fname2} {fname3}\")\n\n            self.assertNotIn(fname1, str(coder.abs_fnames))\n            self.assertNotIn(fname2, str(coder.abs_fnames))\n            self.assertNotIn(fname3, str(coder.abs_fnames))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::40",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 216,
      "span_ids": [
        "TestCommands.test_cmd_read_only"
      ],
      "start_line": 1326,
      "end_line": 1356,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_read_only(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Create a test file\n            test_file = Path(\"test_read.txt\")\n            test_file.write_text(\"Test content\")\n\n            # Test the /read command\n            commands.cmd_read_only(str(test_file))\n\n            # Check if the file was added to abs_read_only_fnames\n            self.assertTrue(\n                any(\n                    os.path.samefile(str(test_file.resolve()), fname)\n                    for fname in coder.abs_read_only_fnames\n                )\n            )\n\n            # Test dropping the read-only file\n            commands.cmd_drop(str(test_file))\n\n            # Check if the file was removed from abs_read_only_fnames\n            self.assertFalse(\n                any(\n                    os.path.samefile(str(test_file.resolve()), fname)\n                    for fname in coder.abs_read_only_fnames\n                )\n            )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::41",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 297,
      "span_ids": [
        "TestCommands.test_cmd_read_only_from_working_dir"
      ],
      "start_line": 1358,
      "end_line": 1393,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_read_only_from_working_dir(self):\n        with GitTemporaryDirectory() as repo_dir:\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Create a subdirectory and a test file within it\n            subdir = Path(repo_dir) / \"subdir\"\n            subdir.mkdir()\n            test_file = subdir / \"test_read_only_file.txt\"\n            test_file.write_text(\"Test content\")\n\n            # Change the current working directory to the subdirectory\n            os.chdir(subdir)\n\n            # Test the /read-only command using git_root referenced name\n            commands.cmd_read_only(os.path.join(\"subdir\", \"test_read_only_file.txt\"))\n\n            # Check if the file was added to abs_read_only_fnames\n            self.assertTrue(\n                any(\n                    os.path.samefile(str(test_file.resolve()), fname)\n                    for fname in coder.abs_read_only_fnames\n                )\n            )\n\n            # Test dropping the read-only file using git_root referenced name\n            commands.cmd_drop(os.path.join(\"subdir\", \"test_read_only_file.txt\"))\n\n            # Check if the file was removed from abs_read_only_fnames\n            self.assertFalse(\n                any(\n                    os.path.samefile(str(test_file.resolve()), fname)\n                    for fname in coder.abs_read_only_fnames\n                )\n            )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::42",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 305,
      "span_ids": [
        "TestCommands.test_cmd_read_only_with_external_file"
      ],
      "start_line": 1395,
      "end_line": 1432,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_read_only_with_external_file(self):\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as external_file:\n            external_file.write(\"External file content\")\n            external_file_path = external_file.name\n\n        try:\n            with GitTemporaryDirectory() as repo_dir:\n                # Create a test file in the repo\n                repo_file = Path(repo_dir) / \"repo_file.txt\"\n                repo_file.write_text(\"Repo file content\")\n                io = InputOutput(pretty=False, fancy_input=False, yes=False)\n                coder = Coder.create(self.GPT35, None, io)\n                commands = Commands(io, coder)\n\n                # Test the /read command with an external file\n                commands.cmd_read_only(external_file_path)\n\n                # Check if the external file was added to abs_read_only_fnames\n                real_external_file_path = os.path.realpath(external_file_path)\n                self.assertTrue(\n                    any(\n                        os.path.samefile(real_external_file_path, fname)\n                        for fname in coder.abs_read_only_fnames\n                    )\n                )\n\n                # Test dropping the external read-only file\n                commands.cmd_drop(Path(external_file_path).name)\n\n                # Check if the file was removed from abs_read_only_fnames\n                self.assertFalse(\n                    any(\n                        os.path.samefile(real_external_file_path, fname)\n                        for fname in coder.abs_read_only_fnames\n                    )\n                )\n        finally:\n            os.unlink(external_file_path)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::43",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 305,
      "span_ids": [
        "TestCommands.test_cmd_drop_read_only_with_relative_path"
      ],
      "start_line": 1434,
      "end_line": 1469,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_drop_read_only_with_relative_path(self):\n        with ChdirTemporaryDirectory() as repo_dir:\n            test_file = Path(\"test_file.txt\")\n            test_file.write_text(\"Test content\")\n\n            # Create a test file in a subdirectory\n            subdir = Path(repo_dir) / \"subdir\"\n            subdir.mkdir()\n            os.chdir(subdir)\n\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Add the file as read-only using absolute path\n            rel_path = str(Path(\"..\") / \"test_file.txt\")\n            commands.cmd_read_only(rel_path)\n            self.assertEqual(len(coder.abs_read_only_fnames), 1)\n\n            # Try to drop using relative path from different working directories\n            commands.cmd_drop(\"test_file.txt\")\n            self.assertEqual(len(coder.abs_read_only_fnames), 0)\n\n            # Add it again\n            commands.cmd_read_only(rel_path)\n            self.assertEqual(len(coder.abs_read_only_fnames), 1)\n\n            commands.cmd_drop(rel_path)\n            self.assertEqual(len(coder.abs_read_only_fnames), 0)\n\n            # Add it one more time\n            commands.cmd_read_only(rel_path)\n            self.assertEqual(len(coder.abs_read_only_fnames), 1)\n\n            commands.cmd_drop(\"test_file.txt\")\n            self.assertEqual(len(coder.abs_read_only_fnames), 0)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::44",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 266,
      "span_ids": [
        "TestCommands.test_cmd_read_only_bulk_conversion"
      ],
      "start_line": 1471,
      "end_line": 1502,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_read_only_bulk_conversion(self):\n        with GitTemporaryDirectory() as repo_dir:\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Create and add some test files\n            test_files = [\"test1.txt\", \"test2.txt\", \"test3.txt\"]\n            for fname in test_files:\n                Path(fname).write_text(f\"Content of {fname}\")\n                commands.cmd_add(fname)\n\n            # Verify files are in editable mode\n            self.assertEqual(len(coder.abs_fnames), 3)\n            self.assertEqual(len(coder.abs_read_only_fnames), 0)\n\n            # Convert all files to read-only mode\n            commands.cmd_read_only(\"\")\n\n            # Verify all files were moved to read-only\n            self.assertEqual(len(coder.abs_fnames), 0)\n            self.assertEqual(len(coder.abs_read_only_fnames), 3)\n\n            # Check specific files\n            for fname in test_files:\n                abs_path = Path(repo_dir) / fname\n                self.assertTrue(\n                    any(\n                        os.path.samefile(str(abs_path), ro_fname)\n                        for ro_fname in coder.abs_read_only_fnames\n                    )\n                )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::45",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 267,
      "span_ids": [
        "TestCommands.test_cmd_read_only_with_multiple_files"
      ],
      "start_line": 1504,
      "end_line": 1533,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_read_only_with_multiple_files(self):\n        with GitTemporaryDirectory() as repo_dir:\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Create multiple test files\n            test_files = [\"test_file1.txt\", \"test_file2.txt\", \"test_file3.txt\"]\n            for file_name in test_files:\n                file_path = Path(repo_dir) / file_name\n                file_path.write_text(f\"Content of {file_name}\")\n\n            # Test the /read-only command with multiple files\n            commands.cmd_read_only(\" \".join(test_files))\n\n            # Check if all test files were added to abs_read_only_fnames\n            for file_name in test_files:\n                file_path = Path(repo_dir) / file_name\n                self.assertTrue(\n                    any(\n                        os.path.samefile(str(file_path), fname)\n                        for fname in coder.abs_read_only_fnames\n                    )\n                )\n\n            # Test dropping all read-only files\n            commands.cmd_drop(\" \".join(test_files))\n\n            # Check if all files were removed from abs_read_only_fnames\n            self.assertEqual(len(coder.abs_read_only_fnames), 0)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::46",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 270,
      "span_ids": [
        "TestCommands.test_cmd_read_only_with_tilde_path"
      ],
      "start_line": 1535,
      "end_line": 1567,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_read_only_with_tilde_path(self):\n        with GitTemporaryDirectory():\n            io = InputOutput(pretty=False, fancy_input=False, yes=False)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Create a test file in the user's home directory\n            home_dir = os.path.expanduser(\"~\")\n            test_file = Path(home_dir) / \"test_read_only_file.txt\"\n            test_file.write_text(\"Test content\")\n\n            try:\n                # Test the /read-only command with a path in the user's home directory\n                relative_path = os.path.join(\"~\", \"test_read_only_file.txt\")\n                commands.cmd_read_only(relative_path)\n\n                # Check if the file was added to abs_read_only_fnames\n                self.assertTrue(\n                    any(\n                        os.path.samefile(str(test_file), fname)\n                        for fname in coder.abs_read_only_fnames\n                    )\n                )\n\n                # Test dropping the read-only file\n                commands.cmd_drop(relative_path)\n\n                # Check if the file was removed from abs_read_only_fnames\n                self.assertEqual(len(coder.abs_read_only_fnames), 0)\n\n            finally:\n                # Clean up: remove the test file from the home directory\n                test_file.unlink()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::47",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 501,
      "span_ids": [
        "TestCommands.test_cmd_diff"
      ],
      "start_line": 1569,
      "end_line": 1633,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_diff(self):\n        with GitTemporaryDirectory() as repo_dir:\n            repo = git.Repo(repo_dir)\n            io = InputOutput(pretty=False, fancy_input=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Create and commit a file\n            filename = \"test_file.txt\"\n            file_path = Path(repo_dir) / filename\n            file_path.write_text(\"Initial content\\n\")\n            repo.git.add(filename)\n            repo.git.commit(\"-m\", \"Initial commit\\n\")\n\n            # Modify the file to make it dirty\n            file_path.write_text(\"Modified content\")\n\n            # Mock repo.get_commit_message to return a canned commit message\n            with mock.patch.object(\n                coder.repo, \"get_commit_message\", return_value=\"Canned commit message\"\n            ):\n                # Run cmd_commit\n                commands.cmd_commit()\n\n                # Capture the output of cmd_diff\n                with mock.patch(\"builtins.print\") as mock_print:\n                    commands.cmd_diff(\"\")\n\n                # Check if the diff output is correct\n                mock_print.assert_called_with(mock.ANY)\n                diff_output = mock_print.call_args[0][0]\n                self.assertIn(\"-Initial content\", diff_output)\n                self.assertIn(\"+Modified content\", diff_output)\n\n                # Modify the file again\n                file_path.write_text(\"Further modified content\")\n\n                # Run cmd_commit again\n                commands.cmd_commit()\n\n                # Capture the output of cmd_diff\n                with mock.patch(\"builtins.print\") as mock_print:\n                    commands.cmd_diff(\"\")\n\n                # Check if the diff output is correct\n                mock_print.assert_called_with(mock.ANY)\n                diff_output = mock_print.call_args[0][0]\n                self.assertIn(\"-Modified content\", diff_output)\n                self.assertIn(\"+Further modified content\", diff_output)\n\n                # Modify the file a third time\n                file_path.write_text(\"Final modified content\")\n\n                # Run cmd_commit again\n                commands.cmd_commit()\n\n                # Capture the output of cmd_diff\n                with mock.patch(\"builtins.print\") as mock_print:\n                    commands.cmd_diff(\"\")\n\n                # Check if the diff output is correct\n                mock_print.assert_called_with(mock.ANY)\n                diff_output = mock_print.call_args[0][0]\n                self.assertIn(\"-Further modified content\", diff_output)\n                self.assertIn(\"+Final modified content\", diff_output)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::48",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 131,
      "span_ids": [
        "TestCommands.test_cmd_ask"
      ],
      "start_line": 1635,
      "end_line": 1650,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_ask(self):\n        io = InputOutput(pretty=False, fancy_input=False, yes=True)\n        coder = Coder.create(self.GPT35, None, io)\n        commands = Commands(io, coder)\n\n        question = \"What is the meaning of life?\"\n        canned_reply = \"The meaning of life is 42.\"\n\n        with mock.patch(\"aider.coders.Coder.run\") as mock_run:\n            mock_run.return_value = canned_reply\n\n            with self.assertRaises(SwitchCoder):\n                commands.cmd_ask(question)\n\n            mock_run.assert_called_once()\n            mock_run.assert_called_once_with(question)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::49",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 334,
      "span_ids": [
        "TestCommands.test_cmd_lint_with_dirty_file"
      ],
      "start_line": 1652,
      "end_line": 1688,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_lint_with_dirty_file(self):\n        with GitTemporaryDirectory() as repo_dir:\n            repo = git.Repo(repo_dir)\n            io = InputOutput(pretty=False, fancy_input=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Create and commit a file\n            filename = \"test_file.py\"\n            file_path = Path(repo_dir) / filename\n            file_path.write_text(\"def hello():\\n    print('Hello, World!')\\n\")\n            repo.git.add(filename)\n            repo.git.commit(\"-m\", \"Add test_file.py\")\n\n            # Modify the file to make it dirty\n            file_path.write_text(\"def hello():\\n    print('Hello, World!')\\n\\n# Dirty line\\n\")\n\n            # Mock the linter.lint method\n            with mock.patch.object(coder.linter, \"lint\") as mock_lint:\n                # Set up the mock to return an empty string (no lint errors)\n                mock_lint.return_value = \"\"\n\n                # Run cmd_lint\n                commands.cmd_lint()\n\n                # Check if the linter was called with a filename string\n                # whose Path().name matches the expected filename\n                mock_lint.assert_called_once()\n                called_arg = mock_lint.call_args[0][0]\n                self.assertEqual(Path(called_arg).name, filename)\n\n            # Verify that the file is still dirty after linting\n            self.assertTrue(repo.is_dirty(filename))\n\n            del coder\n            del commands\n            del repo",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::50",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 302,
      "span_ids": [
        "TestCommands.test_cmd_reset"
      ],
      "start_line": 1690,
      "end_line": 1723,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_reset(self):\n        with GitTemporaryDirectory() as repo_dir:\n            io = InputOutput(pretty=False, fancy_input=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Add some files to the chat\n            file1 = Path(repo_dir) / \"file1.txt\"\n            file2 = Path(repo_dir) / \"file2.txt\"\n            file1.write_text(\"Content of file 1\")\n            file2.write_text(\"Content of file 2\")\n            commands.cmd_add(f\"{file1} {file2}\")\n\n            # Add some messages to the chat history\n            coder.cur_messages = [{\"role\": \"user\", \"content\": \"Test message 1\"}]\n            coder.done_messages = [{\"role\": \"assistant\", \"content\": \"Test message 2\"}]\n\n            # Run the reset command\n            commands.cmd_reset(\"\")\n\n            # Check that all files have been dropped\n            self.assertEqual(len(coder.abs_fnames), 0)\n            self.assertEqual(len(coder.abs_read_only_fnames), 0)\n\n            # Check that the chat history has been cleared\n            self.assertEqual(len(coder.cur_messages), 0)\n            self.assertEqual(len(coder.done_messages), 0)\n\n            # Verify that the files still exist in the repository\n            self.assertTrue(file1.exists())\n            self.assertTrue(file2.exists())\n\n            del coder\n            del commands",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_commands.py::51",
    "metadata": {
      "file_path": "tests/basic/test_commands.py",
      "file_name": "test_commands.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 269,
      "span_ids": [
        "TestCommands.test_cmd_load_with_switch_coder"
      ],
      "start_line": 1725,
      "end_line": 1754,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestCommands(TestCase):\n\n    def test_cmd_load_with_switch_coder(self):\n        with GitTemporaryDirectory() as repo_dir:\n            io = InputOutput(pretty=False, fancy_input=False, yes=True)\n            coder = Coder.create(self.GPT35, None, io)\n            commands = Commands(io, coder)\n\n            # Create a temporary file with commands\n            commands_file = Path(repo_dir) / \"test_commands.txt\"\n            commands_file.write_text(\"/ask Tell me about the code\\n/model gpt-4\\n\")\n\n            # Mock run to raise SwitchCoder for /ask and /model\n            def mock_run(cmd):\n                if cmd.startswith((\"/ask\", \"/model\")):\n                    raise SwitchCoder()\n                return None\n\n            with mock.patch.object(commands, \"run\", side_effect=mock_run):\n                # Capture tool_error output\n                with mock.patch.object(io, \"tool_error\") as mock_tool_error:\n                    commands.cmd_load(str(commands_file))\n\n                    # Check that appropriate error messages were shown\n                    mock_tool_error.assert_any_call(\n                        \"Command '/ask Tell me about the code' is only supported in interactive\"\n                        \" mode, skipping.\"\n                    )\n                    mock_tool_error.assert_any_call(\n                        \"Command '/model gpt-4' is only supported in interactive mode, skipping.\"\n                    )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editblock.py::1",
    "metadata": {
      "file_path": "tests/basic/test_editblock.py",
      "file_name": "test_editblock.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 461,
      "span_ids": [
        "TestUtils.setUp",
        "TestUtils",
        "docstring",
        "TestUtils.test_find_filename"
      ],
      "start_line": 1,
      "end_line": 50,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# flake8: noqa: E501\n\nimport tempfile\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import MagicMock, patch\n\nfrom aider.coders import Coder\nfrom aider.coders import editblock_coder as eb\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import InputOutput\nfrom aider.models import Model\nfrom aider.utils import ChdirTemporaryDirectory\n\n\nclass TestUtils(unittest.TestCase):\n    def setUp(self):\n        self.GPT35 = Model(\"gpt-3.5-turbo\")\n\n    def test_find_filename(self):\n        fence = (\"```\", \"```\")\n        valid_fnames = [\"file1.py\", \"file2.py\", \"dir/file3.py\", r\"\\windows\\__init__.py\"]\n\n        # Test with filename on a single line\n        lines = [\"file1.py\", \"```\"]\n        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), \"file1.py\")\n\n        # Test with filename in fence\n        lines = [\"```python\", \"file3.py\", \"```\"]\n        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), \"dir/file3.py\")\n\n        # Test with no valid filename\n        lines = [\"```\", \"invalid_file.py\", \"```\"]\n        self.assertEqual(\"invalid_file.py\", eb.find_filename(lines, fence, valid_fnames))\n\n        # Test with multiple fences\n        lines = [\"```python\", \"file1.py\", \"```\", \"```\", \"file2.py\", \"```\"]\n        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), \"file2.py\")\n\n        # Test with filename having extra characters\n        lines = [\"# file1.py\", \"```\"]\n        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), \"file1.py\")\n\n        # Test with fuzzy matching\n        lines = [\"file1_py\", \"```\"]\n        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), \"file1.py\")\n\n        # Test with fuzzy matching\n        lines = [r\"\\windows__init__.py\", \"```\"]\n        self.assertEqual(eb.find_filename(lines, fence, valid_fnames), r\"\\windows\\__init__.py\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editblock.py::2",
    "metadata": {
      "file_path": "tests/basic/test_editblock.py",
      "file_name": "test_editblock.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 136,
      "span_ids": [
        "TestUtils.test_find_filename",
        "TestUtils.__test_replace_most_similar_chunk"
      ],
      "start_line": 52,
      "end_line": 62,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUtils(unittest.TestCase):\n\n    # fuzzy logic disabled v0.11.2-dev\n    def __test_replace_most_similar_chunk(self):\n        whole = \"This is a sample text.\\nAnother line of text.\\nYet another line.\\n\"\n        part = \"This is a sample text\\n\"\n        replace = \"This is a replaced text.\\n\"\n        expected_output = \"This is a replaced text.\\nAnother line of text.\\nYet another line.\\n\"\n\n        result = eb.replace_most_similar_chunk(whole, part, replace)\n        self.assertEqual(result, expected_output)\n\n    # fuzzy logic disabled v0.11.2-dev\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editblock.py::3",
    "metadata": {
      "file_path": "tests/basic/test_editblock.py",
      "file_name": "test_editblock.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 126,
      "span_ids": [
        "TestUtils.__test_replace_most_similar_chunk_not_perfect_match"
      ],
      "start_line": 63,
      "end_line": 70,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUtils(unittest.TestCase):\n    def __test_replace_most_similar_chunk_not_perfect_match(self):\n        whole = \"This is a sample text.\\nAnother line of text.\\nYet another line.\\n\"\n        part = \"This was a sample text.\\nAnother line of txt\\n\"\n        replace = \"This is a replaced text.\\nModified line of text.\\n\"\n        expected_output = \"This is a replaced text.\\nModified line of text.\\nYet another line.\\n\"\n\n        result = eb.replace_most_similar_chunk(whole, part, replace)\n        self.assertEqual(result, expected_output)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editblock.py::4",
    "metadata": {
      "file_path": "tests/basic/test_editblock.py",
      "file_name": "test_editblock.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 302,
      "span_ids": [
        "TestUtils.test_find_original_update_blocks",
        "TestUtils.test_strip_quoted_wrapping_no_filename",
        "TestUtils.test_strip_quoted_wrapping",
        "TestUtils.test_strip_quoted_wrapping_no_wrapping"
      ],
      "start_line": 72,
      "end_line": 109,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUtils(unittest.TestCase):\n\n    def test_strip_quoted_wrapping(self):\n        input_text = (\n            \"filename.ext\\n```\\nWe just want this content\\nNot the filename and triple quotes\\n```\"\n        )\n        expected_output = \"We just want this content\\nNot the filename and triple quotes\\n\"\n        result = eb.strip_quoted_wrapping(input_text, \"filename.ext\")\n        self.assertEqual(result, expected_output)\n\n    def test_strip_quoted_wrapping_no_filename(self):\n        input_text = \"```\\nWe just want this content\\nNot the triple quotes\\n```\"\n        expected_output = \"We just want this content\\nNot the triple quotes\\n\"\n        result = eb.strip_quoted_wrapping(input_text)\n        self.assertEqual(result, expected_output)\n\n    def test_strip_quoted_wrapping_no_wrapping(self):\n        input_text = \"We just want this content\\nNot the triple quotes\\n\"\n        expected_output = \"We just want this content\\nNot the triple quotes\\n\"\n        result = eb.strip_quoted_wrapping(input_text)\n        self.assertEqual(result, expected_output)\n\n    def test_find_original_update_blocks(self):\n        edit = \"\"\"\nHere's the change:\n\n```text\nfoo.txt\n<<<<<<< SEARCH\nTwo\n=======\nTooooo\n>>>>>>> REPLACE\n```\n\nHope you like it!\n\"\"\"\n\n        edits = list(eb.find_original_update_blocks(edit))\n        self.assertEqual(edits, [(\"foo.txt\", \"Two\\n\", \"Tooooo\\n\")])",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editblock.py::5",
    "metadata": {
      "file_path": "tests/basic/test_editblock.py",
      "file_name": "test_editblock.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 123,
      "span_ids": [
        "TestUtils.test_find_original_update_blocks_mangled_filename_w_source_tag"
      ],
      "start_line": 111,
      "end_line": 132,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUtils(unittest.TestCase):\n\n    def test_find_original_update_blocks_mangled_filename_w_source_tag(self):\n        source = \"source\"\n\n        edit = \"\"\"\nHere's the change:\n\n<%s>foo.txt\n<<<<<<< SEARCH\nOne\n=======\nTwo\n>>>>>>> REPLACE\n</%s>\n\nHope you like it!\n\"\"\" % (source, source)\n\n        fence = (\"<%s>\" % source, \"</%s>\" % source)\n\n        with self.assertRaises(ValueError) as cm:\n            _edits = list(eb.find_original_update_blocks(edit, fence))\n        self.assertIn(\"missing filename\", str(cm.exception))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editblock.py::6",
    "metadata": {
      "file_path": "tests/basic/test_editblock.py",
      "file_name": "test_editblock.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 226,
      "span_ids": [
        "TestUtils.test_find_original_update_blocks_missing_filename",
        "TestUtils.test_find_original_update_blocks_unclosed",
        "TestUtils.test_find_original_update_blocks_quote_below_filename"
      ],
      "start_line": 134,
      "end_line": 188,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUtils(unittest.TestCase):\n\n    def test_find_original_update_blocks_quote_below_filename(self):\n        edit = \"\"\"\nHere's the change:\n\nfoo.txt\n```text\n<<<<<<< SEARCH\nTwo\n=======\nTooooo\n>>>>>>> REPLACE\n```\n\nHope you like it!\n\"\"\"\n\n        edits = list(eb.find_original_update_blocks(edit))\n        self.assertEqual(edits, [(\"foo.txt\", \"Two\\n\", \"Tooooo\\n\")])\n\n    def test_find_original_update_blocks_unclosed(self):\n        edit = \"\"\"\nHere's the change:\n\n```text\nfoo.txt\n<<<<<<< SEARCH\nTwo\n=======\nTooooo\n\n\noops!\n\"\"\"\n\n        with self.assertRaises(ValueError) as cm:\n            list(eb.find_original_update_blocks(edit))\n        self.assertIn(\"Expected `>>>>>>> REPLACE` or `=======`\", str(cm.exception))\n\n    def test_find_original_update_blocks_missing_filename(self):\n        edit = \"\"\"\nHere's the change:\n\n```text\n<<<<<<< SEARCH\nTwo\n=======\nTooooo\n\n\noops!\n\"\"\"\n\n        with self.assertRaises(ValueError) as cm:\n            list(eb.find_original_update_blocks(edit))\n        self.assertIn(\"filename\", str(cm.exception))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editblock.py::7",
    "metadata": {
      "file_path": "tests/basic/test_editblock.py",
      "file_name": "test_editblock.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 250,
      "span_ids": [
        "TestUtils.test_find_original_update_blocks_no_final_newline"
      ],
      "start_line": 190,
      "end_line": 223,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUtils(unittest.TestCase):\n\n    def test_find_original_update_blocks_no_final_newline(self):\n        edit = \"\"\"\naider/coder.py\n<<<<<<< SEARCH\n            self.console.print(\"[red]^C again to quit\")\n=======\n            self.io.tool_error(\"^C again to quit\")\n>>>>>>> REPLACE\n\naider/coder.py\n<<<<<<< SEARCH\n            self.io.tool_error(\"Malformed ORIGINAL/UPDATE blocks, retrying...\")\n            self.io.tool_error(err)\n=======\n            self.io.tool_error(\"Malformed ORIGINAL/UPDATE blocks, retrying...\")\n            self.io.tool_error(str(err))\n>>>>>>> REPLACE\n\naider/coder.py\n<<<<<<< SEARCH\n            self.console.print(\"[red]Unable to get commit message from gpt-3.5-turbo. Use /commit to try again.\\n\")\n=======\n            self.io.tool_error(\"Unable to get commit message from gpt-3.5-turbo. Use /commit to try again.\")\n>>>>>>> REPLACE\n\naider/coder.py\n<<<<<<< SEARCH\n            self.console.print(\"[red]Skipped commit.\")\n=======\n            self.io.tool_error(\"Skipped commit.\")\n>>>>>>> REPLACE\"\"\"\n\n        # Should not raise a ValueError\n        list(eb.find_original_update_blocks(edit))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editblock.py::8",
    "metadata": {
      "file_path": "tests/basic/test_editblock.py",
      "file_name": "test_editblock.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 422,
      "span_ids": [
        "TestUtils.test_incomplete_edit_block_missing_filename"
      ],
      "start_line": 225,
      "end_line": 269,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUtils(unittest.TestCase):\n\n    def test_incomplete_edit_block_missing_filename(self):\n        edit = \"\"\"\nNo problem! Here are the changes to patch `subprocess.check_output` instead of `subprocess.run` in both tests:\n\n```python\ntests/test_repomap.py\n<<<<<<< SEARCH\n    def test_check_for_ctags_failure(self):\n        with patch(\"subprocess.run\") as mock_run:\n            mock_run.side_effect = Exception(\"ctags not found\")\n=======\n    def test_check_for_ctags_failure(self):\n        with patch(\"subprocess.check_output\") as mock_check_output:\n            mock_check_output.side_effect = Exception(\"ctags not found\")\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    def test_check_for_ctags_success(self):\n        with patch(\"subprocess.run\") as mock_run:\n            mock_run.return_value = CompletedProcess(args=[\"ctags\", \"--version\"], returncode=0, stdout='''{\n  \"_type\": \"tag\",\n  \"name\": \"status\",\n  \"path\": \"aider/main.py\",\n  \"pattern\": \"/^    status = main()$/\",\n  \"kind\": \"variable\"\n}''')\n=======\n    def test_check_for_ctags_success(self):\n        with patch(\"subprocess.check_output\") as mock_check_output:\n            mock_check_output.return_value = '''{\n  \"_type\": \"tag\",\n  \"name\": \"status\",\n  \"path\": \"aider/main.py\",\n  \"pattern\": \"/^    status = main()$/\",\n  \"kind\": \"variable\"\n}'''\n>>>>>>> REPLACE\n```\n\nThese changes replace the `subprocess.run` patches with `subprocess.check_output` patches in both `test_check_for_ctags_failure` and `test_check_for_ctags_success` tests.\n\"\"\"\n        edit_blocks = list(eb.find_original_update_blocks(edit))\n        self.assertEqual(len(edit_blocks), 2)  # 2 edits\n        self.assertEqual(edit_blocks[0][0], \"tests/test_repomap.py\")\n        self.assertEqual(edit_blocks[1][0], \"tests/test_repomap.py\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editblock.py::9",
    "metadata": {
      "file_path": "tests/basic/test_editblock.py",
      "file_name": "test_editblock.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 114,
      "span_ids": [
        "TestUtils.test_replace_part_with_missing_varied_leading_whitespace"
      ],
      "start_line": 271,
      "end_line": 289,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUtils(unittest.TestCase):\n\n    def test_replace_part_with_missing_varied_leading_whitespace(self):\n        whole = \"\"\"\n    line1\n    line2\n        line3\n    line4\n\"\"\"\n\n        part = \"line2\\n    line3\\n\"\n        replace = \"new_line2\\n    new_line3\\n\"\n        expected_output = \"\"\"\n    line1\n    new_line2\n        new_line3\n    line4\n\"\"\"\n\n        result = eb.replace_most_similar_chunk(whole, part, replace)\n        self.assertEqual(result, expected_output)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editblock.py::10",
    "metadata": {
      "file_path": "tests/basic/test_editblock.py",
      "file_name": "test_editblock.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 393,
      "span_ids": [
        "TestUtils.test_replace_part_with_just_some_missing_leading_whitespace",
        "TestUtils.test_replace_multiple_matches_missing_whitespace",
        "TestUtils.test_replace_part_with_missing_leading_whitespace",
        "TestUtils.test_replace_multiple_matches"
      ],
      "start_line": 291,
      "end_line": 329,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUtils(unittest.TestCase):\n\n    def test_replace_part_with_missing_leading_whitespace(self):\n        whole = \"    line1\\n    line2\\n    line3\\n\"\n        part = \"line1\\nline2\\n\"\n        replace = \"new_line1\\nnew_line2\\n\"\n        expected_output = \"    new_line1\\n    new_line2\\n    line3\\n\"\n\n        result = eb.replace_most_similar_chunk(whole, part, replace)\n        self.assertEqual(result, expected_output)\n\n    def test_replace_multiple_matches(self):\n        \"only replace first occurrence\"\n\n        whole = \"line1\\nline2\\nline1\\nline3\\n\"\n        part = \"line1\\n\"\n        replace = \"new_line\\n\"\n        expected_output = \"new_line\\nline2\\nline1\\nline3\\n\"\n\n        result = eb.replace_most_similar_chunk(whole, part, replace)\n        self.assertEqual(result, expected_output)\n\n    def test_replace_multiple_matches_missing_whitespace(self):\n        \"only replace first occurrence\"\n\n        whole = \"    line1\\n    line2\\n    line1\\n    line3\\n\"\n        part = \"line1\\n\"\n        replace = \"new_line\\n\"\n        expected_output = \"    new_line\\n    line2\\n    line1\\n    line3\\n\"\n\n        result = eb.replace_most_similar_chunk(whole, part, replace)\n        self.assertEqual(result, expected_output)\n\n    def test_replace_part_with_just_some_missing_leading_whitespace(self):\n        whole = \"    line1\\n    line2\\n    line3\\n\"\n        part = \" line1\\n line2\\n\"\n        replace = \" new_line1\\n     new_line2\\n\"\n        expected_output = \"    new_line1\\n        new_line2\\n    line3\\n\"\n\n        result = eb.replace_most_similar_chunk(whole, part, replace)\n        self.assertEqual(result, expected_output)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editblock.py::11",
    "metadata": {
      "file_path": "tests/basic/test_editblock.py",
      "file_name": "test_editblock.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 165,
      "span_ids": [
        "TestUtils.test_replace_part_with_missing_leading_whitespace_including_blank_line"
      ],
      "start_line": 331,
      "end_line": 343,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUtils(unittest.TestCase):\n\n    def test_replace_part_with_missing_leading_whitespace_including_blank_line(self):\n        \"\"\"\n        The part has leading whitespace on all lines, so should be ignored.\n        But it has a *blank* line with no whitespace at all, which was causing a\n        bug per issue #25. Test case to repro and confirm fix.\n        \"\"\"\n        whole = \"    line1\\n    line2\\n    line3\\n\"\n        part = \"\\n  line1\\n  line2\\n\"\n        replace = \"  new_line1\\n  new_line2\\n\"\n        expected_output = \"    new_line1\\n    new_line2\\n    line3\\n\"\n\n        result = eb.replace_most_similar_chunk(whole, part, replace)\n        self.assertEqual(result, expected_output)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editblock.py::12",
    "metadata": {
      "file_path": "tests/basic/test_editblock.py",
      "file_name": "test_editblock.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 269,
      "span_ids": [
        "TestUtils.test_create_new_file_with_other_file_in_chat"
      ],
      "start_line": 345,
      "end_line": 383,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUtils(unittest.TestCase):\n\n    def test_create_new_file_with_other_file_in_chat(self):\n        # https://github.com/Aider-AI/aider/issues/2258\n        with ChdirTemporaryDirectory():\n            # Create a few temporary files\n            file1 = \"file.txt\"\n\n            with open(file1, \"w\", encoding=\"utf-8\") as f:\n                f.write(\"one\\ntwo\\nthree\\n\")\n\n            files = [file1]\n\n            # Initialize the Coder object with the mocked IO and mocked repo\n            coder = Coder.create(\n                self.GPT35, \"diff\", use_git=False, io=InputOutput(yes=True), fnames=files\n            )\n\n            def mock_send(*args, **kwargs):\n                coder.partial_response_content = f\"\"\"\nDo this:\n\nnewfile.txt\n<<<<<<< SEARCH\n=======\ncreating a new file\n>>>>>>> REPLACE\n\n\"\"\"\n                coder.partial_response_function_call = dict()\n                return []\n\n            coder.send = mock_send\n\n            coder.run(with_message=\"hi\")\n\n            content = Path(file1).read_text(encoding=\"utf-8\")\n            self.assertEqual(content, \"one\\ntwo\\nthree\\n\")\n\n            content = Path(\"newfile.txt\").read_text(encoding=\"utf-8\")\n            self.assertEqual(content, \"creating a new file\\n\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editblock.py::13",
    "metadata": {
      "file_path": "tests/basic/test_editblock.py",
      "file_name": "test_editblock.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 212,
      "span_ids": [
        "TestUtils.test_full_edit"
      ],
      "start_line": 385,
      "end_line": 418,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUtils(unittest.TestCase):\n\n    def test_full_edit(self):\n        # Create a few temporary files\n        _, file1 = tempfile.mkstemp()\n\n        with open(file1, \"w\", encoding=\"utf-8\") as f:\n            f.write(\"one\\ntwo\\nthree\\n\")\n\n        files = [file1]\n\n        # Initialize the Coder object with the mocked IO and mocked repo\n        coder = Coder.create(self.GPT35, \"diff\", io=InputOutput(), fnames=files)\n\n        def mock_send(*args, **kwargs):\n            coder.partial_response_content = f\"\"\"\nDo this:\n\n{Path(file1).name}\n<<<<<<< SEARCH\ntwo\n=======\nnew\n>>>>>>> REPLACE\n\n\"\"\"\n            coder.partial_response_function_call = dict()\n            return []\n\n        coder.send = mock_send\n\n        # Call the run method with a message\n        coder.run(with_message=\"hi\")\n\n        content = Path(file1).read_text(encoding=\"utf-8\")\n        self.assertEqual(content, \"one\\nnew\\nthree\\n\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editblock.py::14",
    "metadata": {
      "file_path": "tests/basic/test_editblock.py",
      "file_name": "test_editblock.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 233,
      "span_ids": [
        "TestUtils.test_full_edit_dry_run"
      ],
      "start_line": 420,
      "end_line": 461,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUtils(unittest.TestCase):\n\n    def test_full_edit_dry_run(self):\n        # Create a few temporary files\n        _, file1 = tempfile.mkstemp()\n\n        orig_content = \"one\\ntwo\\nthree\\n\"\n\n        with open(file1, \"w\", encoding=\"utf-8\") as f:\n            f.write(orig_content)\n\n        files = [file1]\n\n        # Initialize the Coder object with the mocked IO and mocked repo\n        coder = Coder.create(\n            self.GPT35,\n            \"diff\",\n            io=InputOutput(dry_run=True),\n            fnames=files,\n            dry_run=True,\n        )\n\n        def mock_send(*args, **kwargs):\n            coder.partial_response_content = f\"\"\"\nDo this:\n\n{Path(file1).name}\n<<<<<<< SEARCH\ntwo\n=======\nnew\n>>>>>>> REPLACE\n\n\"\"\"\n            coder.partial_response_function_call = dict()\n            return []\n\n        coder.send = mock_send\n\n        # Call the run method with a message\n        coder.run(with_message=\"hi\")\n\n        content = Path(file1).read_text(encoding=\"utf-8\")\n        self.assertEqual(content, orig_content)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editblock.py::15",
    "metadata": {
      "file_path": "tests/basic/test_editblock.py",
      "file_name": "test_editblock.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 212,
      "span_ids": [
        "TestUtils.test_find_original_update_blocks_mupltiple_same_file",
        "TestUtils.test_deepseek_coder_v2_filename_mangling"
      ],
      "start_line": 463,
      "end_line": 520,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUtils(unittest.TestCase):\n\n    def test_find_original_update_blocks_mupltiple_same_file(self):\n        edit = \"\"\"\nHere's the change:\n\n```text\nfoo.txt\n<<<<<<< SEARCH\none\n=======\ntwo\n>>>>>>> REPLACE\n\n...\n\n<<<<<<< SEARCH\nthree\n=======\nfour\n>>>>>>> REPLACE\n```\n\nHope you like it!\n\"\"\"\n\n        edits = list(eb.find_original_update_blocks(edit))\n        self.assertEqual(\n            edits,\n            [\n                (\"foo.txt\", \"one\\n\", \"two\\n\"),\n                (\"foo.txt\", \"three\\n\", \"four\\n\"),\n            ],\n        )\n\n    def test_deepseek_coder_v2_filename_mangling(self):\n        edit = \"\"\"\nHere's the change:\n\n ```python\nfoo.txt\n```\n```python\n<<<<<<< SEARCH\none\n=======\ntwo\n>>>>>>> REPLACE\n```\n\nHope you like it!\n\"\"\"\n\n        edits = list(eb.find_original_update_blocks(edit))\n        self.assertEqual(\n            edits,\n            [\n                (\"foo.txt\", \"one\\n\", \"two\\n\"),\n            ],\n        )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editblock.py::16",
    "metadata": {
      "file_path": "tests/basic/test_editblock.py",
      "file_name": "test_editblock.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 159,
      "span_ids": [
        "impl",
        "TestUtils.test_new_file_created_in_same_folder"
      ],
      "start_line": 522,
      "end_line": 560,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUtils(unittest.TestCase):\n\n    def test_new_file_created_in_same_folder(self):\n        edit = \"\"\"\nHere's the change:\n\npath/to/a/file2.txt\n```python\n<<<<<<< SEARCH\n=======\nthree\n>>>>>>> REPLACE\n```\n\nanother change\n\npath/to/a/file1.txt\n```python\n<<<<<<< SEARCH\none\n=======\ntwo\n>>>>>>> REPLACE\n```\n\nHope you like it!\n\"\"\"\n\n        edits = list(eb.find_original_update_blocks(edit, valid_fnames=[\"path/to/a/file1.txt\"]))\n        self.assertEqual(\n            edits,\n            [\n                (\"path/to/a/file2.txt\", \"\", \"three\\n\"),\n                (\"path/to/a/file1.txt\", \"one\\n\", \"two\\n\"),\n            ],\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editor.py::1",
    "metadata": {
      "file_path": "tests/basic/test_editor.py",
      "file_name": "test_editor.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 169,
      "span_ids": [
        "test_get_environment_editor",
        "imports"
      ],
      "start_line": 1,
      "end_line": 29,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import os\nfrom unittest.mock import MagicMock, patch\n\nimport pytest\n\nfrom aider.editor import (\n    DEFAULT_EDITOR_NIX,\n    DEFAULT_EDITOR_OS_X,\n    DEFAULT_EDITOR_WINDOWS,\n    discover_editor,\n    get_environment_editor,\n    pipe_editor,\n    print_status_message,\n    write_temp_file,\n)\n\n\ndef test_get_environment_editor():\n    # Test with no environment variables set\n    with patch.dict(os.environ, {}, clear=True):\n        assert get_environment_editor(\"default\") == \"default\"\n\n    # Test EDITOR precedence\n    with patch.dict(os.environ, {\"EDITOR\": \"vim\"}):\n        assert get_environment_editor() == \"vim\"\n\n    # Test VISUAL overrides EDITOR\n    with patch.dict(os.environ, {\"EDITOR\": \"vim\", \"VISUAL\": \"code\"}):\n        assert get_environment_editor() == \"code\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editor.py::2",
    "metadata": {
      "file_path": "tests/basic/test_editor.py",
      "file_name": "test_editor.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 132,
      "span_ids": [
        "test_discover_editor_defaults"
      ],
      "start_line": 32,
      "end_line": 47,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def test_discover_editor_defaults():\n    with patch(\"platform.system\") as mock_system:\n        # Test Windows default\n        mock_system.return_value = \"Windows\"\n        with patch.dict(os.environ, {}, clear=True):\n            assert discover_editor() == [DEFAULT_EDITOR_WINDOWS]\n\n        # Test macOS default\n        mock_system.return_value = \"Darwin\"\n        with patch.dict(os.environ, {}, clear=True):\n            assert discover_editor() == [DEFAULT_EDITOR_OS_X]\n\n        # Test Linux default\n        mock_system.return_value = \"Linux\"\n        with patch.dict(os.environ, {}, clear=True):\n            assert discover_editor() == [DEFAULT_EDITOR_NIX]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editor.py::3",
    "metadata": {
      "file_path": "tests/basic/test_editor.py",
      "file_name": "test_editor.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 127,
      "span_ids": [
        "test_write_temp_file"
      ],
      "start_line": 50,
      "end_line": 67,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def test_write_temp_file():\n    # Test basic file creation\n    content = \"test content\"\n    filepath = write_temp_file(content)\n    assert os.path.exists(filepath)\n    with open(filepath, \"r\") as f:\n        assert f.read() == content\n    os.remove(filepath)\n\n    # Test with suffix\n    filepath = write_temp_file(\"content\", suffix=\"txt\")\n    assert filepath.endswith(\".txt\")\n    os.remove(filepath)\n\n    # Test with prefix\n    filepath = write_temp_file(\"content\", prefix=\"test_\")\n    assert os.path.basename(filepath).startswith(\"test_\")\n    os.remove(filepath)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editor.py::4",
    "metadata": {
      "file_path": "tests/basic/test_editor.py",
      "file_name": "test_editor.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 151,
      "span_ids": [
        "test_discover_editor_override",
        "test_print_status_message"
      ],
      "start_line": 70,
      "end_line": 89,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def test_print_status_message(capsys):\n    # Test success message\n    print_status_message(True, \"Success!\")\n    captured = capsys.readouterr()\n    assert \"Success!\" in captured.out\n\n    # Test failure message\n    print_status_message(False, \"Failed!\")\n    captured = capsys.readouterr()\n    assert \"Failed!\" in captured.out\n\n\ndef test_discover_editor_override():\n    # Test editor override\n    assert discover_editor(\"code\") == [\"code\"]\n    assert discover_editor('vim -c \"set noswapfile\"') == [\"vim\", \"-c\", \"set noswapfile\"]\n\n    # Test invalid editor command\n    with pytest.raises(RuntimeError):\n        discover_editor('vim \"unclosed quote')",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_editor.py::5",
    "metadata": {
      "file_path": "tests/basic/test_editor.py",
      "file_name": "test_editor.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 277,
      "span_ids": [
        "test_pipe_editor"
      ],
      "start_line": 92,
      "end_line": 130,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def test_pipe_editor():\n    # Test with default editor\n    test_content = \"Initial content\"\n    modified_content = \"Modified content\"\n\n    # Mock the file operations and editor call\n    with (\n        patch(\"aider.editor.write_temp_file\") as mock_write,\n        patch(\"builtins.open\") as mock_open,\n        patch(\"os.remove\") as mock_remove,\n        patch(\"subprocess.call\") as mock_subprocess,\n    ):\n        # Setup mocks\n        mock_write.return_value = \"temp.txt\"\n        mock_file = MagicMock()\n        mock_file.__enter__.return_value.read.return_value = modified_content\n        mock_open.return_value = mock_file\n\n        # Test with default editor\n        result = pipe_editor(test_content)\n        assert result == modified_content\n        mock_write.assert_called_with(test_content, None)\n        mock_subprocess.assert_called()\n\n        # Test with custom editor\n        result = pipe_editor(test_content, editor=\"code\")\n        assert result == modified_content\n        mock_subprocess.assert_called()\n\n        # Test with suffix\n        result = pipe_editor(test_content, suffix=\"md\")\n        assert result == modified_content\n        mock_write.assert_called_with(test_content, \"md\")\n\n        # Test cleanup on permission error\n        mock_remove.side_effect = PermissionError\n        result = pipe_editor(test_content)\n        assert result == modified_content",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_exceptions.py::1",
    "metadata": {
      "file_path": "tests/basic/test_exceptions.py",
      "file_name": "test_exceptions.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 283,
      "span_ids": [
        "test_get_ex_info",
        "imports",
        "test_litellm_exceptions_load",
        "test_exceptions_tuple"
      ],
      "start_line": 1,
      "end_line": 42,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from aider.exceptions import ExInfo, LiteLLMExceptions\n\n\ndef test_litellm_exceptions_load():\n    \"\"\"Test that LiteLLMExceptions loads without errors\"\"\"\n    ex = LiteLLMExceptions()\n    assert len(ex.exceptions) > 0\n\n\ndef test_exceptions_tuple():\n    \"\"\"Test that exceptions_tuple returns a non-empty tuple\"\"\"\n    ex = LiteLLMExceptions()\n    assert isinstance(ex.exceptions_tuple(), tuple)\n    assert len(ex.exceptions_tuple()) > 0\n\n\ndef test_get_ex_info():\n    \"\"\"Test get_ex_info returns correct ExInfo\"\"\"\n    ex = LiteLLMExceptions()\n\n    # Test with a known exception type\n    from litellm import AuthenticationError\n\n    auth_error = AuthenticationError(\n        message=\"Invalid API key\", llm_provider=\"openai\", model=\"gpt-4\"\n    )\n    ex_info = ex.get_ex_info(auth_error)\n    assert isinstance(ex_info, ExInfo)\n    assert ex_info.name == \"AuthenticationError\"\n    assert ex_info.retry is False\n    assert \"API key\" in ex_info.description\n\n    # Test with unknown exception type\n    class UnknownError(Exception):\n        pass\n\n    unknown = UnknownError()\n    ex_info = ex.get_ex_info(unknown)\n    assert isinstance(ex_info, ExInfo)\n    assert ex_info.name is None\n    assert ex_info.retry is None\n    assert ex_info.description is None",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_exceptions.py::2",
    "metadata": {
      "file_path": "tests/basic/test_exceptions.py",
      "file_name": "test_exceptions.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 181,
      "span_ids": [
        "test_rate_limit_error",
        "test_context_window_error"
      ],
      "start_line": 45,
      "end_line": 66,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def test_rate_limit_error():\n    \"\"\"Test specific handling of RateLimitError\"\"\"\n    ex = LiteLLMExceptions()\n    from litellm import RateLimitError\n\n    rate_error = RateLimitError(message=\"Rate limit exceeded\", llm_provider=\"openai\", model=\"gpt-4\")\n    ex_info = ex.get_ex_info(rate_error)\n    assert ex_info.retry is True\n    assert \"rate limited\" in ex_info.description.lower()\n\n\ndef test_context_window_error():\n    \"\"\"Test specific handling of ContextWindowExceededError\"\"\"\n    ex = LiteLLMExceptions()\n    from litellm import ContextWindowExceededError\n\n    ctx_error = ContextWindowExceededError(\n        message=\"Context length exceeded\", model=\"gpt-4\", llm_provider=\"openai\"\n    )\n    ex_info = ex.get_ex_info(ctx_error)\n    assert ex_info.retry is False",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_find_or_blocks.py::1",
    "metadata": {
      "file_path": "tests/basic/test_find_or_blocks.py",
      "file_name": "test_find_or_blocks.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 605,
      "span_ids": [
        "process_markdown",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 70,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "#!/usr/bin/env python3\n\nimport difflib\nimport io\nimport re\nimport sys\nimport unittest\n\nfrom aider.coders.base_coder import all_fences\nfrom aider.coders.editblock_coder import find_original_update_blocks\nfrom aider.dump import dump  # noqa: F401\n\n\ndef process_markdown(filename, fh):\n    try:\n        with open(filename, \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n    except FileNotFoundError:\n        print(f\"@@@ File '{filename}' not found.\", \"@\" * 20, file=fh, flush=True)\n        return\n    except UnicodeDecodeError:\n        print(\n            f\"@@@ File '{filename}' has an encoding issue. Make sure it's UTF-8 encoded.\",\n            \"@\" * 20,\n            file=fh,\n            flush=True,\n        )\n        return\n\n    # Split the content into sections based on '####' headers\n    sections = re.split(r\"(?=####\\s)\", content)\n\n    for section in sections:\n        if \"editblock_coder.py\" in section or \"test_editblock.py\" in section:\n            continue\n\n        if not section.strip():  # Ignore empty sections\n            continue\n        # Extract the header (if present)\n        header = section.split(\"\\n\")[0].strip()\n        # Get the content (everything after the header)\n        content = \"\".join(section.splitlines(keepends=True)[1:])\n\n        for fence in all_fences[1:] + all_fences[:1]:\n            if \"\\n\" + fence[0] in content:\n                break\n\n        # Process the content with find_original_update_blocks\n        try:\n            blocks = list(find_original_update_blocks(content, fence))\n        except ValueError as e:\n            print(\"\\n\\n@@@\", header, \"@\" * 20, file=fh, flush=True)\n            print(str(e), file=fh, flush=True)\n            continue\n\n        if blocks:\n            print(\"\\n\\n@@@\", header, \"@\" * 20, file=fh, flush=True)\n\n        for block in blocks:\n            if block[0] is None:  # This is a shell command block\n                print(\"@@@ SHELL\", \"@\" * 20, file=fh, flush=True)\n                print(block[1], end=\"\", file=fh, flush=True)\n                print(\"@@@ ENDSHELL\", \"@\" * 20, file=fh, flush=True)\n\n            else:  # This is a SEARCH/REPLACE block\n                print(\"@@@ SEARCH:\", block[0], \"@\" * 20, file=fh, flush=True)\n                print(block[1], end=\"\", file=fh, flush=True)\n                print(\"@\" * 20, file=fh, flush=True)\n                print(block[2], end=\"\", file=fh, flush=True)\n                print(\"@@@ REPLACE\", \"@\" * 20, file=fh, flush=True)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_find_or_blocks.py::2",
    "metadata": {
      "file_path": "tests/basic/test_find_or_blocks.py",
      "file_name": "test_find_or_blocks.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 303,
      "span_ids": [
        "impl",
        "TestFindOrBlocks",
        "TestFindOrBlocks.test_process_markdown"
      ],
      "start_line": 73,
      "end_line": 116,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestFindOrBlocks(unittest.TestCase):\n    def test_process_markdown(self):\n        # Path to the input markdown file\n        input_file = \"tests/fixtures/chat-history.md\"\n\n        # Path to the expected output file\n        expected_output_file = \"tests/fixtures/chat-history-search-replace-gold.txt\"\n\n        # Create a StringIO object to capture the output\n        output = io.StringIO()\n\n        # Run process_markdown\n        process_markdown(input_file, output)\n\n        # Get the actual output\n        actual_output = output.getvalue()\n\n        # Read the expected output\n        with open(expected_output_file, \"r\", encoding=\"utf-8\") as f:\n            expected_output = f.read()\n\n        # Compare the actual and expected outputs\n        if actual_output != expected_output:\n            # If they're different, create a diff\n            diff = difflib.unified_diff(\n                expected_output.splitlines(keepends=True),\n                actual_output.splitlines(keepends=True),\n                fromfile=expected_output_file,\n                tofile=\"actual output\",\n            )\n\n            # Join the diff lines into a string\n            diff_text = \"\".join(diff)\n\n            # Fail the test and show the diff\n            self.fail(f\"Output doesn't match expected output. Diff:\\n{diff_text}\")\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 2:\n        process_markdown(sys.argv[1], sys.stdout)\n    else:\n        unittest.main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_history.py::1",
    "metadata": {
      "file_path": "tests/basic/test_history.py",
      "file_name": "test_history.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 303,
      "span_ids": [
        "TestChatSummary.test_initialization",
        "TestChatSummary.test_tokenize",
        "TestChatSummary.setUp",
        "TestChatSummary.test_too_big",
        "imports",
        "TestChatSummary"
      ],
      "start_line": 1,
      "end_line": 35,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from unittest import TestCase, mock\n\nfrom aider.history import ChatSummary\nfrom aider.models import Model\n\n\nclass TestChatSummary(TestCase):\n    def setUp(self):\n        self.mock_model = mock.Mock(spec=Model)\n        self.mock_model.name = \"gpt-3.5-turbo\"\n        self.mock_model.token_count = lambda msg: len(msg[\"content\"].split())\n        self.mock_model.info = {\"max_input_tokens\": 4096}\n        self.chat_summary = ChatSummary(self.mock_model, max_tokens=100)\n\n    def test_initialization(self):\n        self.assertIsInstance(self.chat_summary, ChatSummary)\n        self.assertEqual(self.chat_summary.max_tokens, 100)\n\n    def test_too_big(self):\n        messages = [\n            {\"role\": \"user\", \"content\": \"This is a short message\"},\n            {\"role\": \"assistant\", \"content\": \"This is also a short message\"},\n        ]\n        self.assertFalse(self.chat_summary.too_big(messages))\n\n        long_message = {\"role\": \"user\", \"content\": \" \".join([\"word\"] * 101)}\n        self.assertTrue(self.chat_summary.too_big([long_message]))\n\n    def test_tokenize(self):\n        messages = [\n            {\"role\": \"user\", \"content\": \"Hello world\"},\n            {\"role\": \"assistant\", \"content\": \"Hi there\"},\n        ]\n        tokenized = self.chat_summary.tokenize(messages)\n        self.assertEqual(tokenized, [(2, messages[0]), (2, messages[1])])",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_history.py::2",
    "metadata": {
      "file_path": "tests/basic/test_history.py",
      "file_name": "test_history.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 140,
      "span_ids": [
        "TestChatSummary.test_summarize_all"
      ],
      "start_line": 37,
      "end_line": 55,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestChatSummary(TestCase):\n\n    @mock.patch(\"aider.history.simple_send_with_retries\")\n    def test_summarize_all(self, mock_send):\n        mock_send.return_value = \"This is a summary\"\n        messages = [\n            {\"role\": \"user\", \"content\": \"Hello world\"},\n            {\"role\": \"assistant\", \"content\": \"Hi there\"},\n        ]\n        summary = self.chat_summary.summarize_all(messages)\n        self.assertEqual(\n            summary,\n            [\n                {\n                    \"role\": \"user\",\n                    \"content\": (\n                        \"I spoke to you previously about a number of things.\\nThis is a summary\"\n                    ),\n                }\n            ],\n        )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_history.py::3",
    "metadata": {
      "file_path": "tests/basic/test_history.py",
      "file_name": "test_history.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 143,
      "span_ids": [
        "TestChatSummary.test_summarize"
      ],
      "start_line": 57,
      "end_line": 70,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestChatSummary(TestCase):\n\n    def test_summarize(self):\n        messages = [{\"role\": \"user\", \"content\": f\"Message {i}\"} for i in range(10)]\n        messages.extend([{\"role\": \"assistant\", \"content\": f\"Response {i}\"} for i in range(10)])\n\n        with mock.patch.object(\n            self.chat_summary,\n            \"summarize_all\",\n            return_value=[{\"role\": \"user\", \"content\": \"Summary\"}],\n        ):\n            result = self.chat_summary.summarize(messages)\n\n        self.assertIsInstance(result, list)\n        self.assertGreater(len(result), 0)\n        self.assertLessEqual(len(result), len(messages))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_history.py::4",
    "metadata": {
      "file_path": "tests/basic/test_history.py",
      "file_name": "test_history.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 313,
      "span_ids": [
        "TestChatSummary.test_fallback_to_second_model"
      ],
      "start_line": 72,
      "end_line": 110,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestChatSummary(TestCase):\n\n    @mock.patch(\"aider.history.simple_send_with_retries\")\n    def test_fallback_to_second_model(self, mock_send):\n        mock_model1 = mock.Mock(spec=Model)\n        mock_model1.name = \"gpt-4\"\n        mock_model2 = mock.Mock(spec=Model)\n        mock_model2.name = \"gpt-3.5-turbo\"\n\n        chat_summary = ChatSummary([mock_model1, mock_model2], max_tokens=100)\n\n        # Make the first model fail\n        mock_send.side_effect = [Exception(\"Model 1 failed\"), \"Summary from Model 2\"]\n\n        messages = [\n            {\"role\": \"user\", \"content\": \"Hello world\"},\n            {\"role\": \"assistant\", \"content\": \"Hi there\"},\n        ]\n\n        summary = chat_summary.summarize_all(messages)\n\n        # Check that both models were tried\n        self.assertEqual(mock_send.call_count, 2)\n\n        # Check that the calls were made with the correct models\n        self.assertEqual(mock_send.call_args_list[0][0][0], mock_model1)\n        self.assertEqual(mock_send.call_args_list[1][0][0], mock_model2)\n\n        # Check that we got a summary from the second model\n        self.assertEqual(\n            summary,\n            [\n                {\n                    \"role\": \"user\",\n                    \"content\": (\n                        \"I spoke to you previously about a number of things.\\nSummary from Model 2\"\n                    ),\n                }\n            ],\n        )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_io.py::1",
    "metadata": {
      "file_path": "tests/basic/test_io.py",
      "file_name": "test_io.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 337,
      "span_ids": [
        "TestInputOutput.test_no_color_environment_variable",
        "TestInputOutput.test_line_endings_validation",
        "TestInputOutput.test_dumb_terminal",
        "TestInputOutput",
        "imports"
      ],
      "start_line": 1,
      "end_line": 42,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import os\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import MagicMock, patch\n\nfrom prompt_toolkit.completion import CompleteEvent\nfrom prompt_toolkit.document import Document\n\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import AutoCompleter, ConfirmGroup, InputOutput\nfrom aider.utils import ChdirTemporaryDirectory\n\n\nclass TestInputOutput(unittest.TestCase):\n    def test_line_endings_validation(self):\n        # Test valid line endings\n        for ending in [\"platform\", \"lf\", \"crlf\"]:\n            io = InputOutput(line_endings=ending)\n            self.assertEqual(\n                io.newline, None if ending == \"platform\" else \"\\n\" if ending == \"lf\" else \"\\r\\n\"\n            )\n\n        # Test invalid line endings\n        with self.assertRaises(ValueError) as cm:\n            io = InputOutput(line_endings=\"invalid\")\n        self.assertIn(\"Invalid line_endings value: invalid\", str(cm.exception))\n        # Check each valid option is in the error message\n        self.assertIn(\"platform\", str(cm.exception))\n        self.assertIn(\"crlf\", str(cm.exception))\n        self.assertIn(\"lf\", str(cm.exception))\n\n    def test_no_color_environment_variable(self):\n        with patch.dict(os.environ, {\"NO_COLOR\": \"1\"}):\n            io = InputOutput(fancy_input=False)\n            self.assertFalse(io.pretty)\n\n    def test_dumb_terminal(self):\n        with patch.dict(os.environ, {\"TERM\": \"dumb\"}):\n            io = InputOutput(fancy_input=True)\n            self.assertTrue(io.is_dumb_terminal)\n            self.assertFalse(io.pretty)\n            self.assertIsNone(io.prompt_session)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_io.py::2",
    "metadata": {
      "file_path": "tests/basic/test_io.py",
      "file_name": "test_io.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 435,
      "span_ids": [
        "TestInputOutput.test_autocompleter_get_command_completions"
      ],
      "start_line": 44,
      "end_line": 98,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestInputOutput(unittest.TestCase):\n\n    def test_autocompleter_get_command_completions(self):\n        # Step 3: Mock the commands object\n        commands = MagicMock()\n        commands.get_commands.return_value = [\"/help\", \"/add\", \"/drop\"]\n        commands.matching_commands.side_effect = lambda inp: (\n            [cmd for cmd in commands.get_commands() if cmd.startswith(inp.strip().split()[0])],\n            inp.strip().split()[0],\n            \" \".join(inp.strip().split()[1:]),\n        )\n        commands.get_raw_completions.return_value = None\n        commands.get_completions.side_effect = lambda cmd: (\n            [\"file1.txt\", \"file2.txt\"] if cmd == \"/add\" else None\n        )\n\n        # Step 4: Create an instance of AutoCompleter\n        root = \"\"\n        rel_fnames = []\n        addable_rel_fnames = []\n        autocompleter = AutoCompleter(\n            root=root,\n            rel_fnames=rel_fnames,\n            addable_rel_fnames=addable_rel_fnames,\n            commands=commands,\n            encoding=\"utf-8\",\n        )\n\n        # Step 5: Set up test cases\n        test_cases = [\n            # Input text, Expected completion texts\n            (\"/\", [\"/help\", \"/add\", \"/drop\"]),\n            (\"/a\", [\"/add\"]),\n            (\"/add f\", [\"file1.txt\", \"file2.txt\"]),\n        ]\n\n        # Step 6: Iterate through test cases\n        for text, expected_completions in test_cases:\n            document = Document(text=text)\n            complete_event = CompleteEvent()\n            words = text.strip().split()\n\n            # Call get_command_completions\n            completions = list(\n                autocompleter.get_command_completions(\n                    document,\n                    complete_event,\n                    text,\n                    words,\n                )\n            )\n\n            # Extract completion texts\n            completion_texts = [comp.text for comp in completions]\n\n            # Assert that the completions match expected results\n            self.assertEqual(set(completion_texts), set(expected_completions))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_io.py::3",
    "metadata": {
      "file_path": "tests/basic/test_io.py",
      "file_name": "test_io.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 373,
      "span_ids": [
        "TestInputOutput.test_autocompleter_with_unicode_file",
        "TestInputOutput.test_autocompleter_with_non_existent_file"
      ],
      "start_line": 100,
      "end_line": 130,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestInputOutput(unittest.TestCase):\n\n    def test_autocompleter_with_non_existent_file(self):\n        root = \"\"\n        rel_fnames = [\"non_existent_file.txt\"]\n        addable_rel_fnames = []\n        commands = None\n        autocompleter = AutoCompleter(root, rel_fnames, addable_rel_fnames, commands, \"utf-8\")\n        self.assertEqual(autocompleter.words, set(rel_fnames))\n\n    def test_autocompleter_with_unicode_file(self):\n        with ChdirTemporaryDirectory():\n            root = \"\"\n            fname = \"file.py\"\n            rel_fnames = [fname]\n            addable_rel_fnames = []\n            commands = None\n            autocompleter = AutoCompleter(root, rel_fnames, addable_rel_fnames, commands, \"utf-8\")\n            self.assertEqual(autocompleter.words, set(rel_fnames))\n\n            Path(fname).write_text(\"def hello(): pass\\n\")\n            autocompleter = AutoCompleter(root, rel_fnames, addable_rel_fnames, commands, \"utf-8\")\n            autocompleter.tokenize()\n            dump(autocompleter.words)\n            self.assertEqual(autocompleter.words, set(rel_fnames + [(\"hello\", \"`hello`\")]))\n\n            encoding = \"utf-16\"\n            some_content_which_will_error_if_read_with_encoding_utf8 = \"\u00c5\u00cd\u00ce\u00cf\".encode(encoding)\n            with open(fname, \"wb\") as f:\n                f.write(some_content_which_will_error_if_read_with_encoding_utf8)\n\n            autocompleter = AutoCompleter(root, rel_fnames, addable_rel_fnames, commands, \"utf-8\")\n            self.assertEqual(autocompleter.words, set(rel_fnames))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_io.py::4",
    "metadata": {
      "file_path": "tests/basic/test_io.py",
      "file_name": "test_io.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 150,
      "span_ids": [
        "TestInputOutput.test_get_input_is_a_directory_error"
      ],
      "start_line": 132,
      "end_line": 144,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestInputOutput(unittest.TestCase):\n\n    @patch(\"builtins.input\", return_value=\"test input\")\n    def test_get_input_is_a_directory_error(self, mock_input):\n        io = InputOutput(pretty=False, fancy_input=False)  # Windows tests throw UnicodeDecodeError\n        root = \"/\"\n        rel_fnames = [\"existing_file.txt\"]\n        addable_rel_fnames = [\"new_file.txt\"]\n        commands = MagicMock()\n\n        # Simulate IsADirectoryError\n        with patch(\"aider.io.open\", side_effect=IsADirectoryError):\n            result = io.get_input(root, rel_fnames, addable_rel_fnames, commands)\n            self.assertEqual(result, \"test input\")\n            mock_input.assert_called_once()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_io.py::5",
    "metadata": {
      "file_path": "tests/basic/test_io.py",
      "file_name": "test_io.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 270,
      "span_ids": [
        "TestInputOutput.test_confirm_ask_explicit_yes_required"
      ],
      "start_line": 146,
      "end_line": 176,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestInputOutput(unittest.TestCase):\n\n    @patch(\"builtins.input\")\n    def test_confirm_ask_explicit_yes_required(self, mock_input):\n        io = InputOutput(pretty=False, fancy_input=False)\n\n        # Test case 1: explicit_yes_required=True, self.yes=True\n        io.yes = True\n        result = io.confirm_ask(\"Are you sure?\", explicit_yes_required=True)\n        self.assertFalse(result)\n        mock_input.assert_not_called()\n\n        # Test case 2: explicit_yes_required=True, self.yes=False\n        io.yes = False\n        result = io.confirm_ask(\"Are you sure?\", explicit_yes_required=True)\n        self.assertFalse(result)\n        mock_input.assert_not_called()\n\n        # Test case 3: explicit_yes_required=True, user input required\n        io.yes = None\n        mock_input.return_value = \"y\"\n        result = io.confirm_ask(\"Are you sure?\", explicit_yes_required=True)\n        self.assertTrue(result)\n        mock_input.assert_called_once()\n\n        # Reset mock_input\n        mock_input.reset_mock()\n\n        # Test case 4: explicit_yes_required=False, self.yes=True\n        io.yes = True\n        result = io.confirm_ask(\"Are you sure?\", explicit_yes_required=False)\n        self.assertTrue(result)\n        mock_input.assert_not_called()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_io.py::6",
    "metadata": {
      "file_path": "tests/basic/test_io.py",
      "file_name": "test_io.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 374,
      "span_ids": [
        "TestInputOutput.test_confirm_ask_with_group"
      ],
      "start_line": 178,
      "end_line": 218,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestInputOutput(unittest.TestCase):\n\n    @patch(\"builtins.input\")\n    def test_confirm_ask_with_group(self, mock_input):\n        io = InputOutput(pretty=False, fancy_input=False)\n        group = ConfirmGroup()\n\n        # Test case 1: No group preference, user selects 'All'\n        mock_input.return_value = \"a\"\n        result = io.confirm_ask(\"Are you sure?\", group=group)\n        self.assertTrue(result)\n        self.assertEqual(group.preference, \"all\")\n        mock_input.assert_called_once()\n        mock_input.reset_mock()\n\n        # Test case 2: Group preference is 'All', should not prompt\n        result = io.confirm_ask(\"Are you sure?\", group=group)\n        self.assertTrue(result)\n        mock_input.assert_not_called()\n\n        # Test case 3: No group preference, user selects 'Skip all'\n        group.preference = None\n        mock_input.return_value = \"s\"\n        result = io.confirm_ask(\"Are you sure?\", group=group)\n        self.assertFalse(result)\n        self.assertEqual(group.preference, \"skip\")\n        mock_input.assert_called_once()\n        mock_input.reset_mock()\n\n        # Test case 4: Group preference is 'Skip all', should not prompt\n        result = io.confirm_ask(\"Are you sure?\", group=group)\n        self.assertFalse(result)\n        mock_input.assert_not_called()\n\n        # Test case 5: explicit_yes_required=True, should not offer 'All' option\n        group.preference = None\n        mock_input.return_value = \"y\"\n        result = io.confirm_ask(\"Are you sure?\", group=group, explicit_yes_required=True)\n        self.assertTrue(result)\n        self.assertIsNone(group.preference)\n        mock_input.assert_called_once()\n        self.assertNotIn(\"(A)ll\", mock_input.call_args[0][0])\n        mock_input.reset_mock()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_io.py::7",
    "metadata": {
      "file_path": "tests/basic/test_io.py",
      "file_name": "test_io.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 190,
      "span_ids": [
        "TestInputOutput.test_confirm_ask_yes_no"
      ],
      "start_line": 220,
      "end_line": 243,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestInputOutput(unittest.TestCase):\n\n    @patch(\"builtins.input\")\n    def test_confirm_ask_yes_no(self, mock_input):\n        io = InputOutput(pretty=False, fancy_input=False)\n\n        # Test case 1: User selects 'Yes'\n        mock_input.return_value = \"y\"\n        result = io.confirm_ask(\"Are you sure?\")\n        self.assertTrue(result)\n        mock_input.assert_called_once()\n        mock_input.reset_mock()\n\n        # Test case 2: User selects 'No'\n        mock_input.return_value = \"n\"\n        result = io.confirm_ask(\"Are you sure?\")\n        self.assertFalse(result)\n        mock_input.assert_called_once()\n        mock_input.reset_mock()\n\n        # Test case 3: Empty input (default to Yes)\n        mock_input.return_value = \"\"\n        result = io.confirm_ask(\"Are you sure?\")\n        self.assertTrue(result)\n        mock_input.assert_called_once()\n        mock_input.reset_mock()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_io.py::8",
    "metadata": {
      "file_path": "tests/basic/test_io.py",
      "file_name": "test_io.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 384,
      "span_ids": [
        "TestInputOutput.test_confirm_ask_allow_never"
      ],
      "start_line": 245,
      "end_line": 284,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestInputOutput(unittest.TestCase):\n\n    @patch(\"builtins.input\", side_effect=[\"d\"])\n    def test_confirm_ask_allow_never(self, mock_input):\n        \"\"\"Test the 'don't ask again' functionality in confirm_ask\"\"\"\n        io = InputOutput(pretty=False, fancy_input=False)\n\n        # First call: user selects \"Don't ask again\"\n        result = io.confirm_ask(\"Are you sure?\", allow_never=True)\n        self.assertFalse(result)\n        mock_input.assert_called_once()\n        self.assertIn((\"Are you sure?\", None), io.never_prompts)\n\n        # Reset the mock to check for further calls\n        mock_input.reset_mock()\n\n        # Second call: should not prompt, immediately return False\n        result = io.confirm_ask(\"Are you sure?\", allow_never=True)\n        self.assertFalse(result)\n        mock_input.assert_not_called()\n\n        # Test with subject parameter\n        mock_input.reset_mock()\n        mock_input.side_effect = [\"d\"]\n        result = io.confirm_ask(\"Confirm action?\", subject=\"Subject Text\", allow_never=True)\n        self.assertFalse(result)\n        mock_input.assert_called_once()\n        self.assertIn((\"Confirm action?\", \"Subject Text\"), io.never_prompts)\n\n        # Subsequent call with the same question and subject\n        mock_input.reset_mock()\n        result = io.confirm_ask(\"Confirm action?\", subject=\"Subject Text\", allow_never=True)\n        self.assertFalse(result)\n        mock_input.assert_not_called()\n\n        # Test that allow_never=False does not add to never_prompts\n        mock_input.reset_mock()\n        mock_input.side_effect = [\"d\", \"n\"]\n        result = io.confirm_ask(\"Do you want to proceed?\", allow_never=False)\n        self.assertFalse(result)\n        self.assertEqual(mock_input.call_count, 2)\n        self.assertNotIn((\"Do you want to proceed?\", None), io.never_prompts)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_io.py::9",
    "metadata": {
      "file_path": "tests/basic/test_io.py",
      "file_name": "test_io.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 348,
      "span_ids": [
        "TestInputOutputMultilineMode",
        "TestInputOutputMultilineMode.test_tool_message_unicode_fallback",
        "TestInputOutputMultilineMode.test_toggle_multiline_mode",
        "TestInputOutputMultilineMode.setUp",
        "impl"
      ],
      "start_line": 287,
      "end_line": 331,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestInputOutputMultilineMode(unittest.TestCase):\n    def setUp(self):\n        self.io = InputOutput(fancy_input=True)\n        self.io.prompt_session = MagicMock()\n\n    def test_toggle_multiline_mode(self):\n        \"\"\"Test that toggling multiline mode works correctly\"\"\"\n        # Start in single-line mode\n        self.io.multiline_mode = False\n\n        # Toggle to multiline mode\n        self.io.toggle_multiline_mode()\n        self.assertTrue(self.io.multiline_mode)\n\n        # Toggle back to single-line mode\n        self.io.toggle_multiline_mode()\n        self.assertFalse(self.io.multiline_mode)\n\n    def test_tool_message_unicode_fallback(self):\n        \"\"\"Test that Unicode messages are properly converted to ASCII with replacement\"\"\"\n        io = InputOutput(pretty=False, fancy_input=False)\n\n        # Create a message with invalid Unicode that can't be encoded in UTF-8\n        # Using a surrogate pair that's invalid in UTF-8\n        invalid_unicode = \"Hello \\ud800World\"\n\n        # Mock console.print to capture the output\n        with patch.object(io.console, \"print\") as mock_print:\n            # First call will raise UnicodeEncodeError\n            mock_print.side_effect = [UnicodeEncodeError(\"utf-8\", \"\", 0, 1, \"invalid\"), None]\n\n            io._tool_message(invalid_unicode)\n\n            # Verify that the message was converted to ASCII with replacement\n            self.assertEqual(mock_print.call_count, 2)\n            args, kwargs = mock_print.call_args\n            converted_message = args[0]\n\n            # The invalid Unicode should be replaced with '?'\n            self.assertEqual(converted_message, \"Hello ?World\")\n\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_linter.py::1",
    "metadata": {
      "file_path": "tests/basic/test_linter.py",
      "file_name": "test_linter.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 398,
      "span_ids": [
        "TestLinter.test_init",
        "TestLinter.test_run_cmd",
        "TestLinter.test_set_linter",
        "TestLinter.test_get_rel_fname",
        "TestLinter",
        "imports",
        "TestLinter.setUp",
        "TestLinter.test_run_cmd_with_errors",
        "impl"
      ],
      "start_line": 1,
      "end_line": 53,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import unittest\nfrom unittest.mock import MagicMock, patch\n\nfrom aider.dump import dump  # noqa\nfrom aider.linter import Linter\n\n\nclass TestLinter(unittest.TestCase):\n    def setUp(self):\n        self.linter = Linter(encoding=\"utf-8\", root=\"/test/root\")\n\n    def test_init(self):\n        self.assertEqual(self.linter.encoding, \"utf-8\")\n        self.assertEqual(self.linter.root, \"/test/root\")\n        self.assertIn(\"python\", self.linter.languages)\n\n    def test_set_linter(self):\n        self.linter.set_linter(\"javascript\", \"eslint\")\n        self.assertEqual(self.linter.languages[\"javascript\"], \"eslint\")\n\n    def test_get_rel_fname(self):\n        import os\n\n        self.assertEqual(self.linter.get_rel_fname(\"/test/root/file.py\"), \"file.py\")\n        expected_path = os.path.normpath(\"../../other/path/file.py\")\n        actual_path = os.path.normpath(self.linter.get_rel_fname(\"/other/path/file.py\"))\n        self.assertEqual(actual_path, expected_path)\n\n    @patch(\"subprocess.Popen\")\n    def test_run_cmd(self, mock_popen):\n        mock_process = MagicMock()\n        mock_process.returncode = 0\n        mock_process.stdout.read.side_effect = (\"\", None)\n        mock_popen.return_value = mock_process\n\n        result = self.linter.run_cmd(\"test_cmd\", \"test_file.py\", \"code\")\n        self.assertIsNone(result)\n\n    @patch(\"subprocess.Popen\")\n    def test_run_cmd_with_errors(self, mock_popen):\n        mock_process = MagicMock()\n        mock_process.returncode = 1\n        mock_process.stdout.read.side_effect = (\"Error message\", None)\n        mock_popen.return_value = mock_process\n\n        result = self.linter.run_cmd(\"test_cmd\", \"test_file.py\", \"code\")\n        self.assertIsNotNone(result)\n        self.assertIn(\"Error message\", result.text)\n\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::1",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 314,
      "span_ids": [
        "imports",
        "TestMain.setUp",
        "TestMain"
      ],
      "start_line": 1,
      "end_line": 37,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import json\nimport os\nimport subprocess\nimport tempfile\nfrom io import StringIO\nfrom pathlib import Path\nfrom unittest import TestCase\nfrom unittest.mock import MagicMock, patch\n\nimport git\nfrom prompt_toolkit.input import DummyInput\nfrom prompt_toolkit.output import DummyOutput\n\nfrom aider.coders import Coder\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import InputOutput\nfrom aider.main import check_gitignore, main, setup_git\nfrom aider.utils import GitTemporaryDirectory, IgnorantTemporaryDirectory, make_repo\n\n\nclass TestMain(TestCase):\n    def setUp(self):\n        self.original_env = os.environ.copy()\n        os.environ[\"OPENAI_API_KEY\"] = \"deadbeef\"\n        os.environ[\"AIDER_CHECK_UPDATE\"] = \"false\"\n        os.environ[\"AIDER_ANALYTICS\"] = \"false\"\n        self.original_cwd = os.getcwd()\n        self.tempdir_obj = IgnorantTemporaryDirectory()\n        self.tempdir = self.tempdir_obj.name\n        os.chdir(self.tempdir)\n        # Fake home directory prevents tests from using the real ~/.aider.conf.yml file:\n        self.homedir_obj = IgnorantTemporaryDirectory()\n        os.environ[\"HOME\"] = self.homedir_obj.name\n        self.input_patcher = patch(\"builtins.input\", return_value=None)\n        self.mock_input = self.input_patcher.start()\n        self.webbrowser_patcher = patch(\"aider.io.webbrowser.open\")\n        self.mock_webbrowser = self.webbrowser_patcher.start()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::2",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 365,
      "span_ids": [
        "TestMain.test_main_with_empty_git_dir_new_file",
        "TestMain.test_main_with_empty_dir_no_files_on_command",
        "TestMain.test_main_with_dname_and_fname",
        "TestMain.test_main_with_emptqy_dir_new_file",
        "TestMain.tearDown",
        "TestMain.test_main_with_empty_git_dir_new_files"
      ],
      "start_line": 39,
      "end_line": 73,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def tearDown(self):\n        os.chdir(self.original_cwd)\n        self.tempdir_obj.cleanup()\n        self.homedir_obj.cleanup()\n        os.environ.clear()\n        os.environ.update(self.original_env)\n        self.input_patcher.stop()\n        self.webbrowser_patcher.stop()\n\n    def test_main_with_empty_dir_no_files_on_command(self):\n        main([\"--no-git\", \"--exit\", \"--yes\"], input=DummyInput(), output=DummyOutput())\n\n    def test_main_with_emptqy_dir_new_file(self):\n        main([\"foo.txt\", \"--yes\", \"--no-git\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n        self.assertTrue(os.path.exists(\"foo.txt\"))\n\n    @patch(\"aider.repo.GitRepo.get_commit_message\", return_value=\"mock commit message\")\n    def test_main_with_empty_git_dir_new_file(self, _):\n        make_repo()\n        main([\"--yes\", \"foo.txt\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n        self.assertTrue(os.path.exists(\"foo.txt\"))\n\n    @patch(\"aider.repo.GitRepo.get_commit_message\", return_value=\"mock commit message\")\n    def test_main_with_empty_git_dir_new_files(self, _):\n        make_repo()\n        main([\"--yes\", \"foo.txt\", \"bar.txt\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n        self.assertTrue(os.path.exists(\"foo.txt\"))\n        self.assertTrue(os.path.exists(\"bar.txt\"))\n\n    def test_main_with_dname_and_fname(self):\n        subdir = Path(\"subdir\")\n        subdir.mkdir()\n        make_repo(str(subdir))\n        res = main([\"subdir\", \"foo.txt\"], input=DummyInput(), output=DummyOutput())\n        self.assertNotEqual(res, None)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::3",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 126,
      "span_ids": [
        "TestMain.test_main_with_subdir_repo_fnames"
      ],
      "start_line": 75,
      "end_line": 86,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    @patch(\"aider.repo.GitRepo.get_commit_message\", return_value=\"mock commit message\")\n    def test_main_with_subdir_repo_fnames(self, _):\n        subdir = Path(\"subdir\")\n        subdir.mkdir()\n        make_repo(str(subdir))\n        main(\n            [\"--yes\", str(subdir / \"foo.txt\"), str(subdir / \"bar.txt\"), \"--exit\"],\n            input=DummyInput(),\n            output=DummyOutput(),\n        )\n        self.assertTrue((subdir / \"foo.txt\").exists())\n        self.assertTrue((subdir / \"bar.txt\").exists())",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::4",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 156,
      "span_ids": [
        "TestMain.test_main_with_git_config_yml"
      ],
      "start_line": 88,
      "end_line": 101,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_main_with_git_config_yml(self):\n        make_repo()\n\n        Path(\".aider.conf.yml\").write_text(\"auto-commits: false\\n\")\n        with patch(\"aider.coders.Coder.create\") as MockCoder:\n            main([\"--yes\"], input=DummyInput(), output=DummyOutput())\n            _, kwargs = MockCoder.call_args\n            assert kwargs[\"auto_commits\"] is False\n\n        Path(\".aider.conf.yml\").write_text(\"auto-commits: true\\n\")\n        with patch(\"aider.coders.Coder.create\") as MockCoder:\n            main([], input=DummyInput(), output=DummyOutput())\n            _, kwargs = MockCoder.call_args\n            assert kwargs[\"auto_commits\"] is True",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::5",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 254,
      "span_ids": [
        "TestMain.test_setup_git",
        "TestMain.test_main_with_empty_git_dir_new_subdir_file"
      ],
      "start_line": 103,
      "end_line": 127,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_main_with_empty_git_dir_new_subdir_file(self):\n        make_repo()\n        subdir = Path(\"subdir\")\n        subdir.mkdir()\n        fname = subdir / \"foo.txt\"\n        fname.touch()\n        subprocess.run([\"git\", \"add\", str(subdir)])\n        subprocess.run([\"git\", \"commit\", \"-m\", \"added\"])\n\n        # This will throw a git error on windows if get_tracked_files doesn't\n        # properly convert git/posix/paths to git\\posix\\paths.\n        # Because aider will try and `git add` a file that's already in the repo.\n        main([\"--yes\", str(fname), \"--exit\"], input=DummyInput(), output=DummyOutput())\n\n    def test_setup_git(self):\n        io = InputOutput(pretty=False, yes=True)\n        git_root = setup_git(None, io)\n        git_root = Path(git_root).resolve()\n        self.assertEqual(git_root, Path(self.tempdir).resolve())\n\n        self.assertTrue(git.Repo(self.tempdir))\n\n        gitignore = Path.cwd() / \".gitignore\"\n        self.assertTrue(gitignore.exists())\n        self.assertEqual(\".aider*\", gitignore.read_text().splitlines()[0])",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::6",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 214,
      "span_ids": [
        "TestMain.test_check_gitignore"
      ],
      "start_line": 129,
      "end_line": 153,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_check_gitignore(self):\n        with GitTemporaryDirectory():\n            os.environ[\"GIT_CONFIG_GLOBAL\"] = \"globalgitconfig\"\n\n            io = InputOutput(pretty=False, yes=True)\n            cwd = Path.cwd()\n            gitignore = cwd / \".gitignore\"\n\n            self.assertFalse(gitignore.exists())\n            check_gitignore(cwd, io)\n            self.assertTrue(gitignore.exists())\n\n            self.assertEqual(\".aider*\", gitignore.read_text().splitlines()[0])\n\n            # Test without .env file present\n            gitignore.write_text(\"one\\ntwo\\n\")\n            check_gitignore(cwd, io)\n            self.assertEqual(\"one\\ntwo\\n.aider*\\n\", gitignore.read_text())\n\n            # Test with .env file present\n            env_file = cwd / \".env\"\n            env_file.touch()\n            check_gitignore(cwd, io)\n            self.assertEqual(\"one\\ntwo\\n.aider*\\n.env\\n\", gitignore.read_text())\n            del os.environ[\"GIT_CONFIG_GLOBAL\"]",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::7",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 296,
      "span_ids": [
        "TestMain.test_main_args"
      ],
      "start_line": 155,
      "end_line": 182,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_main_args(self):\n        with patch(\"aider.coders.Coder.create\") as MockCoder:\n            # --yes will just ok the git repo without blocking on input\n            # following calls to main will see the new repo already\n            main([\"--no-auto-commits\", \"--yes\"], input=DummyInput())\n            _, kwargs = MockCoder.call_args\n            assert kwargs[\"auto_commits\"] is False\n\n        with patch(\"aider.coders.Coder.create\") as MockCoder:\n            main([\"--auto-commits\"], input=DummyInput())\n            _, kwargs = MockCoder.call_args\n            assert kwargs[\"auto_commits\"] is True\n\n        with patch(\"aider.coders.Coder.create\") as MockCoder:\n            main([], input=DummyInput())\n            _, kwargs = MockCoder.call_args\n            assert kwargs[\"dirty_commits\"] is True\n            assert kwargs[\"auto_commits\"] is True\n\n        with patch(\"aider.coders.Coder.create\") as MockCoder:\n            main([\"--no-dirty-commits\"], input=DummyInput())\n            _, kwargs = MockCoder.call_args\n            assert kwargs[\"dirty_commits\"] is False\n\n        with patch(\"aider.coders.Coder.create\") as MockCoder:\n            main([\"--dirty-commits\"], input=DummyInput())\n            _, kwargs = MockCoder.call_args\n            assert kwargs[\"dirty_commits\"] is True",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::8",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 280,
      "span_ids": [
        "TestMain.test_env_file_override"
      ],
      "start_line": 184,
      "end_line": 214,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_env_file_override(self):\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n            git_env = git_dir / \".env\"\n\n            fake_home = git_dir / \"fake_home\"\n            fake_home.mkdir()\n            os.environ[\"HOME\"] = str(fake_home)\n            home_env = fake_home / \".env\"\n\n            cwd = git_dir / \"subdir\"\n            cwd.mkdir()\n            os.chdir(cwd)\n            cwd_env = cwd / \".env\"\n\n            named_env = git_dir / \"named.env\"\n\n            os.environ[\"E\"] = \"existing\"\n            home_env.write_text(\"A=home\\nB=home\\nC=home\\nD=home\")\n            git_env.write_text(\"A=git\\nB=git\\nC=git\")\n            cwd_env.write_text(\"A=cwd\\nB=cwd\")\n            named_env.write_text(\"A=named\")\n\n            with patch(\"pathlib.Path.home\", return_value=fake_home):\n                main([\"--yes\", \"--exit\", \"--env-file\", str(named_env)])\n\n            self.assertEqual(os.environ[\"A\"], \"named\")\n            self.assertEqual(os.environ[\"B\"], \"cwd\")\n            self.assertEqual(os.environ[\"C\"], \"git\")\n            self.assertEqual(os.environ[\"D\"], \"home\")\n            self.assertEqual(os.environ[\"E\"], \"existing\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::9",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 143,
      "span_ids": [
        "TestMain.test_message_file_flag"
      ],
      "start_line": 216,
      "end_line": 231,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_message_file_flag(self):\n        message_file_content = \"This is a test message from a file.\"\n        message_file_path = tempfile.mktemp()\n        with open(message_file_path, \"w\", encoding=\"utf-8\") as message_file:\n            message_file.write(message_file_content)\n\n        with patch(\"aider.coders.Coder.create\") as MockCoder:\n            MockCoder.return_value.run = MagicMock()\n            main(\n                [\"--yes\", \"--message-file\", message_file_path],\n                input=DummyInput(),\n                output=DummyOutput(),\n            )\n            MockCoder.return_value.run.assert_called_once_with(with_message=message_file_content)\n\n        os.remove(message_file_path)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::10",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 120,
      "span_ids": [
        "TestMain.test_encodings_arg"
      ],
      "start_line": 233,
      "end_line": 246,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_encodings_arg(self):\n        fname = \"foo.py\"\n\n        with GitTemporaryDirectory():\n            with patch(\"aider.coders.Coder.create\") as MockCoder:  # noqa: F841\n                with patch(\"aider.main.InputOutput\") as MockSend:\n\n                    def side_effect(*args, **kwargs):\n                        self.assertEqual(kwargs[\"encoding\"], \"iso-8859-15\")\n                        return MagicMock()\n\n                    MockSend.side_effect = side_effect\n\n                    main([\"--yes\", fname, \"--encoding\", \"iso-8859-15\"])",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::11",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 339,
      "span_ids": [
        "TestMain.test_yes",
        "TestMain.test_main_message_adds_to_input_history",
        "TestMain.test_main_exit_calls_version_check",
        "TestMain.test_default_yes"
      ],
      "start_line": 248,
      "end_line": 284,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_main_exit_calls_version_check(self):\n        with GitTemporaryDirectory():\n            with (\n                patch(\"aider.main.check_version\") as mock_check_version,\n                patch(\"aider.main.InputOutput\") as mock_input_output,\n            ):\n                main([\"--exit\", \"--check-update\"], input=DummyInput(), output=DummyOutput())\n                mock_check_version.assert_called_once()\n                mock_input_output.assert_called_once()\n\n    @patch(\"aider.main.InputOutput\")\n    @patch(\"aider.coders.base_coder.Coder.run\")\n    def test_main_message_adds_to_input_history(self, mock_run, MockInputOutput):\n        test_message = \"test message\"\n        mock_io_instance = MockInputOutput.return_value\n\n        main([\"--message\", test_message], input=DummyInput(), output=DummyOutput())\n\n        mock_io_instance.add_to_input_history.assert_called_once_with(test_message)\n\n    @patch(\"aider.main.InputOutput\")\n    @patch(\"aider.coders.base_coder.Coder.run\")\n    def test_yes(self, mock_run, MockInputOutput):\n        test_message = \"test message\"\n\n        main([\"--yes\", \"--message\", test_message])\n        args, kwargs = MockInputOutput.call_args\n        self.assertTrue(args[1])\n\n    @patch(\"aider.main.InputOutput\")\n    @patch(\"aider.coders.base_coder.Coder.run\")\n    def test_default_yes(self, mock_run, MockInputOutput):\n        test_message = \"test message\"\n\n        main([\"--message\", test_message])\n        args, kwargs = MockInputOutput.call_args\n        self.assertEqual(args[1], None)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::12",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 129,
      "span_ids": [
        "TestMain.test_dark_mode_sets_code_theme"
      ],
      "start_line": 286,
      "end_line": 295,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_dark_mode_sets_code_theme(self):\n        # Mock InputOutput to capture the configuration\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            MockInputOutput.return_value.get_input.return_value = None\n            main([\"--dark-mode\", \"--no-git\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n            # Ensure InputOutput was called\n            MockInputOutput.assert_called_once()\n            # Check if the code_theme setting is for dark mode\n            _, kwargs = MockInputOutput.call_args\n            self.assertEqual(kwargs[\"code_theme\"], \"monokai\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::13",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 166,
      "span_ids": [
        "TestMain.test_light_mode_sets_code_theme",
        "TestMain.create_env_file"
      ],
      "start_line": 297,
      "end_line": 311,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_light_mode_sets_code_theme(self):\n        # Mock InputOutput to capture the configuration\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            MockInputOutput.return_value.get_input.return_value = None\n            main([\"--light-mode\", \"--no-git\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n            # Ensure InputOutput was called\n            MockInputOutput.assert_called_once()\n            # Check if the code_theme setting is for light mode\n            _, kwargs = MockInputOutput.call_args\n            self.assertEqual(kwargs[\"code_theme\"], \"default\")\n\n    def create_env_file(self, file_name, content):\n        env_file_path = Path(self.tempdir) / file_name\n        env_file_path.write_text(content)\n        return env_file_path",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::14",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 157,
      "span_ids": [
        "TestMain.test_env_file_flag_sets_automatic_variable"
      ],
      "start_line": 313,
      "end_line": 326,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_env_file_flag_sets_automatic_variable(self):\n        env_file_path = self.create_env_file(\".env.test\", \"AIDER_DARK_MODE=True\")\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            MockInputOutput.return_value.get_input.return_value = None\n            MockInputOutput.return_value.get_input.confirm_ask = True\n            main(\n                [\"--env-file\", str(env_file_path), \"--no-git\", \"--exit\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n            )\n            MockInputOutput.assert_called_once()\n            # Check if the color settings are for dark mode\n            _, kwargs = MockInputOutput.call_args\n            self.assertEqual(kwargs[\"code_theme\"], \"monokai\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::15",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 145,
      "span_ids": [
        "TestMain.test_default_env_file_sets_automatic_variable"
      ],
      "start_line": 328,
      "end_line": 338,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_default_env_file_sets_automatic_variable(self):\n        self.create_env_file(\".env\", \"AIDER_DARK_MODE=True\")\n        with patch(\"aider.main.InputOutput\") as MockInputOutput:\n            MockInputOutput.return_value.get_input.return_value = None\n            MockInputOutput.return_value.get_input.confirm_ask = True\n            main([\"--no-git\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n            # Ensure InputOutput was called\n            MockInputOutput.assert_called_once()\n            # Check if the color settings are for dark mode\n            _, kwargs = MockInputOutput.call_args\n            self.assertEqual(kwargs[\"code_theme\"], \"monokai\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::16",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 185,
      "span_ids": [
        "TestMain.test_false_vals_in_env_file",
        "TestMain.test_true_vals_in_env_file"
      ],
      "start_line": 340,
      "end_line": 354,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_false_vals_in_env_file(self):\n        self.create_env_file(\".env\", \"AIDER_SHOW_DIFFS=off\")\n        with patch(\"aider.coders.Coder.create\") as MockCoder:\n            main([\"--no-git\", \"--yes\"], input=DummyInput(), output=DummyOutput())\n            MockCoder.assert_called_once()\n            _, kwargs = MockCoder.call_args\n            self.assertEqual(kwargs[\"show_diffs\"], False)\n\n    def test_true_vals_in_env_file(self):\n        self.create_env_file(\".env\", \"AIDER_SHOW_DIFFS=on\")\n        with patch(\"aider.coders.Coder.create\") as MockCoder:\n            main([\"--no-git\", \"--yes\"], input=DummyInput(), output=DummyOutput())\n            MockCoder.assert_called_once()\n            _, kwargs = MockCoder.call_args\n            self.assertEqual(kwargs[\"show_diffs\"], True)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::17",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 281,
      "span_ids": [
        "TestMain.test_lint_option"
      ],
      "start_line": 356,
      "end_line": 387,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_lint_option(self):\n        with GitTemporaryDirectory() as git_dir:\n            # Create a dirty file in the root\n            dirty_file = Path(\"dirty_file.py\")\n            dirty_file.write_text(\"def foo():\\n    return 'bar'\")\n\n            repo = git.Repo(\".\")\n            repo.git.add(str(dirty_file))\n            repo.git.commit(\"-m\", \"new\")\n\n            dirty_file.write_text(\"def foo():\\n    return '!!!!!'\")\n\n            # Create a subdirectory\n            subdir = Path(git_dir) / \"subdir\"\n            subdir.mkdir()\n\n            # Change to the subdirectory\n            os.chdir(subdir)\n\n            # Mock the Linter class\n            with patch(\"aider.linter.Linter.lint\") as MockLinter:\n                MockLinter.return_value = \"\"\n\n                # Run main with --lint option\n                main([\"--lint\", \"--yes\"])\n\n                # Check if the Linter was called with a filename ending in \"dirty_file.py\"\n                # but not ending in \"subdir/dirty_file.py\"\n                MockLinter.assert_called_once()\n                called_arg = MockLinter.call_args[0][0]\n                self.assertTrue(called_arg.endswith(\"dirty_file.py\"))\n                self.assertFalse(called_arg.endswith(f\"subdir{os.path.sep}dirty_file.py\"))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::18",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 202,
      "span_ids": [
        "TestMain.test_verbose_mode_lists_env_vars"
      ],
      "start_line": 389,
      "end_line": 406,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_verbose_mode_lists_env_vars(self):\n        self.create_env_file(\".env\", \"AIDER_DARK_MODE=on\")\n        with patch(\"sys.stdout\", new_callable=StringIO) as mock_stdout:\n            main(\n                [\"--no-git\", \"--verbose\", \"--exit\", \"--yes\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n            )\n            output = mock_stdout.getvalue()\n            relevant_output = \"\\n\".join(\n                line\n                for line in output.splitlines()\n                if \"AIDER_DARK_MODE\" in line or \"dark_mode\" in line\n            )  # this bit just helps failing assertions to be easier to read\n            self.assertIn(\"AIDER_DARK_MODE\", relevant_output)\n            self.assertIn(\"dark_mode\", relevant_output)\n            self.assertRegex(relevant_output, r\"AIDER_DARK_MODE:\\s+on\")\n            self.assertRegex(relevant_output, r\"dark_mode:\\s+True\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::19",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 624,
      "span_ids": [
        "TestMain.test_yaml_config_file_loading"
      ],
      "start_line": 408,
      "end_line": 467,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_yaml_config_file_loading(self):\n        with GitTemporaryDirectory() as git_dir:\n            git_dir = Path(git_dir)\n\n            # Create fake home directory\n            fake_home = git_dir / \"fake_home\"\n            fake_home.mkdir()\n            os.environ[\"HOME\"] = str(fake_home)\n\n            # Create subdirectory as current working directory\n            cwd = git_dir / \"subdir\"\n            cwd.mkdir()\n            os.chdir(cwd)\n\n            # Create .aider.conf.yml files in different locations\n            home_config = fake_home / \".aider.conf.yml\"\n            git_config = git_dir / \".aider.conf.yml\"\n            cwd_config = cwd / \".aider.conf.yml\"\n            named_config = git_dir / \"named.aider.conf.yml\"\n\n            cwd_config.write_text(\"model: gpt-4-32k\\nmap-tokens: 4096\\n\")\n            git_config.write_text(\"model: gpt-4\\nmap-tokens: 2048\\n\")\n            home_config.write_text(\"model: gpt-3.5-turbo\\nmap-tokens: 1024\\n\")\n            named_config.write_text(\"model: gpt-4-1106-preview\\nmap-tokens: 8192\\n\")\n\n            with (\n                patch(\"pathlib.Path.home\", return_value=fake_home),\n                patch(\"aider.coders.Coder.create\") as MockCoder,\n            ):\n                # Test loading from specified config file\n                main(\n                    [\"--yes\", \"--exit\", \"--config\", str(named_config)],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                _, kwargs = MockCoder.call_args\n                self.assertEqual(kwargs[\"main_model\"].name, \"gpt-4-1106-preview\")\n                self.assertEqual(kwargs[\"map_tokens\"], 8192)\n\n                # Test loading from current working directory\n                main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n                _, kwargs = MockCoder.call_args\n                print(\"kwargs:\", kwargs)  # Add this line for debugging\n                self.assertIn(\"main_model\", kwargs, \"main_model key not found in kwargs\")\n                self.assertEqual(kwargs[\"main_model\"].name, \"gpt-4-32k\")\n                self.assertEqual(kwargs[\"map_tokens\"], 4096)\n\n                # Test loading from git root\n                cwd_config.unlink()\n                main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n                _, kwargs = MockCoder.call_args\n                self.assertEqual(kwargs[\"main_model\"].name, \"gpt-4\")\n                self.assertEqual(kwargs[\"map_tokens\"], 2048)\n\n                # Test loading from home directory\n                git_config.unlink()\n                main([\"--yes\", \"--exit\"], input=DummyInput(), output=DummyOutput())\n                _, kwargs = MockCoder.call_args\n                self.assertEqual(kwargs[\"main_model\"].name, \"gpt-3.5-turbo\")\n                self.assertEqual(kwargs[\"map_tokens\"], 1024)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::20",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 290,
      "span_ids": [
        "TestMain.test_map_tokens_option_with_non_zero_value",
        "TestMain.test_map_tokens_option",
        "TestMain.test_read_option"
      ],
      "start_line": 469,
      "end_line": 503,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_map_tokens_option(self):\n        with GitTemporaryDirectory():\n            with patch(\"aider.coders.base_coder.RepoMap\") as MockRepoMap:\n                MockRepoMap.return_value.max_map_tokens = 0\n                main(\n                    [\"--model\", \"gpt-4\", \"--map-tokens\", \"0\", \"--exit\", \"--yes\"],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                MockRepoMap.assert_not_called()\n\n    def test_map_tokens_option_with_non_zero_value(self):\n        with GitTemporaryDirectory():\n            with patch(\"aider.coders.base_coder.RepoMap\") as MockRepoMap:\n                MockRepoMap.return_value.max_map_tokens = 1000\n                main(\n                    [\"--model\", \"gpt-4\", \"--map-tokens\", \"1000\", \"--exit\", \"--yes\"],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                MockRepoMap.assert_called_once()\n\n    def test_read_option(self):\n        with GitTemporaryDirectory():\n            test_file = \"test_file.txt\"\n            Path(test_file).touch()\n\n            coder = main(\n                [\"--read\", test_file, \"--exit\", \"--yes\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n                return_coder=True,\n            )\n\n            self.assertIn(str(Path(test_file).resolve()), coder.abs_read_only_fnames)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::21",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 139,
      "span_ids": [
        "TestMain.test_read_option_with_external_file"
      ],
      "start_line": 505,
      "end_line": 522,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_read_option_with_external_file(self):\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as external_file:\n            external_file.write(\"External file content\")\n            external_file_path = external_file.name\n\n        try:\n            with GitTemporaryDirectory():\n                coder = main(\n                    [\"--read\", external_file_path, \"--exit\", \"--yes\"],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                    return_coder=True,\n                )\n\n                real_external_file_path = os.path.realpath(external_file_path)\n                self.assertIn(real_external_file_path, coder.abs_read_only_fnames)\n        finally:\n            os.unlink(external_file_path)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::22",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 152,
      "span_ids": [
        "TestMain.test_model_metadata_file"
      ],
      "start_line": 524,
      "end_line": 546,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_model_metadata_file(self):\n        with GitTemporaryDirectory():\n            metadata_file = Path(\".aider.model.metadata.json\")\n\n            # must be a fully qualified model name: provider/...\n            metadata_content = {\"deepseek/deepseek-chat\": {\"max_input_tokens\": 1234}}\n            metadata_file.write_text(json.dumps(metadata_content))\n\n            coder = main(\n                [\n                    \"--model\",\n                    \"deepseek/deepseek-chat\",\n                    \"--model-metadata-file\",\n                    str(metadata_file),\n                    \"--exit\",\n                    \"--yes\",\n                ],\n                input=DummyInput(),\n                output=DummyOutput(),\n                return_coder=True,\n            )\n\n            self.assertEqual(coder.main_model.info[\"max_input_tokens\"], 1234)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::23",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 159,
      "span_ids": [
        "TestMain.test_sonnet_and_cache_options"
      ],
      "start_line": 548,
      "end_line": 565,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_sonnet_and_cache_options(self):\n        with GitTemporaryDirectory():\n            with patch(\"aider.coders.base_coder.RepoMap\") as MockRepoMap:\n                mock_repo_map = MagicMock()\n                mock_repo_map.max_map_tokens = 1000  # Set a specific value\n                MockRepoMap.return_value = mock_repo_map\n\n                main(\n                    [\"--sonnet\", \"--cache-prompts\", \"--exit\", \"--yes\"],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n\n                MockRepoMap.assert_called_once()\n                call_args, call_kwargs = MockRepoMap.call_args\n                self.assertEqual(\n                    call_kwargs.get(\"refresh\"), \"files\"\n                )  # Check the 'refresh' keyword argument\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::24",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 768,
      "span_ids": [
        "TestMain.test_suggest_shell_commands_enabled",
        "TestMain.test_suggest_shell_commands_default",
        "TestMain.test_suggest_shell_commands_disabled",
        "TestMain.test_detect_urls_disabled",
        "TestMain.test_return_coder",
        "TestMain.test_detect_urls_enabled",
        "TestMain.test_4o_and_cache_options",
        "TestMain.test_sonnet_and_cache_prompts_options",
        "TestMain.test_set_env_single",
        "TestMain.test_detect_urls_default",
        "TestMain.test_map_mul_option",
        "TestMain.test_pytest_env_vars"
      ],
      "start_line": 567,
      "end_line": 686,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_sonnet_and_cache_prompts_options(self):\n        with GitTemporaryDirectory():\n            coder = main(\n                [\"--sonnet\", \"--cache-prompts\", \"--exit\", \"--yes\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n                return_coder=True,\n            )\n\n            self.assertTrue(coder.add_cache_headers)\n\n    def test_4o_and_cache_options(self):\n        with GitTemporaryDirectory():\n            coder = main(\n                [\"--4o\", \"--cache-prompts\", \"--exit\", \"--yes\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n                return_coder=True,\n            )\n\n            self.assertFalse(coder.add_cache_headers)\n\n    def test_return_coder(self):\n        with GitTemporaryDirectory():\n            result = main(\n                [\"--exit\", \"--yes\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n                return_coder=True,\n            )\n            self.assertIsInstance(result, Coder)\n\n            result = main(\n                [\"--exit\", \"--yes\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n                return_coder=False,\n            )\n            self.assertIsNone(result)\n\n    def test_map_mul_option(self):\n        with GitTemporaryDirectory():\n            coder = main(\n                [\"--map-mul\", \"5\", \"--exit\", \"--yes\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n                return_coder=True,\n            )\n            self.assertIsInstance(coder, Coder)\n            self.assertEqual(coder.repo_map.map_mul_no_files, 5)\n\n    def test_suggest_shell_commands_default(self):\n        with GitTemporaryDirectory():\n            coder = main(\n                [\"--exit\", \"--yes\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n                return_coder=True,\n            )\n            self.assertTrue(coder.suggest_shell_commands)\n\n    def test_suggest_shell_commands_disabled(self):\n        with GitTemporaryDirectory():\n            coder = main(\n                [\"--no-suggest-shell-commands\", \"--exit\", \"--yes\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n                return_coder=True,\n            )\n            self.assertFalse(coder.suggest_shell_commands)\n\n    def test_suggest_shell_commands_enabled(self):\n        with GitTemporaryDirectory():\n            coder = main(\n                [\"--suggest-shell-commands\", \"--exit\", \"--yes\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n                return_coder=True,\n            )\n            self.assertTrue(coder.suggest_shell_commands)\n\n    def test_detect_urls_default(self):\n        with GitTemporaryDirectory():\n            coder = main(\n                [\"--exit\", \"--yes\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n                return_coder=True,\n            )\n            self.assertTrue(coder.detect_urls)\n\n    def test_detect_urls_disabled(self):\n        with GitTemporaryDirectory():\n            coder = main(\n                [\"--no-detect-urls\", \"--exit\", \"--yes\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n                return_coder=True,\n            )\n            self.assertFalse(coder.detect_urls)\n\n    def test_detect_urls_enabled(self):\n        with GitTemporaryDirectory():\n            coder = main(\n                [\"--detect-urls\", \"--exit\", \"--yes\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n                return_coder=True,\n            )\n            self.assertTrue(coder.detect_urls)\n\n    def test_pytest_env_vars(self):\n        # Verify that environment variables from pytest.ini are properly set\n        self.assertEqual(os.environ.get(\"AIDER_ANALYTICS\"), \"false\")\n\n    def test_set_env_single(self):\n        # Test setting a single environment variable\n        with GitTemporaryDirectory():\n            main([\"--set-env\", \"TEST_VAR=test_value\", \"--exit\", \"--yes\"])\n            self.assertEqual(os.environ.get(\"TEST_VAR\"), \"test_value\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::25",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 406,
      "span_ids": [
        "TestMain.test_api_key_invalid_format",
        "TestMain.test_set_env_with_spaces",
        "TestMain.test_api_key_multiple",
        "TestMain.test_api_key_single",
        "TestMain.test_set_env_invalid_format",
        "TestMain.test_set_env_multiple"
      ],
      "start_line": 688,
      "end_line": 733,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_set_env_multiple(self):\n        # Test setting multiple environment variables\n        with GitTemporaryDirectory():\n            main(\n                [\n                    \"--set-env\",\n                    \"TEST_VAR1=value1\",\n                    \"--set-env\",\n                    \"TEST_VAR2=value2\",\n                    \"--exit\",\n                    \"--yes\",\n                ]\n            )\n            self.assertEqual(os.environ.get(\"TEST_VAR1\"), \"value1\")\n            self.assertEqual(os.environ.get(\"TEST_VAR2\"), \"value2\")\n\n    def test_set_env_with_spaces(self):\n        # Test setting env var with spaces in value\n        with GitTemporaryDirectory():\n            main([\"--set-env\", \"TEST_VAR=test value with spaces\", \"--exit\", \"--yes\"])\n            self.assertEqual(os.environ.get(\"TEST_VAR\"), \"test value with spaces\")\n\n    def test_set_env_invalid_format(self):\n        # Test invalid format handling\n        with GitTemporaryDirectory():\n            result = main([\"--set-env\", \"INVALID_FORMAT\", \"--exit\", \"--yes\"])\n            self.assertEqual(result, 1)\n\n    def test_api_key_single(self):\n        # Test setting a single API key\n        with GitTemporaryDirectory():\n            main([\"--api-key\", \"anthropic=test-key\", \"--exit\", \"--yes\"])\n            self.assertEqual(os.environ.get(\"ANTHROPIC_API_KEY\"), \"test-key\")\n\n    def test_api_key_multiple(self):\n        # Test setting multiple API keys\n        with GitTemporaryDirectory():\n            main([\"--api-key\", \"anthropic=key1\", \"--api-key\", \"openai=key2\", \"--exit\", \"--yes\"])\n            self.assertEqual(os.environ.get(\"ANTHROPIC_API_KEY\"), \"key1\")\n            self.assertEqual(os.environ.get(\"OPENAI_API_KEY\"), \"key2\")\n\n    def test_api_key_invalid_format(self):\n        # Test invalid format handling\n        with GitTemporaryDirectory():\n            result = main([\"--api-key\", \"INVALID_FORMAT\", \"--exit\", \"--yes\"])\n            self.assertEqual(result, 1)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::26",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 132,
      "span_ids": [
        "TestMain.test_invalid_edit_format"
      ],
      "start_line": 735,
      "end_line": 746,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_invalid_edit_format(self):\n        with GitTemporaryDirectory():\n            with patch(\"aider.io.InputOutput.offer_url\") as mock_offer_url:\n                result = main(\n                    [\"--edit-format\", \"not-a-real-format\", \"--exit\", \"--yes\"],\n                    input=DummyInput(),\n                    output=DummyOutput(),\n                )\n                self.assertEqual(result, 1)  # main() should return 1 on error\n                mock_offer_url.assert_called_once()\n                args, _ = mock_offer_url.call_args\n                self.assertEqual(args[0], \"https://aider.chat/docs/more/edit-formats.html\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_main.py::27",
    "metadata": {
      "file_path": "tests/basic/test_main.py",
      "file_name": "test_main.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 190,
      "span_ids": [
        "TestMain.test_chat_language_spanish",
        "TestMain.test_main_exit_with_git_command_not_found"
      ],
      "start_line": 748,
      "end_line": 769,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestMain(TestCase):\n\n    def test_chat_language_spanish(self):\n        with GitTemporaryDirectory():\n            coder = main(\n                [\"--chat-language\", \"Spanish\", \"--exit\", \"--yes\"],\n                input=DummyInput(),\n                output=DummyOutput(),\n                return_coder=True,\n            )\n            system_info = coder.get_platform_info()\n            self.assertIn(\"Spanish\", system_info)\n\n    @patch(\"git.Repo.init\")\n    def test_main_exit_with_git_command_not_found(self, mock_git_init):\n        mock_git_init.side_effect = git.exc.GitCommandNotFound(\"git\", \"Command 'git' not found\")\n\n        try:\n            result = main([\"--exit\", \"--yes\"], input=DummyInput(), output=DummyOutput())\n        except Exception as e:\n            self.fail(f\"main() raised an unexpected exception: {e}\")\n\n        self.assertIsNone(result, \"main() should return None when called with --exit\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_models.py::1",
    "metadata": {
      "file_path": "tests/basic/test_models.py",
      "file_name": "test_models.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 280,
      "span_ids": [
        "TestModels.test_get_model_info_nonexistent",
        "imports",
        "TestModels.test_max_context_tokens",
        "TestModels"
      ],
      "start_line": 1,
      "end_line": 37,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import unittest\nfrom unittest.mock import ANY, MagicMock, patch\n\nfrom aider.models import (\n    ANTHROPIC_BETA_HEADER,\n    Model,\n    ModelInfoManager,\n    register_models,\n    sanity_check_model,\n    sanity_check_models,\n)\n\n\nclass TestModels(unittest.TestCase):\n    def test_get_model_info_nonexistent(self):\n        manager = ModelInfoManager()\n        info = manager.get_model_info(\"non-existent-model\")\n        self.assertEqual(info, {})\n\n    def test_max_context_tokens(self):\n        model = Model(\"gpt-3.5-turbo\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 16385)\n\n        model = Model(\"gpt-3.5-turbo-16k\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 16385)\n\n        model = Model(\"gpt-3.5-turbo-1106\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 16385)\n\n        model = Model(\"gpt-4\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 8 * 1024)\n\n        model = Model(\"gpt-4-32k\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 32 * 1024)\n\n        model = Model(\"gpt-4-0613\")\n        self.assertEqual(model.info[\"max_input_tokens\"], 8 * 1024)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_models.py::2",
    "metadata": {
      "file_path": "tests/basic/test_models.py",
      "file_name": "test_models.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 149,
      "span_ids": [
        "TestModels.test_sanity_check_model_all_set"
      ],
      "start_line": 39,
      "end_line": 54,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestModels(unittest.TestCase):\n\n    @patch(\"os.environ\")\n    def test_sanity_check_model_all_set(self, mock_environ):\n        mock_environ.get.return_value = \"dummy_value\"\n        mock_io = MagicMock()\n        model = MagicMock()\n        model.name = \"test-model\"\n        model.missing_keys = [\"API_KEY1\", \"API_KEY2\"]\n        model.keys_in_environment = True\n        model.info = {\"some\": \"info\"}\n\n        sanity_check_model(mock_io, model)\n\n        mock_io.tool_output.assert_called()\n        calls = mock_io.tool_output.call_args_list\n        self.assertIn(\"- API_KEY1: Set\", str(calls))\n        self.assertIn(\"- API_KEY2: Set\", str(calls))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_models.py::3",
    "metadata": {
      "file_path": "tests/basic/test_models.py",
      "file_name": "test_models.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 148,
      "span_ids": [
        "TestModels.test_sanity_check_model_not_set"
      ],
      "start_line": 56,
      "end_line": 71,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestModels(unittest.TestCase):\n\n    @patch(\"os.environ\")\n    def test_sanity_check_model_not_set(self, mock_environ):\n        mock_environ.get.return_value = \"\"\n        mock_io = MagicMock()\n        model = MagicMock()\n        model.name = \"test-model\"\n        model.missing_keys = [\"API_KEY1\", \"API_KEY2\"]\n        model.keys_in_environment = True\n        model.info = {\"some\": \"info\"}\n\n        sanity_check_model(mock_io, model)\n\n        mock_io.tool_output.assert_called()\n        calls = mock_io.tool_output.call_args_list\n        self.assertIn(\"- API_KEY1: Not set\", str(calls))\n        self.assertIn(\"- API_KEY2: Not set\", str(calls))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_models.py::4",
    "metadata": {
      "file_path": "tests/basic/test_models.py",
      "file_name": "test_models.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 201,
      "span_ids": [
        "TestModels.test_sanity_check_models_bogus_editor"
      ],
      "start_line": 73,
      "end_line": 93,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestModels(unittest.TestCase):\n\n    def test_sanity_check_models_bogus_editor(self):\n        mock_io = MagicMock()\n        main_model = Model(\"gpt-4\")\n        main_model.editor_model = Model(\"bogus-model\")\n\n        result = sanity_check_models(mock_io, main_model)\n\n        self.assertTrue(\n            result\n        )  # Should return True because there's a problem with the editor model\n        mock_io.tool_warning.assert_called_with(ANY)  # Ensure a warning was issued\n\n        warning_messages = [\n            warning_call.args[0] for warning_call in mock_io.tool_warning.call_args_list\n        ]\n        print(\"Warning messages:\", warning_messages)  # Add this line\n\n        self.assertGreaterEqual(mock_io.tool_warning.call_count, 1)  # Expect two warnings\n        self.assertTrue(\n            any(\"bogus-model\" in msg for msg in warning_messages)\n        )  # Check that one of the warnings mentions the bogus model\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_models.py::5",
    "metadata": {
      "file_path": "tests/basic/test_models.py",
      "file_name": "test_models.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 254,
      "span_ids": [
        "TestModels.test_model_aliases"
      ],
      "start_line": 95,
      "end_line": 123,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestModels(unittest.TestCase):\n\n    def test_model_aliases(self):\n        # Test common aliases\n        model = Model(\"4\")\n        self.assertEqual(model.name, \"gpt-4-0613\")\n\n        model = Model(\"4o\")\n        self.assertEqual(model.name, \"gpt-4o\")\n\n        model = Model(\"35turbo\")\n        self.assertEqual(model.name, \"gpt-3.5-turbo\")\n\n        model = Model(\"35-turbo\")\n        self.assertEqual(model.name, \"gpt-3.5-turbo\")\n\n        model = Model(\"3\")\n        self.assertEqual(model.name, \"gpt-3.5-turbo\")\n\n        model = Model(\"sonnet\")\n        self.assertEqual(model.name, \"claude-3-5-sonnet-20241022\")\n\n        model = Model(\"haiku\")\n        self.assertEqual(model.name, \"claude-3-5-haiku-20241022\")\n\n        model = Model(\"opus\")\n        self.assertEqual(model.name, \"claude-3-opus-20240229\")\n\n        # Test non-alias passes through unchanged\n        model = Model(\"gpt-4\")\n        self.assertEqual(model.name, \"gpt-4\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_models.py::6",
    "metadata": {
      "file_path": "tests/basic/test_models.py",
      "file_name": "test_models.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 315,
      "span_ids": [
        "TestModels.test_o1_use_temp_false",
        "TestModels.test_get_repo_map_tokens"
      ],
      "start_line": 125,
      "end_line": 158,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestModels(unittest.TestCase):\n\n    def test_o1_use_temp_false(self):\n        # Test GitHub Copilot models\n        model = Model(\"github/o1-mini\")\n        self.assertEqual(model.name, \"github/o1-mini\")\n        self.assertEqual(model.use_temperature, False)\n\n        model = Model(\"github/o1-preview\")\n        self.assertEqual(model.name, \"github/o1-preview\")\n        self.assertEqual(model.use_temperature, False)\n\n    def test_get_repo_map_tokens(self):\n        # Test default case (no max_input_tokens in info)\n        model = Model(\"gpt-4\")\n        model.info = {}\n        self.assertEqual(model.get_repo_map_tokens(), 1024)\n\n        # Test minimum boundary (max_input_tokens < 8192)\n        model.info = {\"max_input_tokens\": 4096}\n        self.assertEqual(model.get_repo_map_tokens(), 1024)\n\n        # Test middle range (max_input_tokens = 16384)\n        model.info = {\"max_input_tokens\": 16384}\n        self.assertEqual(model.get_repo_map_tokens(), 2048)\n\n        # Test maximum boundary (max_input_tokens > 32768)\n        model.info = {\"max_input_tokens\": 65536}\n        self.assertEqual(model.get_repo_map_tokens(), 4096)\n\n        # Test exact boundary values\n        model.info = {\"max_input_tokens\": 8192}\n        self.assertEqual(model.get_repo_map_tokens(), 1024)\n\n        model.info = {\"max_input_tokens\": 32768}\n        self.assertEqual(model.get_repo_map_tokens(), 4096)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_models.py::7",
    "metadata": {
      "file_path": "tests/basic/test_models.py",
      "file_name": "test_models.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 377,
      "span_ids": [
        "TestModels.test_aider_extra_model_settings",
        "impl"
      ],
      "start_line": 160,
      "end_line": 214,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestModels(unittest.TestCase):\n\n    def test_aider_extra_model_settings(self):\n        import tempfile\n\n        import yaml\n\n        # Create temporary YAML file with test settings\n        test_settings = [\n            {\n                \"name\": \"aider/extra_params\",\n                \"extra_params\": {\n                    \"extra_headers\": {\"Foo\": \"bar\"},\n                    \"some_param\": \"some value\",\n                },\n            },\n        ]\n\n        # Write to a regular file instead of NamedTemporaryFile\n        # for better cross-platform compatibility\n        tmp = tempfile.mktemp(suffix=\".yml\")\n        try:\n            with open(tmp, \"w\") as f:\n                yaml.dump(test_settings, f)\n\n            # Register the test settings\n            register_models([tmp])\n\n            # Test that defaults are applied when no exact match\n            model = Model(\"claude-3-5-sonnet-20240620\")\n            # Test that both the override and existing headers are present\n            model = Model(\"claude-3-5-sonnet-20240620\")\n            self.assertEqual(model.extra_params[\"extra_headers\"][\"Foo\"], \"bar\")\n            self.assertEqual(\n                model.extra_params[\"extra_headers\"][\"anthropic-beta\"],\n                ANTHROPIC_BETA_HEADER,\n            )\n            self.assertEqual(model.extra_params[\"some_param\"], \"some value\")\n            self.assertEqual(model.extra_params[\"max_tokens\"], 8192)\n\n            # Test that exact match overrides defaults but not overrides\n            model = Model(\"gpt-4\")\n            self.assertEqual(model.extra_params[\"extra_headers\"][\"Foo\"], \"bar\")\n            self.assertEqual(model.extra_params[\"some_param\"], \"some value\")\n        finally:\n            # Clean up the temporary file\n            import os\n\n            try:\n                os.unlink(tmp)\n            except OSError:\n                pass\n\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repo.py::1",
    "metadata": {
      "file_path": "tests/basic/test_repo.py",
      "file_name": "test_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 210,
      "span_ids": [
        "imports",
        "TestRepo",
        "TestRepo.setUp",
        "TestRepo.test_diffs_empty_repo"
      ],
      "start_line": 1,
      "end_line": 37,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import os\nimport platform\nimport tempfile\nimport time\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import patch\n\nimport git\n\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import InputOutput\nfrom aider.models import Model\nfrom aider.repo import GitRepo\nfrom aider.utils import GitTemporaryDirectory\n\n\nclass TestRepo(unittest.TestCase):\n    def setUp(self):\n        self.GPT35 = Model(\"gpt-3.5-turbo\")\n\n    def test_diffs_empty_repo(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n\n            # Add a change to the index\n            fname = Path(\"foo.txt\")\n            fname.write_text(\"index\\n\")\n            repo.git.add(str(fname))\n\n            # Make a change in the working dir\n            fname.write_text(\"workingdir\\n\")\n\n            git_repo = GitRepo(InputOutput(), None, \".\")\n            diffs = git_repo.get_diffs()\n            self.assertIn(\"index\", diffs)\n            self.assertIn(\"workingdir\", diffs)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repo.py::2",
    "metadata": {
      "file_path": "tests/basic/test_repo.py",
      "file_name": "test_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 144,
      "span_ids": [
        "TestRepo.test_diffs_nonempty_repo"
      ],
      "start_line": 39,
      "end_line": 60,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepo(unittest.TestCase):\n\n    def test_diffs_nonempty_repo(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n            fname = Path(\"foo.txt\")\n            fname.touch()\n            repo.git.add(str(fname))\n\n            fname2 = Path(\"bar.txt\")\n            fname2.touch()\n            repo.git.add(str(fname2))\n\n            repo.git.commit(\"-m\", \"initial\")\n\n            fname.write_text(\"index\\n\")\n            repo.git.add(str(fname))\n\n            fname2.write_text(\"workingdir\\n\")\n\n            git_repo = GitRepo(InputOutput(), None, \".\")\n            diffs = git_repo.get_diffs()\n            self.assertIn(\"index\", diffs)\n            self.assertIn(\"workingdir\", diffs)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repo.py::3",
    "metadata": {
      "file_path": "tests/basic/test_repo.py",
      "file_name": "test_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 195,
      "span_ids": [
        "TestRepo.test_diffs_detached_head"
      ],
      "start_line": 62,
      "end_line": 90,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepo(unittest.TestCase):\n\n    def test_diffs_detached_head(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n            fname = Path(\"foo.txt\")\n            fname.touch()\n            repo.git.add(str(fname))\n            repo.git.commit(\"-m\", \"foo\")\n\n            fname2 = Path(\"bar.txt\")\n            fname2.touch()\n            repo.git.add(str(fname2))\n            repo.git.commit(\"-m\", \"bar\")\n\n            fname3 = Path(\"baz.txt\")\n            fname3.touch()\n            repo.git.add(str(fname3))\n            repo.git.commit(\"-m\", \"baz\")\n\n            repo.git.checkout(\"HEAD^\")\n\n            fname.write_text(\"index\\n\")\n            repo.git.add(str(fname))\n\n            fname2.write_text(\"workingdir\\n\")\n\n            git_repo = GitRepo(InputOutput(), None, \".\")\n            diffs = git_repo.get_diffs()\n            self.assertIn(\"index\", diffs)\n            self.assertIn(\"workingdir\", diffs)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repo.py::4",
    "metadata": {
      "file_path": "tests/basic/test_repo.py",
      "file_name": "test_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 126,
      "span_ids": [
        "TestRepo.test_diffs_between_commits"
      ],
      "start_line": 92,
      "end_line": 107,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepo(unittest.TestCase):\n\n    def test_diffs_between_commits(self):\n        with GitTemporaryDirectory():\n            repo = git.Repo()\n            fname = Path(\"foo.txt\")\n\n            fname.write_text(\"one\\n\")\n            repo.git.add(str(fname))\n            repo.git.commit(\"-m\", \"initial\")\n\n            fname.write_text(\"two\\n\")\n            repo.git.add(str(fname))\n            repo.git.commit(\"-m\", \"second\")\n\n            git_repo = GitRepo(InputOutput(), None, \".\")\n            diffs = git_repo.diff_commits(False, \"HEAD~1\", \"HEAD\")\n            self.assertIn(\"two\", diffs)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repo.py::5",
    "metadata": {
      "file_path": "tests/basic/test_repo.py",
      "file_name": "test_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 296,
      "span_ids": [
        "TestRepo.test_get_commit_message"
      ],
      "start_line": 109,
      "end_line": 136,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepo(unittest.TestCase):\n\n    @patch(\"aider.repo.simple_send_with_retries\")\n    def test_get_commit_message(self, mock_send):\n        mock_send.side_effect = [\"\", \"a good commit message\"]\n\n        model1 = Model(\"gpt-3.5-turbo\")\n        model2 = Model(\"gpt-4\")\n        dump(model1)\n        dump(model2)\n        repo = GitRepo(InputOutput(), None, None, models=[model1, model2])\n\n        # Call the get_commit_message method with dummy diff and context\n        result = repo.get_commit_message(\"dummy diff\", \"dummy context\")\n\n        # Assert that the returned message is the expected one from the second model\n        self.assertEqual(result, \"a good commit message\")\n\n        # Check that simple_send_with_retries was called twice\n        self.assertEqual(mock_send.call_count, 2)\n\n        # Check that it was called with the correct models\n        self.assertEqual(mock_send.call_args_list[0][0][0], model1)\n        self.assertEqual(mock_send.call_args_list[1][0][0], model2)\n\n        # Check that the content of the messages is the same for both calls\n        self.assertEqual(mock_send.call_args_list[0][0][1], mock_send.call_args_list[1][0][1])\n\n        # Optionally, you can still dump the call args if needed for debugging\n        dump(mock_send.call_args_list)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repo.py::6",
    "metadata": {
      "file_path": "tests/basic/test_repo.py",
      "file_name": "test_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 116,
      "span_ids": [
        "TestRepo.test_get_commit_message_strip_quotes"
      ],
      "start_line": 138,
      "end_line": 147,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepo(unittest.TestCase):\n\n    @patch(\"aider.repo.simple_send_with_retries\")\n    def test_get_commit_message_strip_quotes(self, mock_send):\n        mock_send.return_value = '\"a good commit message\"'\n\n        repo = GitRepo(InputOutput(), None, None, models=[self.GPT35])\n        # Call the get_commit_message method with dummy diff and context\n        result = repo.get_commit_message(\"dummy diff\", \"dummy context\")\n\n        # Assert that the returned message is the expected one\n        self.assertEqual(result, \"a good commit message\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repo.py::7",
    "metadata": {
      "file_path": "tests/basic/test_repo.py",
      "file_name": "test_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 122,
      "span_ids": [
        "TestRepo.test_get_commit_message_no_strip_unmatched_quotes"
      ],
      "start_line": 149,
      "end_line": 158,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepo(unittest.TestCase):\n\n    @patch(\"aider.repo.simple_send_with_retries\")\n    def test_get_commit_message_no_strip_unmatched_quotes(self, mock_send):\n        mock_send.return_value = 'a good \"commit message\"'\n\n        repo = GitRepo(InputOutput(), None, None, models=[self.GPT35])\n        # Call the get_commit_message method with dummy diff and context\n        result = repo.get_commit_message(\"dummy diff\", \"dummy context\")\n\n        # Assert that the returned message is the expected one\n        self.assertEqual(result, 'a good \"commit message\"')",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repo.py::8",
    "metadata": {
      "file_path": "tests/basic/test_repo.py",
      "file_name": "test_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 140,
      "span_ids": [
        "TestRepo.test_get_commit_message_with_custom_prompt"
      ],
      "start_line": 160,
      "end_line": 171,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepo(unittest.TestCase):\n\n    @patch(\"aider.repo.simple_send_with_retries\")\n    def test_get_commit_message_with_custom_prompt(self, mock_send):\n        mock_send.return_value = \"Custom commit message\"\n        custom_prompt = \"Generate a commit message in the style of Shakespeare\"\n\n        repo = GitRepo(InputOutput(), None, None, models=[self.GPT35], commit_prompt=custom_prompt)\n        result = repo.get_commit_message(\"dummy diff\", \"dummy context\")\n\n        self.assertEqual(result, \"Custom commit message\")\n        mock_send.assert_called_once()\n        args, _ = mock_send.call_args\n        self.assertEqual(args[1][0][\"content\"], custom_prompt)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repo.py::9",
    "metadata": {
      "file_path": "tests/basic/test_repo.py",
      "file_name": "test_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 376,
      "span_ids": [
        "TestRepo.test_commit_with_custom_committer_name"
      ],
      "start_line": 173,
      "end_line": 217,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepo(unittest.TestCase):\n\n    @patch(\"aider.repo.GitRepo.get_commit_message\")\n    def test_commit_with_custom_committer_name(self, mock_send):\n        mock_send.return_value = '\"a good commit message\"'\n\n        # Cleanup of the git temp dir explodes on windows\n        if platform.system() == \"Windows\":\n            return\n\n        with GitTemporaryDirectory():\n            # new repo\n            raw_repo = git.Repo()\n            raw_repo.config_writer().set_value(\"user\", \"name\", \"Test User\").release()\n\n            # add a file and commit it\n            fname = Path(\"file.txt\")\n            fname.touch()\n            raw_repo.git.add(str(fname))\n            raw_repo.git.commit(\"-m\", \"initial commit\")\n\n            io = InputOutput()\n            git_repo = GitRepo(io, None, None)\n\n            # commit a change\n            fname.write_text(\"new content\")\n            git_repo.commit(fnames=[str(fname)], aider_edits=True)\n\n            # check the committer name\n            commit = raw_repo.head.commit\n            self.assertEqual(commit.author.name, \"Test User (aider)\")\n            self.assertEqual(commit.committer.name, \"Test User (aider)\")\n\n            # commit a change without aider_edits\n            fname.write_text(\"new content again!\")\n            git_repo.commit(fnames=[str(fname)], aider_edits=False)\n\n            # check the committer name\n            commit = raw_repo.head.commit\n            self.assertEqual(commit.author.name, \"Test User\")\n            self.assertEqual(commit.committer.name, \"Test User (aider)\")\n\n            # check that the original committer name is restored\n            original_committer_name = os.environ.get(\"GIT_COMMITTER_NAME\")\n            self.assertIsNone(original_committer_name)\n            original_author_name = os.environ.get(\"GIT_AUTHOR_NAME\")\n            self.assertIsNone(original_author_name)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repo.py::10",
    "metadata": {
      "file_path": "tests/basic/test_repo.py",
      "file_name": "test_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 337,
      "span_ids": [
        "TestRepo.test_get_tracked_files"
      ],
      "start_line": 219,
      "end_line": 253,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepo(unittest.TestCase):\n\n    def test_get_tracked_files(self):\n        # Create a temporary directory\n        tempdir = Path(tempfile.mkdtemp())\n\n        # Initialize a git repository in the temporary directory and set user name and email\n        repo = git.Repo.init(tempdir)\n        repo.config_writer().set_value(\"user\", \"name\", \"Test User\").release()\n        repo.config_writer().set_value(\"user\", \"email\", \"testuser@example.com\").release()\n\n        # Create three empty files and add them to the git repository\n        filenames = [\"README.md\", \"subdir/f\u00e4nny.md\", \"system\u00fcber/blick.md\", 'file\"with\"quotes.txt']\n        created_files = []\n        for filename in filenames:\n            file_path = tempdir / filename\n            try:\n                file_path.parent.mkdir(parents=True, exist_ok=True)\n                file_path.touch()\n                repo.git.add(str(file_path))\n                created_files.append(Path(filename))\n            except OSError:\n                # windows won't allow files with quotes, that's ok\n                self.assertIn('\"', filename)\n                self.assertEqual(os.name, \"nt\")\n\n        self.assertTrue(len(created_files) >= 3)\n\n        repo.git.commit(\"-m\", \"added\")\n\n        tracked_files = GitRepo(InputOutput(), [tempdir], None).get_tracked_files()\n\n        # On windows, paths will come back \\like\\this, so normalize them back to Paths\n        tracked_files = [Path(fn) for fn in tracked_files]\n\n        # Assert that coder.get_tracked_files() returns the three filenames\n        self.assertEqual(set(tracked_files), set(created_files))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repo.py::11",
    "metadata": {
      "file_path": "tests/basic/test_repo.py",
      "file_name": "test_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 226,
      "span_ids": [
        "TestRepo.test_get_tracked_files_with_new_staged_file"
      ],
      "start_line": 255,
      "end_line": 284,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepo(unittest.TestCase):\n\n    def test_get_tracked_files_with_new_staged_file(self):\n        with GitTemporaryDirectory():\n            # new repo\n            raw_repo = git.Repo()\n\n            # add it, but no commits at all in the raw_repo yet\n            fname = Path(\"new.txt\")\n            fname.touch()\n            raw_repo.git.add(str(fname))\n\n            git_repo = GitRepo(InputOutput(), None, None)\n\n            # better be there\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n\n            # commit it, better still be there\n            raw_repo.git.commit(\"-m\", \"new\")\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n\n            # new file, added but not committed\n            fname2 = Path(\"new2.txt\")\n            fname2.touch()\n            raw_repo.git.add(str(fname2))\n\n            # both should be there\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n            self.assertIn(str(fname2), fnames)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repo.py::12",
    "metadata": {
      "file_path": "tests/basic/test_repo.py",
      "file_name": "test_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 391,
      "span_ids": [
        "TestRepo.test_get_tracked_files_with_aiderignore"
      ],
      "start_line": 286,
      "end_line": 334,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepo(unittest.TestCase):\n\n    def test_get_tracked_files_with_aiderignore(self):\n        with GitTemporaryDirectory():\n            # new repo\n            raw_repo = git.Repo()\n\n            # add it, but no commits at all in the raw_repo yet\n            fname = Path(\"new.txt\")\n            fname.touch()\n            raw_repo.git.add(str(fname))\n\n            aiderignore = Path(\".aiderignore\")\n            git_repo = GitRepo(InputOutput(), None, None, str(aiderignore))\n\n            # better be there\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n\n            # commit it, better still be there\n            raw_repo.git.commit(\"-m\", \"new\")\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n\n            # new file, added but not committed\n            fname2 = Path(\"new2.txt\")\n            fname2.touch()\n            raw_repo.git.add(str(fname2))\n\n            # both should be there\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n            self.assertIn(str(fname2), fnames)\n\n            aiderignore.write_text(\"new.txt\\n\")\n            time.sleep(2)\n\n            # new.txt should be gone!\n            fnames = git_repo.get_tracked_files()\n            self.assertNotIn(str(fname), fnames)\n            self.assertIn(str(fname2), fnames)\n\n            # This does not work in github actions?!\n            # The mtime doesn't change, even if I time.sleep(1)\n            # Before doing this write_text()!?\n            #\n            # aiderignore.write_text(\"new2.txt\\n\")\n            # new2.txt should be gone!\n            # fnames = git_repo.get_tracked_files()\n            # self.assertIn(str(fname), fnames)\n            # self.assertNotIn(str(fname2), fnames)\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repo.py::13",
    "metadata": {
      "file_path": "tests/basic/test_repo.py",
      "file_name": "test_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 166,
      "span_ids": [
        "TestRepo.test_get_tracked_files_from_subdir"
      ],
      "start_line": 336,
      "end_line": 358,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepo(unittest.TestCase):\n\n    def test_get_tracked_files_from_subdir(self):\n        with GitTemporaryDirectory():\n            # new repo\n            raw_repo = git.Repo()\n\n            # add it, but no commits at all in the raw_repo yet\n            fname = Path(\"subdir/new.txt\")\n            fname.parent.mkdir()\n            fname.touch()\n            raw_repo.git.add(str(fname))\n\n            os.chdir(fname.parent)\n\n            git_repo = GitRepo(InputOutput(), None, None)\n\n            # better be there\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)\n\n            # commit it, better still be there\n            raw_repo.git.commit(\"-m\", \"new\")\n            fnames = git_repo.get_tracked_files()\n            self.assertIn(str(fname), fnames)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repo.py::14",
    "metadata": {
      "file_path": "tests/basic/test_repo.py",
      "file_name": "test_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 294,
      "span_ids": [
        "TestRepo.test_subtree_only"
      ],
      "start_line": 360,
      "end_line": 394,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepo(unittest.TestCase):\n\n    def test_subtree_only(self):\n        with GitTemporaryDirectory():\n            # Create a new repo\n            raw_repo = git.Repo()\n\n            # Create files in different directories\n            root_file = Path(\"root.txt\")\n            subdir_file = Path(\"subdir/subdir_file.txt\")\n            another_subdir_file = Path(\"another_subdir/another_file.txt\")\n\n            root_file.touch()\n            subdir_file.parent.mkdir()\n            subdir_file.touch()\n            another_subdir_file.parent.mkdir()\n            another_subdir_file.touch()\n\n            raw_repo.git.add(str(root_file), str(subdir_file), str(another_subdir_file))\n            raw_repo.git.commit(\"-m\", \"Initial commit\")\n\n            # Change to the subdir\n            os.chdir(subdir_file.parent)\n\n            # Create GitRepo instance with subtree_only=True\n            git_repo = GitRepo(InputOutput(), None, None, subtree_only=True)\n\n            # Test ignored_file method\n            self.assertFalse(git_repo.ignored_file(str(subdir_file)))\n            self.assertTrue(git_repo.ignored_file(str(root_file)))\n            self.assertTrue(git_repo.ignored_file(str(another_subdir_file)))\n\n            # Test get_tracked_files method\n            tracked_files = git_repo.get_tracked_files()\n            self.assertIn(str(subdir_file), tracked_files)\n            self.assertNotIn(str(root_file), tracked_files)\n            self.assertNotIn(str(another_subdir_file), tracked_files)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repo.py::15",
    "metadata": {
      "file_path": "tests/basic/test_repo.py",
      "file_name": "test_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 132,
      "span_ids": [
        "TestRepo.test_noop_commit"
      ],
      "start_line": 396,
      "end_line": 413,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepo(unittest.TestCase):\n\n    @patch(\"aider.repo.simple_send_with_retries\")\n    def test_noop_commit(self, mock_send):\n        mock_send.return_value = '\"a good commit message\"'\n\n        with GitTemporaryDirectory():\n            # new repo\n            raw_repo = git.Repo()\n\n            # add it, but no commits at all in the raw_repo yet\n            fname = Path(\"file.txt\")\n            fname.touch()\n            raw_repo.git.add(str(fname))\n            raw_repo.git.commit(\"-m\", \"new\")\n\n            git_repo = GitRepo(InputOutput(), None, None)\n\n            git_repo.commit(fnames=[str(fname)])",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repomap.py::1",
    "metadata": {
      "file_path": "tests/basic/test_repomap.py",
      "file_name": "test_repomap.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 330,
      "span_ids": [
        "imports",
        "TestRepoMap.setUp",
        "TestRepoMap.test_get_repo_map",
        "TestRepoMap"
      ],
      "start_line": 1,
      "end_line": 47,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import difflib\nimport os\nimport re\nimport time\nimport unittest\nfrom pathlib import Path\n\nimport git\n\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import InputOutput\nfrom aider.models import Model\nfrom aider.repomap import RepoMap\nfrom aider.utils import GitTemporaryDirectory, IgnorantTemporaryDirectory\n\n\nclass TestRepoMap(unittest.TestCase):\n    def setUp(self):\n        self.GPT35 = Model(\"gpt-3.5-turbo\")\n\n    def test_get_repo_map(self):\n        # Create a temporary directory with sample files for testing\n        test_files = [\n            \"test_file1.py\",\n            \"test_file2.py\",\n            \"test_file3.md\",\n            \"test_file4.json\",\n        ]\n\n        with IgnorantTemporaryDirectory() as temp_dir:\n            for file in test_files:\n                with open(os.path.join(temp_dir, file), \"w\") as f:\n                    f.write(\"\")\n\n            io = InputOutput()\n            repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=io)\n            other_files = [os.path.join(temp_dir, file) for file in test_files]\n            result = repo_map.get_repo_map([], other_files)\n\n            # Check if the result contains the expected tags map\n            self.assertIn(\"test_file1.py\", result)\n            self.assertIn(\"test_file2.py\", result)\n            self.assertIn(\"test_file3.md\", result)\n            self.assertIn(\"test_file4.json\", result)\n\n            # close the open cache files, so Windows won't error\n            del repo_map",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repomap.py::2",
    "metadata": {
      "file_path": "tests/basic/test_repomap.py",
      "file_name": "test_repomap.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 536,
      "span_ids": [
        "TestRepoMap.test_repo_map_refresh_files"
      ],
      "start_line": 49,
      "end_line": 104,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepoMap(unittest.TestCase):\n\n    def test_repo_map_refresh_files(self):\n        with GitTemporaryDirectory() as temp_dir:\n            repo = git.Repo(temp_dir)\n\n            # Create three source files with one function each\n            file1_content = \"def function1():\\n    return 'Hello from file1'\\n\"\n            file2_content = \"def function2():\\n    return 'Hello from file2'\\n\"\n            file3_content = \"def function3():\\n    return 'Hello from file3'\\n\"\n\n            with open(os.path.join(temp_dir, \"file1.py\"), \"w\") as f:\n                f.write(file1_content)\n            with open(os.path.join(temp_dir, \"file2.py\"), \"w\") as f:\n                f.write(file2_content)\n            with open(os.path.join(temp_dir, \"file3.py\"), \"w\") as f:\n                f.write(file3_content)\n\n            # Add files to git\n            repo.index.add([\"file1.py\", \"file2.py\", \"file3.py\"])\n            repo.index.commit(\"Initial commit\")\n\n            # Initialize RepoMap with refresh=\"files\"\n            io = InputOutput()\n            repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=io, refresh=\"files\")\n            other_files = [\n                os.path.join(temp_dir, \"file1.py\"),\n                os.path.join(temp_dir, \"file2.py\"),\n                os.path.join(temp_dir, \"file3.py\"),\n            ]\n\n            # Get initial repo map\n            initial_map = repo_map.get_repo_map([], other_files)\n            dump(initial_map)\n            self.assertIn(\"function1\", initial_map)\n            self.assertIn(\"function2\", initial_map)\n            self.assertIn(\"function3\", initial_map)\n\n            # Add a new function to file1.py\n            with open(os.path.join(temp_dir, \"file1.py\"), \"a\") as f:\n                f.write(\"\\ndef functionNEW():\\n    return 'Hello NEW'\\n\")\n\n            # Get another repo map\n            second_map = repo_map.get_repo_map([], other_files)\n            self.assertEqual(\n                initial_map, second_map, \"RepoMap should not change with refresh='files'\"\n            )\n\n            other_files = [\n                os.path.join(temp_dir, \"file1.py\"),\n                os.path.join(temp_dir, \"file2.py\"),\n            ]\n            second_map = repo_map.get_repo_map([], other_files)\n            self.assertIn(\"functionNEW\", second_map)\n\n            # close the open cache files, so Windows won't error\n            del repo_map\n            del repo",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repomap.py::3",
    "metadata": {
      "file_path": "tests/basic/test_repomap.py",
      "file_name": "test_repomap.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 572,
      "span_ids": [
        "TestRepoMap.test_repo_map_refresh_auto"
      ],
      "start_line": 106,
      "end_line": 161,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepoMap(unittest.TestCase):\n\n    def test_repo_map_refresh_auto(self):\n        with GitTemporaryDirectory() as temp_dir:\n            repo = git.Repo(temp_dir)\n\n            # Create two source files with one function each\n            file1_content = \"def function1():\\n    return 'Hello from file1'\\n\"\n            file2_content = \"def function2():\\n    return 'Hello from file2'\\n\"\n\n            with open(os.path.join(temp_dir, \"file1.py\"), \"w\") as f:\n                f.write(file1_content)\n            with open(os.path.join(temp_dir, \"file2.py\"), \"w\") as f:\n                f.write(file2_content)\n\n            # Add files to git\n            repo.index.add([\"file1.py\", \"file2.py\"])\n            repo.index.commit(\"Initial commit\")\n\n            # Initialize RepoMap with refresh=\"auto\"\n            io = InputOutput()\n            repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=io, refresh=\"auto\")\n            chat_files = []\n            other_files = [os.path.join(temp_dir, \"file1.py\"), os.path.join(temp_dir, \"file2.py\")]\n\n            # Force the RepoMap computation to take more than 1 second\n            original_get_ranked_tags = repo_map.get_ranked_tags\n\n            def slow_get_ranked_tags(*args, **kwargs):\n                time.sleep(1.1)  # Sleep for 1.1 seconds to ensure it's over 1 second\n                return original_get_ranked_tags(*args, **kwargs)\n\n            repo_map.get_ranked_tags = slow_get_ranked_tags\n\n            # Get initial repo map\n            initial_map = repo_map.get_repo_map(chat_files, other_files)\n            self.assertIn(\"function1\", initial_map)\n            self.assertIn(\"function2\", initial_map)\n            self.assertNotIn(\"functionNEW\", initial_map)\n\n            # Add a new function to file1.py\n            with open(os.path.join(temp_dir, \"file1.py\"), \"a\") as f:\n                f.write(\"\\ndef functionNEW():\\n    return 'Hello NEW'\\n\")\n\n            # Get another repo map without force_refresh\n            second_map = repo_map.get_repo_map(chat_files, other_files)\n            self.assertEqual(\n                initial_map, second_map, \"RepoMap should not change without force_refresh\"\n            )\n\n            # Get a new repo map with force_refresh\n            final_map = repo_map.get_repo_map(chat_files, other_files, force_refresh=True)\n            self.assertIn(\"functionNEW\", final_map)\n            self.assertNotEqual(initial_map, final_map, \"RepoMap should change with force_refresh\")\n\n            # close the open cache files, so Windows won't error\n            del repo_map\n            del repo",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repomap.py::4",
    "metadata": {
      "file_path": "tests/basic/test_repomap.py",
      "file_name": "test_repomap.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 415,
      "span_ids": [
        "TestRepoMap.test_get_repo_map_with_identifiers"
      ],
      "start_line": 163,
      "end_line": 214,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepoMap(unittest.TestCase):\n\n    def test_get_repo_map_with_identifiers(self):\n        # Create a temporary directory with a sample Python file containing identifiers\n        test_file1 = \"test_file_with_identifiers.py\"\n        file_content1 = \"\"\"\\\nclass MyClass:\n    def my_method(self, arg1, arg2):\n        return arg1 + arg2\n\ndef my_function(arg1, arg2):\n    return arg1 * arg2\n\"\"\"\n\n        test_file2 = \"test_file_import.py\"\n        file_content2 = \"\"\"\\\nfrom test_file_with_identifiers import MyClass\n\nobj = MyClass()\nprint(obj.my_method(1, 2))\nprint(my_function(3, 4))\n\"\"\"\n\n        test_file3 = \"test_file_pass.py\"\n        file_content3 = \"pass\"\n\n        with IgnorantTemporaryDirectory() as temp_dir:\n            with open(os.path.join(temp_dir, test_file1), \"w\") as f:\n                f.write(file_content1)\n\n            with open(os.path.join(temp_dir, test_file2), \"w\") as f:\n                f.write(file_content2)\n\n            with open(os.path.join(temp_dir, test_file3), \"w\") as f:\n                f.write(file_content3)\n\n            io = InputOutput()\n            repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=io)\n            other_files = [\n                os.path.join(temp_dir, test_file1),\n                os.path.join(temp_dir, test_file2),\n                os.path.join(temp_dir, test_file3),\n            ]\n            result = repo_map.get_repo_map([], other_files)\n\n            # Check if the result contains the expected tags map with identifiers\n            self.assertIn(\"test_file_with_identifiers.py\", result)\n            self.assertIn(\"MyClass\", result)\n            self.assertIn(\"my_method\", result)\n            self.assertIn(\"my_function\", result)\n            self.assertIn(\"test_file_pass.py\", result)\n\n            # close the open cache files, so Windows won't error\n            del repo_map",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repomap.py::5",
    "metadata": {
      "file_path": "tests/basic/test_repomap.py",
      "file_name": "test_repomap.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 229,
      "span_ids": [
        "TestRepoMap.test_get_repo_map_all_files"
      ],
      "start_line": 216,
      "end_line": 244,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepoMap(unittest.TestCase):\n\n    def test_get_repo_map_all_files(self):\n        test_files = [\n            \"test_file0.py\",\n            \"test_file1.txt\",\n            \"test_file2.md\",\n            \"test_file3.json\",\n            \"test_file4.html\",\n            \"test_file5.css\",\n            \"test_file6.js\",\n        ]\n\n        with IgnorantTemporaryDirectory() as temp_dir:\n            for file in test_files:\n                with open(os.path.join(temp_dir, file), \"w\") as f:\n                    f.write(\"\")\n\n            repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=InputOutput())\n\n            other_files = [os.path.join(temp_dir, file) for file in test_files]\n            result = repo_map.get_repo_map([], other_files)\n            dump(other_files)\n            dump(repr(result))\n\n            # Check if the result contains each specific file in the expected tags map without ctags\n            for file in test_files:\n                self.assertIn(file, result)\n\n            # close the open cache files, so Windows won't error\n            del repo_map",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repomap.py::6",
    "metadata": {
      "file_path": "tests/basic/test_repomap.py",
      "file_name": "test_repomap.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 290,
      "span_ids": [
        "TestRepoMapTypescript.setUp",
        "TestRepoMap.test_get_repo_map_excludes_added_files",
        "TestRepoMapTypescript"
      ],
      "start_line": 246,
      "end_line": 279,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepoMap(unittest.TestCase):\n\n    def test_get_repo_map_excludes_added_files(self):\n        # Create a temporary directory with sample files for testing\n        test_files = [\n            \"test_file1.py\",\n            \"test_file2.py\",\n            \"test_file3.md\",\n            \"test_file4.json\",\n        ]\n\n        with IgnorantTemporaryDirectory() as temp_dir:\n            for file in test_files:\n                with open(os.path.join(temp_dir, file), \"w\") as f:\n                    f.write(\"def foo(): pass\\n\")\n\n            io = InputOutput()\n            repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=io)\n            test_files = [os.path.join(temp_dir, file) for file in test_files]\n            result = repo_map.get_repo_map(test_files[:2], test_files[2:])\n\n            dump(result)\n\n            # Check if the result contains the expected tags map\n            self.assertNotIn(\"test_file1.py\", result)\n            self.assertNotIn(\"test_file2.py\", result)\n            self.assertIn(\"test_file3.md\", result)\n            self.assertIn(\"test_file4.json\", result)\n\n            # close the open cache files, so Windows won't error\n            del repo_map\n\n\nclass TestRepoMapTypescript(unittest.TestCase):\n    def setUp(self):\n        self.GPT35 = Model(\"gpt-3.5-turbo\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repomap.py::7",
    "metadata": {
      "file_path": "tests/basic/test_repomap.py",
      "file_name": "test_repomap.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 602,
      "span_ids": [
        "TestRepoMapAllLanguages",
        "TestRepoMapAllLanguages.setUp",
        "TestRepoMapAllLanguages.test_get_repo_map_all_languages"
      ],
      "start_line": 282,
      "end_line": 351,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepoMapAllLanguages(unittest.TestCase):\n    def setUp(self):\n        self.GPT35 = Model(\"gpt-3.5-turbo\")\n\n    def test_get_repo_map_all_languages(self):\n        language_files = {\n            \"c\": (\"c\", \"main\"),\n            \"cpp\": (\"cpp\", \"main\"),\n            \"elixir\": (\"ex\", \"Greeter\"),\n            \"java\": (\"java\", \"Greeting\"),\n            \"javascript\": (\"js\", \"Person\"),\n            \"kotlin\": (\"kt\", \"Greeting\"),\n            \"ocaml\": (\"ml\", \"Greeter\"),\n            \"php\": (\"php\", \"greet\"),\n            \"python\": (\"py\", \"Person\"),\n            \"ql\": (\"ql\", \"greet\"),\n            \"ruby\": (\"rb\", \"greet\"),\n            \"rust\": (\"rs\", \"Person\"),\n            \"typescript\": (\"ts\", \"greet\"),\n            \"tsx\": (\"tsx\", \"UserProps\"),\n            \"csharp\": (\"cs\", \"IGreeter\"),\n            \"elisp\": (\"el\", \"greeter\"),\n            \"elm\": (\"elm\", \"Person\"),\n            \"go\": (\"go\", \"Greeter\"),\n        }\n\n        fixtures_dir = Path(__file__).parent.parent / \"fixtures\" / \"languages\"\n\n        for lang, key_symbol in language_files.items():\n            # Get the fixture file path and name based on language\n            fixture_dir = fixtures_dir / lang\n            ext, key_symbol = language_files[lang]\n            filename = f\"test.{ext}\"\n            fixture_path = fixture_dir / filename\n            self.assertTrue(\n                fixture_path.exists(), f\"Fixture file missing for {lang}: {fixture_path}\"\n            )\n\n            # Read the fixture content\n            with open(fixture_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n            with GitTemporaryDirectory() as temp_dir:\n                test_file = os.path.join(temp_dir, filename)\n                with open(test_file, \"w\", encoding=\"utf-8\") as f:\n                    f.write(content)\n\n                io = InputOutput()\n                repo_map = RepoMap(main_model=self.GPT35, root=temp_dir, io=io)\n                other_files = [filename]\n                result = repo_map.get_repo_map([], other_files)\n                dump(lang)\n                dump(result)\n\n                self.assertGreater(len(result.strip().splitlines()), 1)\n\n                # Check if the result contains all the expected files and symbols\n                self.assertIn(\n                    filename, result, f\"File for language {lang} not found in repo map: {result}\"\n                )\n                self.assertIn(\n                    key_symbol,\n                    result,\n                    (\n                        f\"Key symbol '{key_symbol}' for language {lang} not found in repo map:\"\n                        f\" {result}\"\n                    ),\n                )\n\n                # close the open cache files, so Windows won't error\n                del repo_map",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_repomap.py::8",
    "metadata": {
      "file_path": "tests/basic/test_repomap.py",
      "file_name": "test_repomap.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 543,
      "span_ids": [
        "impl",
        "TestRepoMapAllLanguages.test_repo_map_sample_code_base"
      ],
      "start_line": 353,
      "end_line": 419,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestRepoMapAllLanguages(unittest.TestCase):\n\n    def test_repo_map_sample_code_base(self):\n        # Path to the sample code base\n        sample_code_base = Path(__file__).parent.parent / \"fixtures\" / \"sample-code-base\"\n\n        # Path to the expected repo map file\n        expected_map_file = (\n            Path(__file__).parent.parent / \"fixtures\" / \"sample-code-base-repo-map.txt\"\n        )\n\n        # Ensure the paths exist\n        self.assertTrue(sample_code_base.exists(), \"Sample code base directory not found\")\n        self.assertTrue(expected_map_file.exists(), \"Expected repo map file not found\")\n\n        # Initialize RepoMap with the sample code base as root\n        io = InputOutput()\n        repomap_root = Path(__file__).parent.parent.parent\n        repo_map = RepoMap(\n            main_model=self.GPT35,\n            root=str(repomap_root),\n            io=io,\n        )\n\n        # Get all files in the sample code base\n        other_files = [str(f) for f in sample_code_base.rglob(\"*\") if f.is_file()]\n\n        # Generate the repo map\n        generated_map_str = repo_map.get_repo_map([], other_files).strip()\n\n        # Read the expected map from the file using UTF-8 encoding\n        with open(expected_map_file, \"r\", encoding=\"utf-8\") as f:\n            expected_map = f.read().strip()\n\n        # Normalize path separators for Windows\n        if os.name == \"nt\":  # Check if running on Windows\n            expected_map = re.sub(\n                r\"tests/fixtures/sample-code-base/([^:]+)\",\n                r\"tests\\\\fixtures\\\\sample-code-base\\\\\\1\",\n                expected_map,\n            )\n            generated_map_str = re.sub(\n                r\"tests/fixtures/sample-code-base/([^:]+)\",\n                r\"tests\\\\fixtures\\\\sample-code-base\\\\\\1\",\n                generated_map_str,\n            )\n\n        # Compare the generated map with the expected map\n        if generated_map_str != expected_map:\n            # If they differ, show the differences and fail the test\n            diff = list(\n                difflib.unified_diff(\n                    expected_map.splitlines(),\n                    generated_map_str.splitlines(),\n                    fromfile=\"expected\",\n                    tofile=\"generated\",\n                    lineterm=\"\",\n                )\n            )\n            diff_str = \"\\n\".join(diff)\n            self.fail(f\"Generated map differs from expected map:\\n{diff_str}\")\n\n        # If we reach here, the maps are identical\n        self.assertEqual(generated_map_str, expected_map, \"Generated map matches expected map\")\n\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_run_cmd.py::1",
    "metadata": {
      "file_path": "tests/basic/test_run_cmd.py",
      "file_name": "test_run_cmd.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 61,
      "span_ids": [
        "imports",
        "test_run_cmd_echo"
      ],
      "start_line": 1,
      "end_line": 12,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import pytest  # noqa: F401\n\nfrom aider.run_cmd import run_cmd\n\n\ndef test_run_cmd_echo():\n    command = \"echo Hello, World!\"\n    exit_code, output = run_cmd(command)\n\n    assert exit_code == 0\n    assert output.strip() == \"Hello, World!\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_sanity_check_repo.py::1",
    "metadata": {
      "file_path": "tests/basic/test_sanity_check_repo.py",
      "file_name": "test_sanity_check_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 182,
      "span_ids": [
        "imports",
        "create_repo",
        "mock_io"
      ],
      "start_line": 1,
      "end_line": 34,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import os\nimport shutil\nimport struct\nfrom unittest import mock\n\nimport pytest\nfrom git import GitError, Repo\n\nfrom aider import urls\nfrom aider.main import sanity_check_repo\nfrom aider.repo import GitRepo\nfrom aider.io import InputOutput\n\n\n@pytest.fixture\ndef mock_io():\n    \"\"\"Fixture to create a mock io object.\"\"\"\n    return mock.Mock()\n\n\n@pytest.fixture\ndef create_repo(tmp_path):\n    \"\"\"\n    Fixture to create a standard Git repository.\n    Returns the path to the repo and the Repo object.\n    \"\"\"\n    repo_path = tmp_path / \"test_repo\"\n    repo = Repo.init(repo_path)\n    # Create an initial commit\n    file_path = repo_path / \"README.md\"\n    file_path.write_text(\"# Test Repository\")\n    repo.index.add([str(file_path.relative_to(repo_path))])\n    repo.index.commit(\"Initial commit\")\n    return repo_path, repo",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_sanity_check_repo.py::2",
    "metadata": {
      "file_path": "tests/basic/test_sanity_check_repo.py",
      "file_name": "test_sanity_check_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 189,
      "span_ids": [
        "detach_head",
        "set_git_index_version"
      ],
      "start_line": 37,
      "end_line": 58,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def set_git_index_version(repo_path, version):\n    \"\"\"\n    Sets the Git index version by modifying the .git/index file.\n    The index version is stored in the first 4 bytes as a little-endian integer.\n    \"\"\"\n    index_path = os.path.join(repo_path, \".git\", \"index\")\n    with open(index_path, \"r+b\") as f:\n        # Read the first 4 bytes (signature) and the next 4 bytes (version)\n        signature = f.read(4)\n        if signature != b\"DIRC\":\n            raise ValueError(\"Invalid git index file signature.\")\n        # Write the new version\n        f.seek(4)\n        f.write(struct.pack(\"<I\", version))\n\n\ndef detach_head(repo):\n    \"\"\"\n    Detaches the HEAD of the repository by checking out the current commit hash.\n    \"\"\"\n    current_commit = repo.head.commit\n    repo.git.checkout(current_commit.hexsha)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_sanity_check_repo.py::3",
    "metadata": {
      "file_path": "tests/basic/test_sanity_check_repo.py",
      "file_name": "test_sanity_check_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 187,
      "span_ids": [
        "mock_repo_wrapper"
      ],
      "start_line": 61,
      "end_line": 83,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def mock_repo_wrapper(repo_obj, git_repo_error=None):\n    \"\"\"\n    Creates a mock 'repo' object to pass to sanity_check_repo.\n    The mock object has:\n    - repo.repo: the Repo object\n    - repo.get_tracked_files(): returns a list of tracked files or raises GitError\n    - repo.git_repo_error: the GitError if any\n    \"\"\"\n    mock_repo = mock.Mock()\n    mock_repo.repo = repo_obj\n    if git_repo_error:\n\n        def get_tracked_files_side_effect():\n            raise git_repo_error\n\n        mock_repo.get_tracked_files.side_effect = get_tracked_files_side_effect\n        mock_repo.git_repo_error = git_repo_error\n    else:\n        mock_repo.get_tracked_files.return_value = [\n            str(path) for path in repo_obj.git.ls_files().splitlines()\n        ]\n        mock_repo.git_repo_error = None\n    return mock_repo",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_sanity_check_repo.py::4",
    "metadata": {
      "file_path": "tests/basic/test_sanity_check_repo.py",
      "file_name": "test_sanity_check_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 114,
      "span_ids": [
        "test_detached_head_state"
      ],
      "start_line": 86,
      "end_line": 102,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def test_detached_head_state(create_repo, mock_io):\n    repo_path, repo = create_repo\n    # Detach the HEAD\n    detach_head(repo)\n\n    # Create the mock 'repo' object\n    mock_repo_obj = mock_repo_wrapper(repo)\n\n    # Call the function\n    result = sanity_check_repo(mock_repo_obj, mock_io)\n\n    # Assert that the function returns True\n    assert result is True\n\n    # Assert that no errors were logged\n    mock_io.tool_error.assert_not_called()\n    mock_io.tool_output.assert_not_called()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_sanity_check_repo.py::5",
    "metadata": {
      "file_path": "tests/basic/test_sanity_check_repo.py",
      "file_name": "test_sanity_check_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 293,
      "span_ids": [
        "test_git_index_version_greater_than_2"
      ],
      "start_line": 105,
      "end_line": 135,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "@mock.patch(\"webbrowser.open\")\ndef test_git_index_version_greater_than_2(mock_browser, create_repo, mock_io):\n    repo_path, repo = create_repo\n    # Set the git index version to 3\n    set_git_index_version(str(repo_path), 3)\n\n    # Simulate that get_tracked_files raises an error due to index version\n    git_error = GitError(\"index version in (1, 2) is required\")\n    mock_repo_obj = mock_repo_wrapper(repo, git_repo_error=git_error)\n\n    # Call the function\n    result = sanity_check_repo(mock_repo_obj, mock_io)\n\n    # Assert that the function returns False\n    assert result is False\n\n    # Assert that the appropriate error messages were logged\n    mock_io.tool_error.assert_called_with(\n        \"Aider only works with git repos with version number 1 or 2.\"\n    )\n    mock_io.tool_error.assert_any_call(\n        \"Aider only works with git repos with version number 1 or 2.\"\n    )\n    mock_io.tool_output.assert_any_call(\n        \"You may be able to convert your repo: git update-index --index-version=2\"\n    )\n    mock_io.tool_output.assert_any_call(\"Or run aider --no-git to proceed without using git.\")\n    mock_io.offer_url.assert_any_call(\n        urls.git_index_version,\n        \"Open documentation url for more info?\",\n    )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_sanity_check_repo.py::6",
    "metadata": {
      "file_path": "tests/basic/test_sanity_check_repo.py",
      "file_name": "test_sanity_check_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 145,
      "span_ids": [
        "test_bare_repository"
      ],
      "start_line": 138,
      "end_line": 154,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def test_bare_repository(create_repo, mock_io, tmp_path):\n    # Initialize a bare repository\n    bare_repo_path = tmp_path / \"bare_repo.git\"\n    bare_repo = Repo.init(bare_repo_path, bare=True)\n\n    # Create the mock 'repo' object\n    mock_repo_obj = mock_repo_wrapper(bare_repo)\n\n    # Call the function\n    result = sanity_check_repo(mock_repo_obj, mock_io)\n\n    # Assert that the function returns False\n    assert result is False\n\n    # Assert that the appropriate error message was logged\n    mock_io.tool_error.assert_called_with(\"The git repo does not seem to have a working tree?\")\n    mock_io.tool_output.assert_not_called()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_sanity_check_repo.py::7",
    "metadata": {
      "file_path": "tests/basic/test_sanity_check_repo.py",
      "file_name": "test_sanity_check_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 177,
      "span_ids": [
        "test_sanity_check_repo_with_corrupt_repo"
      ],
      "start_line": 157,
      "end_line": 174,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def test_sanity_check_repo_with_corrupt_repo(create_repo, mock_io):\n    repo_path, repo = create_repo\n    # Simulate a corrupt repository by removing the .git directory\n    shutil.rmtree(os.path.join(repo_path, \".git\"))\n\n    # Create the mock 'repo' object with GitError\n    git_error = GitError(\"Unable to read git repository, it may be corrupt?\")\n    mock_repo_obj = mock_repo_wrapper(repo, git_repo_error=git_error)\n\n    # Call the function\n    result = sanity_check_repo(mock_repo_obj, mock_io)\n\n    # Assert that the function returns False\n    assert result is False\n\n    # Assert that the appropriate error messages were logged\n    mock_io.tool_error.assert_called_with(\"Unable to read git repository, it may be corrupt?\")\n    mock_io.tool_output.assert_called_with(str(git_error))",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_sanity_check_repo.py::8",
    "metadata": {
      "file_path": "tests/basic/test_sanity_check_repo.py",
      "file_name": "test_sanity_check_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 183,
      "span_ids": [
        "test_sanity_check_repo_with_no_repo",
        "corrupt_git_index"
      ],
      "start_line": 177,
      "end_line": 199,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def test_sanity_check_repo_with_no_repo(mock_io):\n    # Call the function with repo=None\n    result = sanity_check_repo(None, mock_io)\n\n    # Assert that the function returns True\n    assert result is True\n\n    # Assert that no errors or outputs were logged\n    mock_io.tool_error.assert_not_called()\n    mock_io.tool_output.assert_not_called()\n\n\ndef corrupt_git_index(repo_path):\n    index_path = os.path.join(repo_path, \".git\", \"index\")\n    with open(index_path, \"r+b\") as f:\n        # Verify the file has the correct signature\n        signature = f.read(4)\n        if signature != b\"DIRC\":\n            raise ValueError(\"Invalid git index file signature.\")\n\n        # Seek to the data section and inject invalid bytes to simulate encoding error\n        f.seek(77)\n        f.write(b\"\\xF5\" * 5)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_sanity_check_repo.py::9",
    "metadata": {
      "file_path": "tests/basic/test_sanity_check_repo.py",
      "file_name": "test_sanity_check_repo.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 207,
      "span_ids": [
        "test_sanity_check_repo_with_corrupt_index"
      ],
      "start_line": 202,
      "end_line": 225,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def test_sanity_check_repo_with_corrupt_index(create_repo, mock_io):\n    repo_path, repo = create_repo\n    # Corrupt the Git index file\n    corrupt_git_index(repo_path)\n\n    # Create GitRepo instance\n    git_repo = GitRepo(InputOutput(), None, repo_path)\n\n    # Call the function\n    result = sanity_check_repo(git_repo, mock_io)\n\n    # Assert that the function returns False\n    assert result is False\n\n    # Assert that the appropriate error messages were logged\n    mock_io.tool_error.assert_called_with(\"Unable to read git repository, it may be corrupt?\")\n    mock_io.tool_output.assert_called_with(\n        (\n            \"Failed to read the Git repository. This issue is likely caused by a path encoded \"\n            \"in a format different from the expected encoding \\\"utf-8\\\".\\n\"\n            \"Internal error: 'utf-8' codec can't decode byte 0xf5 in position 3: invalid start byte\"\n        )\n    )",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_scripting.py::1",
    "metadata": {
      "file_path": "tests/basic/test_scripting.py",
      "file_name": "test_scripting.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 261,
      "span_ids": [
        "imports",
        "TestScriptingAPI.test_basic_scripting",
        "TestScriptingAPI",
        "impl"
      ],
      "start_line": 1,
      "end_line": 40,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import unittest\nfrom pathlib import Path\nfrom unittest.mock import patch\n\nfrom aider.coders import Coder\nfrom aider.models import Model\nfrom aider.utils import GitTemporaryDirectory\n\n\nclass TestScriptingAPI(unittest.TestCase):\n    @patch(\"aider.coders.base_coder.Coder.send\")\n    def test_basic_scripting(self, mock_send):\n        with GitTemporaryDirectory():\n            # Setup\n            def mock_send_side_effect(messages, functions=None):\n                coder.partial_response_content = \"Changes applied successfully.\"\n                coder.partial_response_function_call = None\n                return \"Changes applied successfully.\"\n\n            mock_send.side_effect = mock_send_side_effect\n\n            # Test script\n            fname = Path(\"greeting.py\")\n            fname.touch()\n            fnames = [str(fname)]\n            model = Model(\"gpt-4-turbo\")\n            coder = Coder.create(main_model=model, fnames=fnames)\n\n            result1 = coder.run(\"make a script that prints hello world\")\n            result2 = coder.run(\"make it say goodbye\")\n\n            # Assertions\n            self.assertEqual(mock_send.call_count, 2)\n            self.assertEqual(result1, \"Changes applied successfully.\")\n            self.assertEqual(result2, \"Changes applied successfully.\")\n\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_sendchat.py::1",
    "metadata": {
      "file_path": "tests/basic/test_sendchat.py",
      "file_name": "test_sendchat.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 270,
      "span_ids": [
        "TestSendChat",
        "TestSendChat.setUp",
        "TestSendChat.test_simple_send_with_retries_rate_limit_error",
        "imports",
        "PrintCalled",
        "TestSendChat.test_litellm_exceptions"
      ],
      "start_line": 1,
      "end_line": 42,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import unittest\nfrom unittest.mock import MagicMock, patch\n\nfrom aider.exceptions import LiteLLMExceptions\nfrom aider.llm import litellm\nfrom aider.models import Model\nfrom aider.sendchat import send_completion, simple_send_with_retries\n\n\nclass PrintCalled(Exception):\n    pass\n\n\nclass TestSendChat(unittest.TestCase):\n    def setUp(self):\n        self.mock_messages = [{\"role\": \"user\", \"content\": \"Hello\"}]\n        self.mock_model = \"gpt-4\"\n\n    def test_litellm_exceptions(self):\n        litellm_ex = LiteLLMExceptions()\n        litellm_ex._load(strict=True)\n\n    @patch(\"litellm.completion\")\n    @patch(\"builtins.print\")\n    def test_simple_send_with_retries_rate_limit_error(self, mock_print, mock_completion):\n        mock = MagicMock()\n        mock.status_code = 500\n\n        # Set up the mock to raise\n        mock_completion.side_effect = [\n            litellm.RateLimitError(\n                \"rate limit exceeded\",\n                response=mock,\n                llm_provider=\"llm_provider\",\n                model=\"model\",\n            ),\n            None,\n        ]\n\n        # Call the simple_send_with_retries method\n        simple_send_with_retries(Model(self.mock_model), self.mock_messages)\n        assert mock_print.call_count == 3",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_sendchat.py::2",
    "metadata": {
      "file_path": "tests/basic/test_sendchat.py",
      "file_name": "test_sendchat.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 210,
      "span_ids": [
        "TestSendChat.test_send_completion_basic",
        "TestSendChat.test_send_completion_with_functions"
      ],
      "start_line": 44,
      "end_line": 69,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestSendChat(unittest.TestCase):\n\n    @patch(\"litellm.completion\")\n    def test_send_completion_basic(self, mock_completion):\n        # Setup mock response\n        mock_response = MagicMock()\n        mock_completion.return_value = mock_response\n\n        # Test basic send_completion\n        hash_obj, response = send_completion(\n            self.mock_model, self.mock_messages, functions=None, stream=False\n        )\n\n        assert response == mock_response\n        mock_completion.assert_called_once()\n\n    @patch(\"litellm.completion\")\n    def test_send_completion_with_functions(self, mock_completion):\n        mock_function = {\"name\": \"test_function\", \"parameters\": {\"type\": \"object\"}}\n\n        hash_obj, response = send_completion(\n            self.mock_model, self.mock_messages, functions=[mock_function], stream=False\n        )\n\n        # Verify function was properly included in tools\n        called_kwargs = mock_completion.call_args.kwargs\n        assert \"tools\" in called_kwargs\n        assert called_kwargs[\"tools\"][0][\"function\"] == mock_function",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_sendchat.py::3",
    "metadata": {
      "file_path": "tests/basic/test_sendchat.py",
      "file_name": "test_sendchat.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 218,
      "span_ids": [
        "TestSendChat.test_simple_send_attribute_error",
        "TestSendChat.test_simple_send_non_retryable_error"
      ],
      "start_line": 71,
      "end_line": 96,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestSendChat(unittest.TestCase):\n\n    @patch(\"litellm.completion\")\n    def test_simple_send_attribute_error(self, mock_completion):\n        # Setup mock to raise AttributeError\n        mock_completion.return_value = MagicMock()\n        mock_completion.return_value.choices = None\n\n        # Should return None on AttributeError\n        result = simple_send_with_retries(Model(self.mock_model), self.mock_messages)\n        assert result is None\n\n    @patch(\"litellm.completion\")\n    @patch(\"builtins.print\")\n    def test_simple_send_non_retryable_error(self, mock_print, mock_completion):\n        # Test with an error that shouldn't trigger retries\n        mock = MagicMock()\n        mock.status_code = 400\n\n        mock_completion.side_effect = litellm.NotFoundError(\n            message=\"Invalid request\", llm_provider=\"test_provider\", model=\"test_model\"\n        )\n\n        result = simple_send_with_retries(Model(self.mock_model), self.mock_messages)\n        assert result is None\n        # Should only print the error message\n        assert mock_print.call_count == 1",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_special.py::1",
    "metadata": {
      "file_path": "tests/basic/test_special.py",
      "file_name": "test_special.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 161,
      "span_ids": [
        "imports",
        "test_is_important"
      ],
      "start_line": 1,
      "end_line": 22,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import os\n\nimport pytest\n\nfrom aider.special import filter_important_files, is_important\n\n\ndef test_is_important():\n    # Test common important files\n    assert is_important(\"README.md\")\n    assert is_important(\".gitignore\")\n    assert is_important(\"requirements.txt\")\n    assert is_important(\"setup.py\")\n\n    # Test files in .github/workflows\n    assert is_important(os.path.join(\".github\", \"workflows\", \"test.yml\"))\n    assert is_important(os.path.join(\".github\", \"workflows\", \"deploy.yml\"))\n\n    # Test files that should not be considered important\n    assert not is_important(\"random_file.txt\")\n    assert not is_important(\"src/main.py\")\n    assert not is_important(\"tests/test_app.py\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_special.py::2",
    "metadata": {
      "file_path": "tests/basic/test_special.py",
      "file_name": "test_special.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 284,
      "span_ids": [
        "test_is_important_with_paths",
        "test_is_important_case_sensitivity",
        "test_is_important_various_files",
        "test_filter_important_files"
      ],
      "start_line": 25,
      "end_line": 77,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def test_filter_important_files():\n    files = [\n        \"README.md\",\n        \"src/main.py\",\n        \".gitignore\",\n        \"tests/test_app.py\",\n        \"requirements.txt\",\n        \".github/workflows/test.yml\",\n        \"random_file.txt\",\n    ]\n\n    important_files = filter_important_files(files)\n\n    assert set(important_files) == {\n        \"README.md\",\n        \".gitignore\",\n        \"requirements.txt\",\n        \".github/workflows/test.yml\",\n    }\n\n\ndef test_is_important_case_sensitivity():\n    # Test case sensitivity\n    assert is_important(\"README.md\")\n    assert not is_important(\"readme.md\")\n    assert is_important(\".gitignore\")\n    assert not is_important(\".GITIGNORE\")\n\n\ndef test_is_important_with_paths():\n    # Test with different path formats\n    assert not is_important(\"project/README.md\")\n    assert is_important(\"./README.md\")\n    assert not is_important(\"/absolute/path/to/README.md\")\n\n\n@pytest.mark.parametrize(\n    \"file_path\",\n    [\n        \"README\",\n        \"README.txt\",\n        \"README.rst\",\n        \"LICENSE\",\n        \"LICENSE.md\",\n        \"LICENSE.txt\",\n        \"Dockerfile\",\n        \"package.json\",\n        \"pyproject.toml\",\n    ],\n)\ndef test_is_important_various_files(file_path):\n    assert is_important(file_path)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_udiff.py::1",
    "metadata": {
      "file_path": "tests/basic/test_udiff.py",
      "file_name": "test_udiff.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 149,
      "span_ids": [
        "imports",
        "TestUnifiedDiffCoder.test_find_diffs_single_hunk",
        "TestUnifiedDiffCoder"
      ],
      "start_line": 1,
      "end_line": 27,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import unittest\n\nfrom aider.coders.udiff_coder import find_diffs\nfrom aider.dump import dump  # noqa: F401\n\n\nclass TestUnifiedDiffCoder(unittest.TestCase):\n    def test_find_diffs_single_hunk(self):\n        # Test find_diffs with a single hunk\n        content = \"\"\"\nSome text...\n\n```diff\n--- file.txt\n+++ file.txt\n@@ ... @@\n-Original\n+Modified\n```\n\"\"\"\n        edits = find_diffs(content)\n        dump(edits)\n        self.assertEqual(len(edits), 1)\n\n        edit = edits[0]\n        self.assertEqual(edit[0], \"file.txt\")\n        self.assertEqual(edit[1], [\"-Original\\n\", \"+Modified\\n\"])",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_udiff.py::2",
    "metadata": {
      "file_path": "tests/basic/test_udiff.py",
      "file_name": "test_udiff.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 120,
      "span_ids": [
        "TestUnifiedDiffCoder.test_find_diffs_dev_null"
      ],
      "start_line": 29,
      "end_line": 48,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUnifiedDiffCoder(unittest.TestCase):\n\n    def test_find_diffs_dev_null(self):\n        # Test find_diffs with a single hunk\n        content = \"\"\"\nSome text...\n\n```diff\n--- /dev/null\n+++ file.txt\n@@ ... @@\n-Original\n+Modified\n```\n\"\"\"\n        edits = find_diffs(content)\n        dump(edits)\n        self.assertEqual(len(edits), 1)\n\n        edit = edits[0]\n        self.assertEqual(edit[0], \"file.txt\")\n        self.assertEqual(edit[1], [\"-Original\\n\", \"+Modified\\n\"])",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_udiff.py::3",
    "metadata": {
      "file_path": "tests/basic/test_udiff.py",
      "file_name": "test_udiff.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 133,
      "span_ids": [
        "TestUnifiedDiffCoder.test_find_diffs_dirname_with_spaces"
      ],
      "start_line": 50,
      "end_line": 69,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUnifiedDiffCoder(unittest.TestCase):\n\n    def test_find_diffs_dirname_with_spaces(self):\n        # Test find_diffs with a single hunk\n        content = \"\"\"\nSome text...\n\n```diff\n--- dir name with spaces/file.txt\n+++ dir name with spaces/file.txt\n@@ ... @@\n-Original\n+Modified\n```\n\"\"\"\n        edits = find_diffs(content)\n        dump(edits)\n        self.assertEqual(len(edits), 1)\n\n        edit = edits[0]\n        self.assertEqual(edit[0], \"dir name with spaces/file.txt\")\n        self.assertEqual(edit[1], [\"-Original\\n\", \"+Modified\\n\"])",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_udiff.py::4",
    "metadata": {
      "file_path": "tests/basic/test_udiff.py",
      "file_name": "test_udiff.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 399,
      "span_ids": [
        "impl",
        "TestUnifiedDiffCoder.test_find_multi_diffs"
      ],
      "start_line": 71,
      "end_line": 120,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestUnifiedDiffCoder(unittest.TestCase):\n\n    def test_find_multi_diffs(self):\n        content = \"\"\"\nTo implement the `--check-update` option, I will make the following changes:\n\n1. Add the `--check-update` argument to the argument parser in `aider/main.py`.\n2. Modify the `check_version` function in `aider/versioncheck.py` to return a boolean indicating whether an update is available.\n3. Use the returned value from `check_version` in `aider/main.py` to set the exit status code when `--check-update` is used.\n\nHere are the diffs for those changes:\n\n```diff\n--- aider/versioncheck.py\n+++ aider/versioncheck.py\n@@ ... @@\n     except Exception as err:\n         print_cmd(f\"Error checking pypi for new version: {err}\")\n+        return False\n\n--- aider/main.py\n+++ aider/main.py\n@@ ... @@\n     other_group.add_argument(\n         \"--version\",\n         action=\"version\",\n         version=f\"%(prog)s {__version__}\",\n         help=\"Show the version number and exit\",\n     )\n+    other_group.add_argument(\n+        \"--check-update\",\n+        action=\"store_true\",\n+        help=\"Check for updates and return status in the exit code\",\n+        default=False,\n+    )\n     other_group.add_argument(\n         \"--apply\",\n         metavar=\"FILE\",\n```\n\nThese changes will add the `--check-update` option to the command-line interface and use the `check_version` function to determine if an update is available, exiting with status code `0` if no update is available and `1` if an update is available.\n\"\"\"  # noqa: E501\n\n        edits = find_diffs(content)\n        dump(edits)\n        self.assertEqual(len(edits), 2)\n        self.assertEqual(len(edits[0][1]), 3)\n\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_urls.py::1",
    "metadata": {
      "file_path": "tests/basic/test_urls.py",
      "file_name": "test_urls.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 90,
      "span_ids": [
        "imports",
        "test_urls"
      ],
      "start_line": 1,
      "end_line": 16,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import requests\n\nfrom aider import urls\n\n\ndef test_urls():\n    url_attributes = [\n        attr\n        for attr in dir(urls)\n        if not callable(getattr(urls, attr)) and not attr.startswith(\"__\")\n    ]\n    for attr in url_attributes:\n        url = getattr(urls, attr)\n        response = requests.get(url)\n        assert response.status_code == 200, f\"URL {url} returned status code {response.status_code}\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_voice.py::1",
    "metadata": {
      "file_path": "tests/basic/test_voice.py",
      "file_name": "test_voice.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 330,
      "span_ids": [
        "test_voice_init_invalid_format",
        "mock_soundfile",
        "test_voice_init_specific_device",
        "test_voice_init_invalid_device",
        "test_voice_init_default_device",
        "imports",
        "mock_sounddevice"
      ],
      "start_line": 1,
      "end_line": 53,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import os\nimport queue\nfrom unittest.mock import MagicMock, patch\n\nimport numpy as np\nimport pytest\n\nfrom aider.voice import SoundDeviceError, Voice\n\n\n# Mock the entire sounddevice module\n@pytest.fixture\ndef mock_sounddevice():\n    mock_sd = MagicMock()\n    mock_sd.query_devices.return_value = [\n        {\"name\": \"test_device\", \"max_input_channels\": 2},\n        {\"name\": \"another_device\", \"max_input_channels\": 1},\n    ]\n    with patch.dict(\"sys.modules\", {\"sounddevice\": mock_sd}):\n        yield mock_sd\n\n\n@pytest.fixture\ndef mock_soundfile():\n    with patch(\"aider.voice.sf\") as mock_sf:\n        yield mock_sf\n\n\ndef test_voice_init_default_device(mock_sounddevice):\n    voice = Voice()\n    assert voice.device_id is None\n    assert voice.audio_format == \"wav\"\n    assert voice.sd == mock_sounddevice\n\n\ndef test_voice_init_specific_device(mock_sounddevice):\n    voice = Voice(device_name=\"test_device\")\n    assert voice.device_id == 0\n    assert voice.sd == mock_sounddevice\n\n\ndef test_voice_init_invalid_device(mock_sounddevice):\n    with pytest.raises(ValueError) as exc:\n        Voice(device_name=\"nonexistent_device\")\n    assert \"Device\" in str(exc.value)\n    assert \"not found\" in str(exc.value)\n\n\ndef test_voice_init_invalid_format():\n    with patch(\"aider.voice.sf\", MagicMock()):  # Need to mock sf to avoid SoundDeviceError\n        with pytest.raises(ValueError) as exc:\n            Voice(audio_format=\"invalid\")\n        assert \"Unsupported audio format\" in str(exc.value)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_voice.py::2",
    "metadata": {
      "file_path": "tests/basic/test_voice.py",
      "file_name": "test_voice.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 166,
      "span_ids": [
        "test_callback_processing"
      ],
      "start_line": 56,
      "end_line": 72,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def test_callback_processing():\n    with patch(\"aider.voice.sf\", MagicMock()):  # Need to mock sf to avoid SoundDeviceError\n        voice = Voice()\n        voice.q = queue.Queue()\n\n        # Test with silence (low amplitude)\n        test_data = np.zeros((1000, 1))\n        voice.callback(test_data, None, None, None)\n        assert voice.pct == 0.5  # When range is too small (<=0.001), pct is set to 0.5\n\n        # Test with loud signal (high amplitude)\n        test_data = np.ones((1000, 1))\n        voice.callback(test_data, None, None, None)\n        assert voice.pct > 0.9\n\n        # Verify data is queued\n        assert not voice.q.empty()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_voice.py::3",
    "metadata": {
      "file_path": "tests/basic/test_voice.py",
      "file_name": "test_voice.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 240,
      "span_ids": [
        "test_record_and_transcribe_device_error",
        "test_get_prompt",
        "test_record_and_transcribe_keyboard_interrupt"
      ],
      "start_line": 75,
      "end_line": 104,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def test_get_prompt():\n    with patch(\"aider.voice.sf\", MagicMock()):  # Need to mock sf to avoid SoundDeviceError\n        voice = Voice()\n        voice.start_time = os.times().elapsed\n        voice.pct = 0.5  # 50% volume level\n\n        prompt = voice.get_prompt()\n        assert \"Recording\" in prompt\n        assert \"sec\" in prompt\n        assert \"\u2588\" in prompt  # Should contain some filled blocks\n        assert \"\u2591\" in prompt  # Should contain some empty blocks\n\n\ndef test_record_and_transcribe_keyboard_interrupt():\n    with patch(\"aider.voice.sf\", MagicMock()):\n        voice = Voice()\n        with patch.object(voice, \"raw_record_and_transcribe\", side_effect=KeyboardInterrupt()):\n            result = voice.record_and_transcribe()\n            assert result is None\n\n\ndef test_record_and_transcribe_device_error():\n    with patch(\"aider.voice.sf\", MagicMock()):\n        voice = Voice()\n        with patch.object(\n            voice, \"raw_record_and_transcribe\", side_effect=SoundDeviceError(\"Test error\")\n        ):\n            result = voice.record_and_transcribe()\n            assert result is None",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_watch.py::1",
    "metadata": {
      "file_path": "tests/basic/test_watch.py",
      "file_name": "test_watch.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 506,
      "span_ids": [
        "imports",
        "test_gitignore_patterns"
      ],
      "start_line": 1,
      "end_line": 61,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from pathlib import Path\n\nfrom aider.io import InputOutput\nfrom aider.watch import FileWatcher\n\n\ndef test_gitignore_patterns():\n    \"\"\"Test that gitignore patterns are properly loaded and matched\"\"\"\n    from pathlib import Path\n\n    from aider.watch import load_gitignores\n\n    # Create a temporary gitignore file with test patterns\n    tmp_gitignore = Path(\"test.gitignore\")\n    tmp_gitignore.write_text(\"custom_pattern\\n*.custom\")\n\n    gitignores = [tmp_gitignore]\n    spec = load_gitignores(gitignores)\n\n    # Test built-in patterns\n    assert spec.match_file(\".aider.conf\")\n    assert spec.match_file(\".git/config\")\n    assert spec.match_file(\"file~\")  # Emacs/vim backup\n    assert spec.match_file(\"file.bak\")\n    assert spec.match_file(\"file.swp\")\n    assert spec.match_file(\"file.swo\")\n    assert spec.match_file(\"#temp#\")  # Emacs auto-save\n    assert spec.match_file(\".#lock\")  # Emacs lock\n    assert spec.match_file(\"temp.tmp\")\n    assert spec.match_file(\"temp.temp\")\n    assert spec.match_file(\"conflict.orig\")\n    assert spec.match_file(\"script.pyc\")\n    assert spec.match_file(\"__pycache__/module.pyc\")\n    assert spec.match_file(\".DS_Store\")\n    assert spec.match_file(\"Thumbs.db\")\n    assert spec.match_file(\".idea/workspace.xml\")\n    assert spec.match_file(\".vscode/settings.json\")\n    assert spec.match_file(\"project.sublime-workspace\")\n    assert spec.match_file(\".project\")\n    assert spec.match_file(\".settings/config.json\")\n    assert spec.match_file(\"workspace.code-workspace\")\n    assert spec.match_file(\".env\")\n    assert spec.match_file(\".venv/bin/python\")\n    assert spec.match_file(\"node_modules/package/index.js\")\n    assert spec.match_file(\"vendor/lib/module.py\")\n    assert spec.match_file(\"debug.log\")\n    assert spec.match_file(\".cache/files\")\n    assert spec.match_file(\".pytest_cache/v/cache\")\n    assert spec.match_file(\"coverage/lcov.info\")\n\n    # Test custom patterns from gitignore file\n    assert spec.match_file(\"custom_pattern\")\n    assert spec.match_file(\"file.custom\")\n\n    # Test non-matching patterns\n    assert not spec.match_file(\"regular_file.txt\")\n    assert not spec.match_file(\"src/main.py\")\n    assert not spec.match_file(\"docs/index.html\")\n\n    # Cleanup\n    tmp_gitignore.unlink()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_watch.py::2",
    "metadata": {
      "file_path": "tests/basic/test_watch.py",
      "file_name": "test_watch.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 477,
      "span_ids": [
        "test_ai_comment_pattern"
      ],
      "start_line": 64,
      "end_line": 116,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def test_ai_comment_pattern():\n    # Create minimal IO and Coder instances for testing\n    class MinimalCoder:\n        def __init__(self, io):\n            self.io = io\n            self.root = \".\"\n            self.abs_fnames = set()\n\n        def get_rel_fname(self, fname):\n            return fname\n\n    io = InputOutput(pretty=False, fancy_input=False, yes=False)\n    coder = MinimalCoder(io)\n    watcher = FileWatcher(coder)\n    fixtures_dir = Path(__file__).parent.parent / \"fixtures\"\n\n    # Test Python fixture\n    py_path = fixtures_dir / \"watch.py\"\n    py_lines, py_comments, py_has_bang = watcher.get_ai_comments(str(py_path))\n\n    # Count unique AI comments (excluding duplicates and variations with extra spaces)\n    unique_py_comments = set(comment.strip().lower() for comment in py_comments)\n\n    py_expected = 10\n    assert len(unique_py_comments) == 10, (\n        f\"Expected {py_expected} unique AI comments in Python fixture, found\"\n        f\" {len(unique_py_comments)}\"\n    )\n    assert py_has_bang == \"!\", \"Expected at least one bang (!) comment in Python fixture\"\n\n    # Test JavaScript fixture\n    js_path = fixtures_dir / \"watch.js\"\n    js_lines, js_comments, js_has_bang = watcher.get_ai_comments(str(js_path))\n    js_expected = 16\n    assert (\n        len(js_lines) == js_expected\n    ), f\"Expected {js_expected} AI comments in JavaScript fixture, found {len(js_lines)}\"\n    assert js_has_bang == \"!\", \"Expected at least one bang (!) comment in JavaScript fixture\"\n\n    # Test watch_question.js fixture\n    question_js_path = fixtures_dir / \"watch_question.js\"\n    question_js_lines, question_js_comments, question_js_has_bang = watcher.get_ai_comments(\n        str(question_js_path)\n    )\n    question_js_expected = 6\n    assert len(question_js_lines) == question_js_expected, (\n        f\"Expected {question_js_expected} AI comments in watch_question.js fixture, found\"\n        f\" {len(question_js_lines)}\"\n    )\n    assert (\n        question_js_has_bang == \"?\"\n    ), \"Expected at least one bang (!) comment in watch_question.js fixture\"",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_wholefile.py::1",
    "metadata": {
      "file_path": "tests/basic/test_wholefile.py",
      "file_name": "test_wholefile.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 281,
      "span_ids": [
        "TestWholeFileCoder.setUp",
        "TestWholeFileCoder",
        "TestWholeFileCoder.tearDown",
        "TestWholeFileCoder.test_no_files",
        "imports"
      ],
      "start_line": 1,
      "end_line": 39,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import os\nimport shutil\nimport tempfile\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import MagicMock\n\nfrom aider.coders import Coder\nfrom aider.coders.wholefile_coder import WholeFileCoder\nfrom aider.dump import dump  # noqa: F401\nfrom aider.io import InputOutput\nfrom aider.models import Model\n\n\nclass TestWholeFileCoder(unittest.TestCase):\n    def setUp(self):\n        self.original_cwd = os.getcwd()\n        self.tempdir = tempfile.mkdtemp()\n        os.chdir(self.tempdir)\n\n        self.GPT35 = Model(\"gpt-3.5-turbo\")\n\n    def tearDown(self):\n        os.chdir(self.original_cwd)\n        shutil.rmtree(self.tempdir, ignore_errors=True)\n\n    def test_no_files(self):\n        # Initialize WholeFileCoder with the temporary directory\n        io = InputOutput(yes=True)\n\n        coder = WholeFileCoder(main_model=self.GPT35, io=io, fnames=[])\n        coder.partial_response_content = (\n            'To print \"Hello, World!\" in most programming languages, you can use the following'\n            ' code:\\n\\n```python\\nprint(\"Hello, World!\")\\n```\\n\\nThis code will output \"Hello,'\n            ' World!\" to the console.'\n        )\n\n        # This is throwing ValueError!\n        coder.render_incremental_response(True)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_wholefile.py::2",
    "metadata": {
      "file_path": "tests/basic/test_wholefile.py",
      "file_name": "test_wholefile.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 141,
      "span_ids": [
        "TestWholeFileCoder.test_no_files_new_file_should_ask"
      ],
      "start_line": 41,
      "end_line": 50,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestWholeFileCoder(unittest.TestCase):\n\n    def test_no_files_new_file_should_ask(self):\n        io = InputOutput(yes=False)  # <- yes=FALSE\n        coder = WholeFileCoder(main_model=self.GPT35, io=io, fnames=[])\n        coder.partial_response_content = (\n            'To print \"Hello, World!\" in most programming languages, you can use the following'\n            ' code:\\n\\nfoo.js\\n```python\\nprint(\"Hello, World!\")\\n```\\n\\nThis code will output'\n            ' \"Hello, World!\" to the console.'\n        )\n        coder.apply_updates()\n        self.assertFalse(Path(\"foo.js\").exists())",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_wholefile.py::3",
    "metadata": {
      "file_path": "tests/basic/test_wholefile.py",
      "file_name": "test_wholefile.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 207,
      "span_ids": [
        "TestWholeFileCoder.test_update_files"
      ],
      "start_line": 52,
      "end_line": 74,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestWholeFileCoder(unittest.TestCase):\n\n    def test_update_files(self):\n        # Create a sample file in the temporary directory\n        sample_file = \"sample.txt\"\n        with open(sample_file, \"w\") as f:\n            f.write(\"Original content\\n\")\n\n        # Initialize WholeFileCoder with the temporary directory\n        io = InputOutput(yes=True)\n        coder = WholeFileCoder(main_model=self.GPT35, io=io, fnames=[sample_file])\n\n        # Set the partial response content with the updated content\n        coder.partial_response_content = f\"{sample_file}\\n```\\nUpdated content\\n```\"\n\n        # Call update_files method\n        edited_files = coder.apply_updates()\n\n        # Check if the sample file was updated\n        self.assertIn(\"sample.txt\", edited_files)\n\n        # Check if the content of the sample file was updated\n        with open(sample_file, \"r\") as f:\n            updated_content = f.read()\n        self.assertEqual(updated_content, \"Updated content\\n\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_wholefile.py::4",
    "metadata": {
      "file_path": "tests/basic/test_wholefile.py",
      "file_name": "test_wholefile.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 184,
      "span_ids": [
        "TestWholeFileCoder.test_update_files_live_diff"
      ],
      "start_line": 76,
      "end_line": 92,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestWholeFileCoder(unittest.TestCase):\n\n    def test_update_files_live_diff(self):\n        # Create a sample file in the temporary directory\n        sample_file = \"sample.txt\"\n        with open(sample_file, \"w\") as f:\n            f.write(\"\\n\".join(map(str, range(0, 100))))\n\n        # Initialize WholeFileCoder with the temporary directory\n        io = InputOutput(yes=True)\n        coder = WholeFileCoder(main_model=self.GPT35, io=io, fnames=[sample_file])\n\n        # Set the partial response content with the updated content\n        coder.partial_response_content = f\"{sample_file}\\n```\\n0\\n\\1\\n2\\n\"\n\n        lines = coder.get_edits(mode=\"diff\").splitlines()\n\n        # the live diff should be concise, since we haven't changed anything yet\n        self.assertLess(len(lines), 20)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_wholefile.py::5",
    "metadata": {
      "file_path": "tests/basic/test_wholefile.py",
      "file_name": "test_wholefile.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 260,
      "span_ids": [
        "TestWholeFileCoder.test_update_files_with_existing_fence"
      ],
      "start_line": 94,
      "end_line": 128,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestWholeFileCoder(unittest.TestCase):\n\n    def test_update_files_with_existing_fence(self):\n        # Create a sample file in the temporary directory\n        sample_file = \"sample.txt\"\n        original_content = \"\"\"\nHere is some quoted text:\n```\nQuote!\n```\n\"\"\"\n        with open(sample_file, \"w\") as f:\n            f.write(original_content)\n\n        # Initialize WholeFileCoder with the temporary directory\n        io = InputOutput(yes=True)\n        coder = WholeFileCoder(main_model=self.GPT35, io=io, fnames=[sample_file])\n\n        coder.choose_fence()\n\n        self.assertNotEqual(coder.fence[0], \"```\")\n\n        # Set the partial response content with the updated content\n        coder.partial_response_content = (\n            f\"{sample_file}\\n{coder.fence[0]}\\nUpdated content\\n{coder.fence[1]}\"\n        )\n\n        # Call update_files method\n        edited_files = coder.apply_updates()\n\n        # Check if the sample file was updated\n        self.assertIn(\"sample.txt\", edited_files)\n\n        # Check if the content of the sample file was updated\n        with open(sample_file, \"r\") as f:\n            updated_content = f.read()\n        self.assertEqual(updated_content, \"Updated content\\n\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_wholefile.py::6",
    "metadata": {
      "file_path": "tests/basic/test_wholefile.py",
      "file_name": "test_wholefile.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 226,
      "span_ids": [
        "TestWholeFileCoder.test_update_files_bogus_path_prefix"
      ],
      "start_line": 130,
      "end_line": 153,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestWholeFileCoder(unittest.TestCase):\n\n    def test_update_files_bogus_path_prefix(self):\n        # Create a sample file in the temporary directory\n        sample_file = \"sample.txt\"\n        with open(sample_file, \"w\") as f:\n            f.write(\"Original content\\n\")\n\n        # Initialize WholeFileCoder with the temporary directory\n        io = InputOutput(yes=True)\n        coder = WholeFileCoder(main_model=self.GPT35, io=io, fnames=[sample_file])\n\n        # Set the partial response content with the updated content\n        # With path/to/ prepended onto the filename\n        coder.partial_response_content = f\"path/to/{sample_file}\\n```\\nUpdated content\\n```\"\n\n        # Call update_files method\n        edited_files = coder.apply_updates()\n\n        # Check if the sample file was updated\n        self.assertIn(\"sample.txt\", edited_files)\n\n        # Check if the content of the sample file was updated\n        with open(sample_file, \"r\") as f:\n            updated_content = f.read()\n        self.assertEqual(updated_content, \"Updated content\\n\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_wholefile.py::7",
    "metadata": {
      "file_path": "tests/basic/test_wholefile.py",
      "file_name": "test_wholefile.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 204,
      "span_ids": [
        "TestWholeFileCoder.test_update_files_not_in_chat"
      ],
      "start_line": 155,
      "end_line": 177,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestWholeFileCoder(unittest.TestCase):\n\n    def test_update_files_not_in_chat(self):\n        # Create a sample file in the temporary directory\n        sample_file = \"sample.txt\"\n        with open(sample_file, \"w\") as f:\n            f.write(\"Original content\\n\")\n\n        # Initialize WholeFileCoder with the temporary directory\n        io = InputOutput(yes=True)\n        coder = WholeFileCoder(main_model=self.GPT35, io=io)\n\n        # Set the partial response content with the updated content\n        coder.partial_response_content = f\"{sample_file}\\n```\\nUpdated content\\n```\"\n\n        # Call update_files method\n        edited_files = coder.apply_updates()\n\n        # Check if the sample file was updated\n        self.assertIn(\"sample.txt\", edited_files)\n\n        # Check if the content of the sample file was updated\n        with open(sample_file, \"r\") as f:\n            updated_content = f.read()\n        self.assertEqual(updated_content, \"Updated content\\n\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_wholefile.py::8",
    "metadata": {
      "file_path": "tests/basic/test_wholefile.py",
      "file_name": "test_wholefile.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 292,
      "span_ids": [
        "TestWholeFileCoder.test_update_files_no_filename_single_file_in_chat"
      ],
      "start_line": 179,
      "end_line": 210,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestWholeFileCoder(unittest.TestCase):\n\n    def test_update_files_no_filename_single_file_in_chat(self):\n        sample_file = \"accumulate.py\"\n        content = (\n            \"def accumulate(collection, operation):\\n    return [operation(x) for x in\"\n            \" collection]\\n\"\n        )\n\n        with open(sample_file, \"w\") as f:\n            f.write(\"Original content\\n\")\n\n        # Initialize WholeFileCoder with the temporary directory\n        io = InputOutput(yes=True)\n        coder = WholeFileCoder(main_model=self.GPT35, io=io, fnames=[sample_file])\n\n        # Set the partial response content with the updated content\n        coder.partial_response_content = (\n            f\"Here's the modified `{sample_file}` file that implements the `accumulate`\"\n            f\" function as per the given instructions:\\n\\n```\\n{content}```\\n\\nThis\"\n            \" implementation uses a list comprehension to apply the `operation` function to\"\n            \" each element of the `collection` and returns the resulting list.\"\n        )\n\n        # Call update_files method\n        edited_files = coder.apply_updates()\n\n        # Check if the sample file was updated\n        self.assertIn(sample_file, edited_files)\n\n        # Check if the content of the sample file was updated\n        with open(sample_file, \"r\") as f:\n            updated_content = f.read()\n        self.assertEqual(updated_content, content)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_wholefile.py::9",
    "metadata": {
      "file_path": "tests/basic/test_wholefile.py",
      "file_name": "test_wholefile.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 236,
      "span_ids": [
        "TestWholeFileCoder.test_update_files_earlier_filename"
      ],
      "start_line": 212,
      "end_line": 247,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestWholeFileCoder(unittest.TestCase):\n\n    def test_update_files_earlier_filename(self):\n        fname_a = Path(\"a.txt\")\n        fname_b = Path(\"b.txt\")\n\n        fname_a.write_text(\"before a\\n\")\n        fname_b.write_text(\"before b\\n\")\n\n        response = \"\"\"\nHere is a new version of `a.txt` for you to consider:\n\n```\nafter a\n```\n\nAnd here is `b.txt`:\n\n```\nafter b\n```\n\"\"\"\n        # Initialize WholeFileCoder with the temporary directory\n        io = InputOutput(yes=True)\n        coder = WholeFileCoder(main_model=self.GPT35, io=io, fnames=[fname_a, fname_b])\n\n        # Set the partial response content with the updated content\n        coder.partial_response_content = response\n\n        # Call update_files method\n        edited_files = coder.apply_updates()\n\n        # Check if the sample file was updated\n        self.assertIn(str(fname_a), edited_files)\n        self.assertIn(str(fname_b), edited_files)\n\n        self.assertEqual(fname_a.read_text(), \"after a\\n\")\n        self.assertEqual(fname_b.read_text(), \"after b\\n\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_wholefile.py::10",
    "metadata": {
      "file_path": "tests/basic/test_wholefile.py",
      "file_name": "test_wholefile.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 224,
      "span_ids": [
        "TestWholeFileCoder.test_update_hash_filename"
      ],
      "start_line": 249,
      "end_line": 285,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestWholeFileCoder(unittest.TestCase):\n\n    def test_update_hash_filename(self):\n        fname_a = Path(\"a.txt\")\n        fname_b = Path(\"b.txt\")\n\n        fname_a.write_text(\"before a\\n\")\n        fname_b.write_text(\"before b\\n\")\n\n        response = \"\"\"\n\n### a.txt\n```\nafter a\n```\n\n### b.txt\n```\nafter b\n```\n\"\"\"\n        # Initialize WholeFileCoder with the temporary directory\n        io = InputOutput(yes=True)\n        coder = WholeFileCoder(main_model=self.GPT35, io=io, fnames=[fname_a, fname_b])\n\n        # Set the partial response content with the updated content\n        coder.partial_response_content = response\n\n        # Call update_files method\n        edited_files = coder.apply_updates()\n\n        dump(edited_files)\n\n        # Check if the sample file was updated\n        self.assertIn(str(fname_a), edited_files)\n        self.assertIn(str(fname_b), edited_files)\n\n        self.assertEqual(fname_a.read_text(), \"after a\\n\")\n        self.assertEqual(fname_b.read_text(), \"after b\\n\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_wholefile.py::11",
    "metadata": {
      "file_path": "tests/basic/test_wholefile.py",
      "file_name": "test_wholefile.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 307,
      "span_ids": [
        "TestWholeFileCoder.test_update_named_file_but_extra_unnamed_code_block"
      ],
      "start_line": 287,
      "end_line": 317,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestWholeFileCoder(unittest.TestCase):\n\n    def test_update_named_file_but_extra_unnamed_code_block(self):\n        sample_file = \"hello.py\"\n        new_content = \"new\\ncontent\\ngoes\\nhere\\n\"\n\n        with open(sample_file, \"w\") as f:\n            f.write(\"Original content\\n\")\n\n        # Initialize WholeFileCoder with the temporary directory\n        io = InputOutput(yes=True)\n        coder = WholeFileCoder(main_model=self.GPT35, io=io, fnames=[sample_file])\n\n        # Set the partial response content with the updated content\n        coder.partial_response_content = (\n            f\"Here's the modified `{sample_file}` file that implements the `accumulate`\"\n            f\" function as per the given instructions:\\n\\n```\\n{new_content}```\\n\\nThis\"\n            \" implementation uses a list comprehension to apply the `operation` function to\"\n            \" each element of the `collection` and returns the resulting list.\\n\"\n            \"Run it like this:\\n\\n\"\n            \"```\\npython {sample_file}\\n```\\n\\n\"\n        )\n\n        # Call update_files method\n        edited_files = coder.apply_updates()\n\n        # Check if the sample file was updated\n        self.assertIn(sample_file, edited_files)\n\n        # Check if the content of the sample file was updated\n        with open(sample_file, \"r\") as f:\n            updated_content = f.read()\n        self.assertEqual(updated_content, new_content)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "basic/test_wholefile.py::12",
    "metadata": {
      "file_path": "tests/basic/test_wholefile.py",
      "file_name": "test_wholefile.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 265,
      "span_ids": [
        "TestWholeFileCoder.test_full_edit",
        "impl"
      ],
      "start_line": 319,
      "end_line": 360,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestWholeFileCoder(unittest.TestCase):\n\n    def test_full_edit(self):\n        # Create a few temporary files\n        _, file1 = tempfile.mkstemp()\n\n        with open(file1, \"w\", encoding=\"utf-8\") as f:\n            f.write(\"one\\ntwo\\nthree\\n\")\n\n        files = [file1]\n\n        # Initialize the Coder object with the mocked IO and mocked repo\n        coder = Coder.create(self.GPT35, \"whole\", io=InputOutput(), fnames=files, stream=False)\n\n        # no trailing newline so the response content below doesn't add ANOTHER newline\n        new_content = \"new\\ntwo\\nthree\"\n\n        def mock_send(*args, **kwargs):\n            coder.partial_response_content = f\"\"\"\nDo this:\n\n{Path(file1).name}\n```\n{new_content}\n```\n\n\"\"\"\n            coder.partial_response_function_call = dict()\n            return []\n\n        coder.send = MagicMock(side_effect=mock_send)\n\n        # Call the run method with a message\n        coder.run(with_message=\"hi\")\n\n        content = Path(file1).read_text(encoding=\"utf-8\")\n\n        # check for one trailing newline\n        self.assertEqual(content, new_content + \"\\n\")\n\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "browser/test_browser.py::1",
    "metadata": {
      "file_path": "tests/browser/test_browser.py",
      "file_name": "test_browser.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 191,
      "span_ids": [
        "TestBrowser",
        "imports",
        "TestBrowser.test_browser_flag_imports_streamlit",
        "impl"
      ],
      "start_line": 1,
      "end_line": 35,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import os\nimport unittest\nfrom unittest.mock import patch\n\nfrom aider.main import main\n\n\nclass TestBrowser(unittest.TestCase):\n    @patch(\"aider.main.launch_gui\")\n    def test_browser_flag_imports_streamlit(self, mock_launch_gui):\n        os.environ[\"AIDER_ANALYTICS\"] = \"false\"\n\n        # Run main with --browser and --yes flags\n        main([\"--browser\", \"--yes\"])\n\n        # Check that launch_gui was called\n        mock_launch_gui.assert_called_once()\n\n        # Try to import streamlit\n        try:\n            import streamlit  # noqa: F401\n\n            streamlit_imported = True\n        except ImportError:\n            streamlit_imported = False\n\n        # Assert that streamlit was successfully imported\n        self.assertTrue(\n            streamlit_imported, \"Streamlit should be importable after running with --browser flag\"\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "python/test.py::1",
    "metadata": {
      "file_path": "tests/fixtures/languages/python/test.py",
      "file_name": "test.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 169,
      "span_ids": [
        "Person.__init__",
        "create_greeting_list",
        "Person",
        "Person.greet",
        "imports",
        "impl"
      ],
      "start_line": 1,
      "end_line": 29,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "from typing import List, Optional\n\n\nclass Person:\n    \"\"\"A class representing a person.\"\"\"\n\n    def __init__(self, name: str, age: Optional[int] = None):\n        self.name = name\n        self.age = age\n\n    def greet(self, formal: bool = False) -> str:\n        \"\"\"Generate a greeting.\"\"\"\n        prefix = \"Good day\" if formal else \"Hello\"\n        return f\"{prefix}, {self.name}!\"\n\n\ndef create_greeting_list(people: List[Person]) -> List[str]:\n    \"\"\"Create greetings for a list of people.\"\"\"\n    return [person.greet() for person in people]\n\n\n# Constants\nDEFAULT_NAME = \"World\"\nMAX_AGE = 150\n\nif __name__ == \"__main__\":\n    person = Person(DEFAULT_NAME)\n    print(person.greet())",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "sample-code-base/sample.py::1",
    "metadata": {
      "file_path": "tests/fixtures/sample-code-base/sample.py",
      "file_name": "sample.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 135,
      "span_ids": [
        "Car.__init__",
        "Car.honk",
        "Car",
        "Car.brake",
        "Car.accelerate"
      ],
      "start_line": 1,
      "end_line": 17,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Car:\n    def __init__(self, make, model, year):\n        self.make = make\n        self.model = model\n        self.year = year\n        self.speed = 0\n\n    def accelerate(self, increment):\n        self.speed += increment\n        print(f\"{self.make} {self.model} is now going {self.speed} mph.\")\n\n    def brake(self, decrement):\n        self.speed = max(0, self.speed - decrement)\n        print(f\"{self.make} {self.model} slowed down to {self.speed} mph.\")\n\n    def honk(self):\n        print(f\"{self.make} {self.model}: Beep beep!\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "sample-code-base/sample.py::2",
    "metadata": {
      "file_path": "tests/fixtures/sample-code-base/sample.py",
      "file_name": "sample.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 164,
      "span_ids": [
        "Garage.add_car",
        "Garage.remove_car",
        "Garage.__init__",
        "Garage",
        "Garage.list_cars"
      ],
      "start_line": 20,
      "end_line": 41,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class Garage:\n    def __init__(self):\n        self.cars = []\n\n    def add_car(self, car):\n        self.cars.append(car)\n        print(f\"Added {car.make} {car.model} to the garage.\")\n\n    def remove_car(self, car):\n        if car in self.cars:\n            self.cars.remove(car)\n            print(f\"Removed {car.make} {car.model} from the garage.\")\n        else:\n            print(f\"{car.make} {car.model} is not in the garage.\")\n\n    def list_cars(self):\n        if self.cars:\n            print(\"Cars in the garage:\")\n            for car in self.cars:\n                print(f\"- {car.year} {car.make} {car.model}\")\n        else:\n            print(\"The garage is empty.\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "sample-code-base/sample.py::3",
    "metadata": {
      "file_path": "tests/fixtures/sample-code-base/sample.py",
      "file_name": "sample.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 165,
      "span_ids": [
        "impl",
        "main"
      ],
      "start_line": 44,
      "end_line": 69,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "def main():\n    # Create some cars\n    car1 = Car(\"Toyota\", \"Corolla\", 2020)\n    car2 = Car(\"Tesla\", \"Model 3\", 2022)\n\n    # Demonstrate car methods\n    car1.accelerate(30)\n    car1.honk()\n    car1.brake(10)\n\n    # Create a garage and add cars\n    my_garage = Garage()\n    my_garage.add_car(car1)\n    my_garage.add_car(car2)\n\n    # List cars in the garage\n    my_garage.list_cars()\n\n    # Remove a car and list again\n    my_garage.remove_car(car1)\n    my_garage.list_cars()\n\n\nif __name__ == \"__main__\":\n    main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "fixtures/watch.py::1",
    "metadata": {
      "file_path": "tests/fixtures/watch.py",
      "file_name": "watch.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 126,
      "span_ids": [
        "dummy_function",
        "docstring"
      ],
      "start_line": 1,
      "end_line": 22,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "# fmt: off\n# flake8: noqa\n\n# Regular not AI comment\n#ai 1 do something\n# AI 2 make this better\n# ai! 3 urgent change needed\n#AI! 4 another urgent one\n# this is not an ai comment\n# aider is not an ai comment\n# not an ai! comment\n\n\ndef dummy_function():\n    #ai inside 5 function\n    # final 6 ai!\n    # final 7 ai\n    # ai\n    #ai\n    # those are 8+9\n    pass  # ai 10\r",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "help/test_help.py::1",
    "metadata": {
      "file_path": "tests/help/test_help.py",
      "file_name": "test_help.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 218,
      "span_ids": [
        "TestHelp",
        "imports",
        "TestHelp.test_init",
        "TestHelp.setUpClass"
      ],
      "start_line": 1,
      "end_line": 37,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import unittest\nfrom unittest.mock import MagicMock\n\nimport aider\nfrom aider.coders import Coder\nfrom aider.commands import Commands\nfrom aider.help import Help, fname_to_url\nfrom aider.io import InputOutput\nfrom aider.models import Model\n\n\nclass TestHelp(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        io = InputOutput(pretty=False, yes=True)\n\n        GPT35 = Model(\"gpt-3.5-turbo\")\n\n        coder = Coder.create(GPT35, None, io)\n        commands = Commands(io, coder)\n\n        help_coder_run = MagicMock(return_value=\"\")\n        aider.coders.HelpCoder.run = help_coder_run\n\n        try:\n            commands.cmd_help(\"hi\")\n        except aider.commands.SwitchCoder:\n            pass\n        else:\n            # If no exception was raised, fail the test\n            assert False, \"SwitchCoder exception was not raised\"\n\n        help_coder_run.assert_called_once()\n\n    def test_init(self):\n        help_inst = Help()\n        self.assertIsNotNone(help_inst.retriever)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "help/test_help.py::2",
    "metadata": {
      "file_path": "tests/help/test_help.py",
      "file_name": "test_help.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 154,
      "span_ids": [
        "TestHelp.test_ask_without_mock"
      ],
      "start_line": 39,
      "end_line": 55,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestHelp(unittest.TestCase):\n\n    def test_ask_without_mock(self):\n        help_instance = Help()\n        question = \"What is aider?\"\n        result = help_instance.ask(question)\n\n        self.assertIn(f\"# Question: {question}\", result)\n        self.assertIn(\"<doc\", result)\n        self.assertIn(\"</doc>\", result)\n        self.assertGreater(len(result), 100)  # Ensure we got a substantial response\n\n        # Check for some expected content (adjust based on your actual help content)\n        self.assertIn(\"aider\", result.lower())\n        self.assertIn(\"ai\", result.lower())\n        self.assertIn(\"chat\", result.lower())\n\n        # Assert that there are more than 5 <doc> entries\n        self.assertGreater(result.count(\"<doc\"), 5)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "help/test_help.py::3",
    "metadata": {
      "file_path": "tests/help/test_help.py",
      "file_name": "test_help.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 172,
      "span_ids": [
        "TestHelp.test_fname_to_url_unix"
      ],
      "start_line": 57,
      "end_line": 73,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestHelp(unittest.TestCase):\n\n    def test_fname_to_url_unix(self):\n        # Test relative Unix-style paths\n        self.assertEqual(fname_to_url(\"website/docs/index.md\"), \"https://aider.chat/docs\")\n        self.assertEqual(\n            fname_to_url(\"website/docs/usage.md\"), \"https://aider.chat/docs/usage.html\"\n        )\n        self.assertEqual(fname_to_url(\"website/_includes/header.md\"), \"\")\n\n        # Test absolute Unix-style paths\n        self.assertEqual(\n            fname_to_url(\"/home/user/project/website/docs/index.md\"), \"https://aider.chat/docs\"\n        )\n        self.assertEqual(\n            fname_to_url(\"/home/user/project/website/docs/usage.md\"),\n            \"https://aider.chat/docs/usage.html\",\n        )\n        self.assertEqual(fname_to_url(\"/home/user/project/website/_includes/header.md\"), \"\")",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "help/test_help.py::4",
    "metadata": {
      "file_path": "tests/help/test_help.py",
      "file_name": "test_help.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 309,
      "span_ids": [
        "impl",
        "TestHelp.test_fname_to_url_windows",
        "TestHelp.test_fname_to_url_edge_cases"
      ],
      "start_line": 75,
      "end_line": 107,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestHelp(unittest.TestCase):\n\n    def test_fname_to_url_windows(self):\n        # Test relative Windows-style paths\n        self.assertEqual(fname_to_url(r\"website\\docs\\index.md\"), \"https://aider.chat/docs\")\n        self.assertEqual(\n            fname_to_url(r\"website\\docs\\usage.md\"), \"https://aider.chat/docs/usage.html\"\n        )\n        self.assertEqual(fname_to_url(r\"website\\_includes\\header.md\"), \"\")\n\n        # Test absolute Windows-style paths\n        self.assertEqual(\n            fname_to_url(r\"C:\\Users\\user\\project\\website\\docs\\index.md\"), \"https://aider.chat/docs\"\n        )\n        self.assertEqual(\n            fname_to_url(r\"C:\\Users\\user\\project\\website\\docs\\usage.md\"),\n            \"https://aider.chat/docs/usage.html\",\n        )\n        self.assertEqual(fname_to_url(r\"C:\\Users\\user\\project\\website\\_includes\\header.md\"), \"\")\n\n    def test_fname_to_url_edge_cases(self):\n        # Test paths that don't contain 'website'\n        self.assertEqual(fname_to_url(\"/home/user/project/docs/index.md\"), \"\")\n        self.assertEqual(fname_to_url(r\"C:\\Users\\user\\project\\docs\\index.md\"), \"\")\n\n        # Test empty path\n        self.assertEqual(fname_to_url(\"\"), \"\")\n\n        # Test path with 'website' in the wrong place\n        self.assertEqual(fname_to_url(\"/home/user/website_project/docs/index.md\"), \"\")\n\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scrape/test_scrape.py::1",
    "metadata": {
      "file_path": "tests/scrape/test_scrape.py",
      "file_name": "test_scrape.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 261,
      "span_ids": [
        "imports",
        "TestScrape.test_scrape_self_signed_ssl",
        "TestScrape"
      ],
      "start_line": 1,
      "end_line": 35,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "import time\nimport unittest\nfrom unittest.mock import MagicMock\n\nfrom aider.commands import Commands\nfrom aider.io import InputOutput\nfrom aider.scrape import Scraper\n\n\nclass TestScrape(unittest.TestCase):\n    def test_scrape_self_signed_ssl(self):\n        def scrape_with_retries(scraper, url, max_retries=5, delay=0.5):\n            for _ in range(max_retries):\n                result = scraper.scrape(url)\n                if result is not None:\n                    return result\n                time.sleep(delay)\n            return None\n\n        # Test with SSL verification\n        scraper_verify = Scraper(\n            print_error=MagicMock(), playwright_available=True, verify_ssl=True\n        )\n        result_verify = scrape_with_retries(scraper_verify, \"https://self-signed.badssl.com\")\n        self.assertIsNone(result_verify)\n        scraper_verify.print_error.assert_called()\n\n        # Test without SSL verification\n        scraper_no_verify = Scraper(\n            print_error=MagicMock(), playwright_available=True, verify_ssl=False\n        )\n        result_no_verify = scrape_with_retries(scraper_no_verify, \"https://self-signed.badssl.com\")\n        self.assertIsNotNone(result_no_verify)\n        self.assertIn(\"self-signed\", result_no_verify)\n        scraper_no_verify.print_error.assert_not_called()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scrape/test_scrape.py::2",
    "metadata": {
      "file_path": "tests/scrape/test_scrape.py",
      "file_name": "test_scrape.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 206,
      "span_ids": [
        "TestScrape.setUp",
        "TestScrape.test_cmd_web_imports_playwright"
      ],
      "start_line": 37,
      "end_line": 67,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestScrape(unittest.TestCase):\n\n    def setUp(self):\n        self.io = InputOutput(yes=True)\n        self.commands = Commands(self.io, None)\n\n    def test_cmd_web_imports_playwright(self):\n        # Create a mock print_error function\n        mock_print_error = MagicMock()\n        self.commands.io.tool_error = mock_print_error\n\n        # Run the cmd_web command\n        result = self.commands.cmd_web(\"https://example.com\", return_content=True)\n\n        # Assert that the result contains some content\n        self.assertIsNotNone(result)\n        self.assertNotEqual(result, \"\")\n\n        # Try to import playwright\n        try:\n            import playwright  # noqa: F401\n\n            playwright_imported = True\n        except ImportError:\n            playwright_imported = False\n\n        # Assert that playwright was successfully imported\n        self.assertTrue(\n            playwright_imported, \"Playwright should be importable after running cmd_web\"\n        )\n\n        # Assert that print_error was never called\n        mock_print_error.assert_not_called()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scrape/test_scrape.py::3",
    "metadata": {
      "file_path": "tests/scrape/test_scrape.py",
      "file_name": "test_scrape.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 119,
      "span_ids": [
        "TestScrape.test_scrape_actual_url_with_playwright"
      ],
      "start_line": 69,
      "end_line": 82,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestScrape(unittest.TestCase):\n\n    def test_scrape_actual_url_with_playwright(self):\n        # Create a Scraper instance with a mock print_error function\n        mock_print_error = MagicMock()\n        scraper = Scraper(print_error=mock_print_error, playwright_available=True)\n\n        # Scrape a real URL\n        result = scraper.scrape(\"https://example.com\")\n\n        # Assert that the result contains expected content\n        self.assertIsNotNone(result)\n        self.assertIn(\"Example Domain\", result)\n\n        # Assert that print_error was never called\n        mock_print_error.assert_not_called()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scrape/test_scrape.py::4",
    "metadata": {
      "file_path": "tests/scrape/test_scrape.py",
      "file_name": "test_scrape.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 124,
      "span_ids": [
        "TestScrape.test_scraper_print_error_not_called"
      ],
      "start_line": 84,
      "end_line": 95,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestScrape(unittest.TestCase):\n\n    def test_scraper_print_error_not_called(self):\n        # Create a Scraper instance with a mock print_error function\n        mock_print_error = MagicMock()\n        scraper = Scraper(print_error=mock_print_error)\n\n        # Test various methods of the Scraper class\n        scraper.scrape_with_httpx(\"https://example.com\")\n        scraper.try_pandoc()\n        scraper.html_to_markdown(\"<html><body><h1>Test</h1></body></html>\")\n\n        # Assert that print_error was never called\n        mock_print_error.assert_not_called()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scrape/test_scrape.py::5",
    "metadata": {
      "file_path": "tests/scrape/test_scrape.py",
      "file_name": "test_scrape.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 295,
      "span_ids": [
        "TestScrape.test_scrape_with_playwright_error_handling"
      ],
      "start_line": 97,
      "end_line": 136,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestScrape(unittest.TestCase):\n\n    def test_scrape_with_playwright_error_handling(self):\n        # Create a Scraper instance with a mock print_error function\n        mock_print_error = MagicMock()\n        scraper = Scraper(print_error=mock_print_error, playwright_available=True)\n\n        # Mock the playwright module to raise an error\n        import playwright\n\n        playwright._impl._errors.Error = Exception  # Mock the Error class\n\n        def mock_content():\n            raise playwright._impl._errors.Error(\"Test error\")\n\n        # Mock the necessary objects and methods\n        scraper.scrape_with_playwright = MagicMock()\n        scraper.scrape_with_playwright.return_value = (None, None)\n\n        # Call the scrape method\n        result = scraper.scrape(\"https://example.com\")\n\n        # Assert that the result is None\n        self.assertIsNone(result)\n\n        # Assert that print_error was called with the expected error message\n        mock_print_error.assert_called_once_with(\n            \"Failed to retrieve content from https://example.com\"\n        )\n\n        # Reset the mock\n        mock_print_error.reset_mock()\n\n        # Test with a different return value\n        scraper.scrape_with_playwright.return_value = (\"Some content\", \"text/html\")\n        result = scraper.scrape(\"https://example.com\")\n\n        # Assert that the result is not None\n        self.assertIsNotNone(result)\n\n        # Assert that print_error was not called\n        mock_print_error.assert_not_called()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scrape/test_scrape.py::6",
    "metadata": {
      "file_path": "tests/scrape/test_scrape.py",
      "file_name": "test_scrape.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 121,
      "span_ids": [
        "TestScrape.test_scrape_text_plain"
      ],
      "start_line": 138,
      "end_line": 150,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestScrape(unittest.TestCase):\n\n    def test_scrape_text_plain(self):\n        # Create a Scraper instance\n        scraper = Scraper(print_error=MagicMock(), playwright_available=True)\n\n        # Mock the scrape_with_playwright method\n        plain_text = \"This is plain text content.\"\n        scraper.scrape_with_playwright = MagicMock(return_value=(plain_text, \"text/plain\"))\n\n        # Call the scrape method\n        result = scraper.scrape(\"https://example.com\")\n\n        # Assert that the result is the same as the input plain text\n        self.assertEqual(result, plain_text)",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  },
  {
    "id": "scrape/test_scrape.py::7",
    "metadata": {
      "file_path": "tests/scrape/test_scrape.py",
      "file_name": "test_scrape.py",
      "file_type": "text/x-python",
      "category": "test",
      "tokens": 215,
      "span_ids": [
        "impl",
        "TestScrape.test_scrape_text_html"
      ],
      "start_line": 152,
      "end_line": 176,
      "imports": {},
      "contexts": [],
      "type": "chunk"
    },
    "content": "class TestScrape(unittest.TestCase):\n\n    def test_scrape_text_html(self):\n        # Create a Scraper instance\n        scraper = Scraper(print_error=MagicMock(), playwright_available=True)\n\n        # Mock the scrape_with_playwright method\n        html_content = \"<html><body><h1>Test</h1><p>This is HTML content.</p></body></html>\"\n        scraper.scrape_with_playwright = MagicMock(return_value=(html_content, \"text/html\"))\n\n        # Mock the html_to_markdown method\n        expected_markdown = \"# Test\\n\\nThis is HTML content.\"\n        scraper.html_to_markdown = MagicMock(return_value=expected_markdown)\n\n        # Call the scrape method\n        result = scraper.scrape(\"https://example.com\")\n\n        # Assert that the result is the expected markdown\n        self.assertEqual(result, expected_markdown)\n\n        # Assert that html_to_markdown was called with the HTML content\n        scraper.html_to_markdown.assert_called_once_with(html_content)\n\n\nif __name__ == \"__main__\":\n    unittest.main()",
    "input_type": "chunk",
    "summary": {
      "long_description": "",
      "short_description": "",
      "questions": []
    }
  }
]