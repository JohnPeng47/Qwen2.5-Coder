from src import PythonChunker, SourceRepo # codesearch-backend

from pathlib import Path
from typing import List, Tuple
from pydantic import BaseModel
from openai import OpenAI
import json
import argparse

from dotenv import load_dotenv
import os

load_dotenv()

OAI_API_KEY = os.getenv("OPENAI_API_KEY")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

def invoke_lm(model_name, prompt):
    if "deepseek" in model_name:
        client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.deepseek.com")

        messages = [{"role": "user", "content": prompt}]
        response = client.chat.completions.create(
            model=model_name,
            messages=messages
        )

        content = response.choices[0].message.content
        if model_name == "deepseek-reasoner":
            reasoning_content = response.choices[0].message.reasoning_content
            content = reasoning_content + "\n" + content

    elif "gpt" in model_name:
        client = OpenAI(api_key=OAI_API_KEY)

        messages = [{"role": "user", "content": prompt}]
        response = client.chat.completions.create(
            model=model_name,
            messages=messages
        )

        content = response.choices[0].message.content

    return content


def create_qa_pairs(repo_path,
                    outdir: str,
                    code_blocks: List[Tuple[str, str]], 
                    model_name: str,
                    qa_num: int,
                    chunked: bool = False,
                    reasoning: bool = False):
    
    outfile = f"{outdir}/{model_name}_{qa_num}_{'chunked' if chunked else 'file'}_{'reason' if reasoning else 'noreason'}.json"
    prompt = """
{code_block}
        
Given the following source code chunk, generate a single question that a user might ask about this
codebase
"""
    print("Writing data to ", outfile)

    qa_data_output = {
        "repo_path": str(repo_path),
        "chunked": chunked,
        "model_name": model_name,
        "reasoning": reasoning,
        "code_blocks": []
    }
    for id, code_block in code_blocks:
        code_block = {
            "id": str(id),
            "content": code_block,
            "questions": [],
        }

        for _ in range(qa_num):
            q = invoke_lm(model_name, prompt.format(code_block=code_block))
            code_block["questions"].append(q)
        
        qa_data_output["code_blocks"].append(code_block)

    json.dump(qa_data_output, open(outfile, "w"))
    
# NOTES (2025-01-20)
# - benchmark reasoning trace generated by r1 compared to claude
# - include reasoning in the response
# - measure cost for distilling reasoning
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate code QA pairs from a repository")
    parser.add_argument("repo_path", type=str, help="Path to the repository to analyze")
    parser.add_argument("outdir", type=str, help="Path to the output file")
    parser.add_argument("--chunked", action="store_true", default=False)
    parser.add_argument("--qa-num", type=int, default=5, help="Number of QA pairs to generate per chunk")
    parser.add_argument("--model-name", type=str, default="gpt-4o", help="Name of model")

    args = parser.parse_args()

    repo_path = Path(args.repo_path)
    code_blocks = []

    if args.chunked:
        chunker = PythonChunker(repo_path)
        code_blocks = [
            (chunk.id, f"{chunk.id}:\n{chunk.content}") for chunk in 
            chunker.chunk(persist_path=f"{repo_path.name}_chunks.json")
        ]
    else:
        src_repo = SourceRepo(repo_path)
        code_blocks = [
            (f.path, f"{str(f.path)}:\n{f.to_code()}") for f in
            src_repo.source_files
        ]

    create_qa_pairs(repo_path, args.outdir, code_blocks, args.model_name, int(args.qa_num), chunked=args.chunked, reasoning=False)

